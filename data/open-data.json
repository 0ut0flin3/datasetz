{"data request - Is there a list of all US Government agencies and sub agencies and is it available via API?": "\nCheck out:\n\nAn official source for agency information: https://www.federalregister.gov/api/v1/agencies\nGSA API: http://www.usa.gov/About/developer-resources/federal-agency-directory/index.shtml\n\nSource: https://github.com/unitedstates/orgchart/issues/1\n", "usa - Is there a list of all municipal government forms in a machine readable format?": "\nProbably not.\nThere is definitely no such list for Washington, DC (Government of the District of Columbia).\nThere was forms automation project back in 2006/2007 where some forms where converted to database backend using LiquidOffice tool. You can see list of all currently published forms for DC.gov here http://forms.dc.gov/index.html (click Forms on the left). But beware, there are plenty of forms that are just stan alone PDFs or custom webpages.\n", "usa - Is there a list of all utilities that offer the Green Button Download and Green Button Connect?": "\nThe Department of Energy's Green Button website offers a list of companies who have implemented it:\n\nThe Green Button initiative was officially launched in January 2012.\n  To date, a total of 35 utilities and electricity suppliers have signed\n  on to the initiative. In total, these commitments ensure that 36\n  million homes and businesses will be able to securely access their own\n  energy information in a standard format. This number will continue to\n  grow as utilities nation-wide voluntarily make energy data more\n  available to their customers in this common, machine-readable format.\nThe following utilities have already committed to Green Button:\n  American Electric Power, Austin Energy, Baltimore Gas & Electric,\n  CenterPoint Energy, Chattanooga EPB, Commonwealth Edison, Glendale\n  Water and Power, National Grid, NSTAR, Oncor, Pacific Power, Pepco\n  Holdings, PG&E, PECO, Portland General Electric, PPL Electric\n  Utilities, Reliant, Rocky Mountain Power, SDG&E, Southern California\n  Edison, TXU Energy, and Virginia Dominion Power.\n\n... there is also a much more substantial list of companies who have have expressed interest in supporting it.\n", "usa - U.S. City and County Web Data (API)": "\nUsing the Census Bureau's API, you can retrieve quite a bit of information about counties, but you need to learn where it's stored. You can request up to 50 different variables in a single call, and you can ask for all counties in the US with one call.\nTo the census, \"cities\" are \"places,\" and that includes things that you and I might think of as a city but which are not exactly cities legally.\nFor example, this URL would return the total population from the 2010 Decennial Census for all US counties, as a list of lists (in JSON format). The first row is a header, followed by 3221 rows, one per county. The county name and state and county FIPS codes are included in addition to the variables requested.\nhttp://api.census.gov/data/2010/sf1?key=[your API key here]&get=P0010001,NAME&for=county:*\nAnd this is for all the \"places\" in the US (almost 30K):\nhttp://api.census.gov/data/2010/sf1?key=[your API key here]&get=P0010001,NAME&for=place:*\nThere's more info about the API at http://www.census.gov/developers/ and you can request a key here.\nUpdate:\nThe Census Bureau is developing a CitySDK project which is specifically intended to make it easier for developers to pull together city-level data, including, eventually, data from sources other than the US Census. The project is also an effort to experiment with agile development methodologies and community involvement, so if you're at all interested, they invite suggestions of user stories and other participation. See also the Github project and Waffle.io project board.\n", "usa - Does any City have POIA (Public Online Information act) laws in place?": "\nThe closest law I've seen to being as comprehensive as POIA is NYC's Local Law 11, but many other cities have an open data law with looser language about whether \"all\" public data has to be put online.\nMore broadly speaking there's a fairly comprehensive list of city open data laws at:\nhttp://wiki.civiccommons.org/Open_Data_Policy It's also worth noting that most cities are covered by the FOIA laws in place by the state that they're in. You can find a list of those laws at http://www.rcfp.org/open-government-guide\n", "medical - De-identified patient data": "\nCDC's National Health Interview Survey (HIS) provides public use files containing person-level data including demographic data and physical parameters such as weight, and disability. Dating back to 1957, it is an annual survey of a nationally representative sample dates.\nCDC's National Health And Nutritional Examination Survey (NHANES) provides public use files containing person-level data including demographic, dietary, examination, and laboratory data.\nThe Health and Retirement Study (sponsored by the National Institute on Aging, and Social Security Administration) is longitudinal panel cohort study that started in 1992, and covers a representative sample of Americans over the age of 50, as they transition into retirement. Only certain person-level data are public use, because of the sensitive data collected including genetic information, prescription drug use, cognitive function, biomarkers, etc. The \"core\" survey is supplemented by focused add-on modules.\nThe 3 surveys above are used for econometric microsimulation models; in case you wanted another way to search for these datasets that would be one way to back into it. \nNote, de-identified patient data will not include names. \nConcerning EAV models that use large datasets where information in certain categories is relatively sparse, \"sparse\" suggests that an individual could be re-identified. At least with US government release of data, any such sparse data would be suppressed if an individual could be re-identified. Protection of personally identifiable information (PII) and maintaining patient confidentiality are considered fundamental to ensuring public trust and participation in these surveys.\nDisclosure: I work for the Asst Secretary for Planning and Evaluation (ASPE) in the Dept of Health and Human Services (HHS) on the open data initiative. Surveys mentioned above are all HHS surveys, though I have no direct connection to any of them.\n", "usa - Is the data used to calculate national CPI public?": "\nThe CPI is based on surveys conducted by BLS (Bureau of Labor Statistics). Those surveys collect price information which is reviewed by commodity experts. Those experts review (and possible adjust) the data based on their knowledge of the particular commodity. That adjusted data is then averaged to calculate the CPI. You can definitely get the values that are averaged in the final calculation. I'm less clear on how to find the actual survey responses.\n\nVarious options for consuming the data: http://www.bls.gov/cpi/data.htm\nThe average price data: ftp site, documentation\n\n", "data request - Is there a complete list of all US tax-exempt nonprofits in machine-readable format?": "\nThe IRS website has an Exempt Organizations Select Check that allows the user to search for organizations that:\n\nAre eligible to receive tax-deductible contributions\nWere automatically revoked\nHave filed Form 990-N (e-Postcard)\n\nAfter selecting one of these options, a link will appear to download the entire database of organizations. The databases (plain-text files) are delimited by a vertical bar and should be easily imported into any RDBMS or spreadsheet application.\n\nDatabase of organizations eligible to receive tax-deductible contributions (Pub. 78 data).\nDownload page\nDatabase file (16 MB compressed, 59 MB raw)\nDatabase of organizations whose federal tax exemption was automatically revoked for not filing a Form 990-series return or notice for three consecutive years (Automatic Revocation of Exemption List). \nDownload page\nDatabase file (16 MB compressed, 55 MB raw)\nDatabase of e-Postcard filings.\nDownload page\nDatabase file (39 MB compressed, 130 MB raw)\n\nSample of the database\n000344394|Pandas Foundation Inc.|Salem|MA|United States|PC\n000587764|Iglesia Bethesda Inc.|Lowell|MA|United States|PC\n000635913|Ministerio Apostolico Jesucristo Es El Senor Inc.|Lawrence|MA|United States|PC\nThe columns (best guess) are:\nEIN|Name|City|State|Country|Deductibility Status\nThe Deductibility Status is a code that:\n\ndescribes the basis for the organization's or organizations' ability to accept tax-deductible, charitable contributions. \n\nDeductibility Status Codes\n", "usa - Given a federal agency, what can I expect to find on the regulations.gov API?": "\nRegulations.gov offers a list of participating agencies in PDF Format on its About Page:\n\nParticipating Agencies: http://www.regulations.gov/docs/Participating_Agencies.pdf\nNon-Participating Agencies: http://www.regulations.gov/docs/Non_Participating_Agencies.pdf\n\nOn the Regulations.gov FAQ, they describe what a non-participating agency does and does not provide:\n\nA Non Participating agency is a Federal agency that publishes Federal\n  Register documents on Regulations.gov but does not participate in the\n  eRulemaking program; therefore, public comments and additional\n  supporting documentation are not posted on Regulations.gov. In order\n  to view these comments, users should contact the agency directly. In\n  order to find the contact, reference the section in the Federal\n  Register entitled \"For further information contact.\"\n\n", "usa - How can I get a list of all non-geodata datasets on Data.gov?": "\nCurrently Data.gov is run on bunch of technologies. Specifically one of the primary data platforms is Socrata - explore.data.gov.\nThere is API for it defined here http://dev.socrata.com/, but essentially you can call this endpoint and iterate through all pages of master list of datasets\nhttps://explore.data.gov/api/dcat.json?page=1\n\nI have sample scrapers for list of datasets and associated metadata here:\nhttps://github.com/kachok/data-json/tree/master/data-gov\nYou then can filter spatial vs tabular data based on metadata (I believe all of the datasets in explore.data.gov are tabular)\nWith imminent migration of data.gov to CKAN, all of the above will be obsolete (but new API will allow to do similar things as well)\n", "How can I trust the authenticity of an open data source?": "\nThere are attempts at creating a registry of open data sites, see ckan's datahub http://datahub.io/. I'm not aware of any effort to list bad sites, but I've not come across any instances of deliberately false open data.\n", "licensing - Benefits of using CC0 over CC-BY for data": "\nI have worked for numerous companies in the past who have had a policy of not using any software, libraries, or datasets that would impose requirements on their product, this would include the attribution clause in the CC-BY licence.\nPlacing a dataset in the public domain will maximise its potential audience; whether or not this is more desirable than attribution is going to be a matter of opinion for those involved.\n", "analysis - What's an easy-to-use tool to manage datasets?": "\nI found OpenRefine really useful for people new to Open Data. It offer a nice and clean spreadsheet like interface with the power of querying by field (like SQL).\n", "data request - Is there an official/complete list of all US government agency \"developers' pages\"?": "\nHere is most comprehensive list of all .gov /Developer pages compiled by Gray Brooks at GSA\nhttp://gsa.github.io/slash-developer-pages/\nYou can suggest new additions if some info is missing. It is currently well maintained.\n", "data request - Is there a free downloadable administrative division database of Germany?": "\nHere is German Open Data portal for geodata - http://www.geodatenzentrum.de/geodaten/gdz_rahmen.gdz_div?gdz_spr=eng&gdz_akt_zeile=5&gdz_anz_zeile=0&gdz_user_id=0\nIt has administrative areas, zipcodes and geo names (cities, points of interests), etc available for download (Shapefiles) and as webservices (WMS).\nSpecifically German addresses data is not available, though.\nAlso, you can find zipcodes/admin areas on Wikipedia or DBpedia \nhttp://en.wikipedia.org/wiki/List_of_postal_codes_in_Germany\nhttp://dbpedia.org/page/List_of_postal_codes_in_Germany\n", "usa - Should I approach an agency unofficially before FOIAing them?": "\nIt is very recommended and would very likely smooth the process by at least an order of magnitude.\n", "language - Database with English words with grammatical classification?": "\nThere are plenty of open corpora (databases) of English words available.\nSpecifically, take a look at Brown and WordNet.\nCheck out Natural Language Toolkit it is written in Python and has those corpora available for download. It is one of the most popular packages to work with human languages data.\nIf you prefer to use web based API, take a look at Wordnik API \n", "data request - Multinational list of popular first names and surnames?": "\nThere is a pretty massive list of given (first) names (~50,000), and it's carefully curated (not machine generated).\nMore details are available on another answer:\n\nThe best source of international human given (first) names comes from a German computer magazine. The text file has nearly 50k names that are classified by likely gender, and how popular in each country. It's carefully curated and has a friendly license (GNU Free Documentation License).\nThe file can be downloaded here : ftp://ftp.heise.de/pub/ct/listings/0717-182.zip (name_dict.txt contains the data).\n\n(archive link: https://web.archive.org/web/20200414235453/ftp://ftp.heise.de/pub/ct/listings/0717-182.zip )\n", "data request - Database with the historical prices of most popular products in U.S.?": "\nThe CPI indices from the Bureau of Labor and Statistics is probably what you're looking for. While it doesn't tell you what the price of 12 ounces of Coca Cola is, it does tell you what the average price of, say, \"carbonated beverages\" is over time. It will take some munging, but the raw data is located here\n", "tool request - What server and technologies can I use to extract data out of Wikipedia's infoboxes (e.g., ATC code for drugs)": "\nProject DBpedia is a crowd-sourced community effort to extract structured information from Wikipedia.\nDrugs and Chemicals infoboxes are available in structured form, already.\nCheck out \nhttp://dbpedia.org/page/Rosiglitazone\n", "government - Open Data Scorecards?": "\nThe Open Data Institute are building Open Data Certificates ( https://certificates.theodi.org/ ) as a mechanism for describing how a particular dataset does in terms of legal, practical, technical and social information.\nIt's still being built, but you can see an example at: https://certificates.theodi.org/datasets/347/certificates/90 . Although it does require human input it seems like it might possibly be an effective way of 'grading' open data.\n", "geospatial - Where can I get digital cartographic maps of Brazil?": "\nOpen Street Maps and GEOTIFF Files from MODIS, wold do the job.\n", "computing - GPU hardware specifications data?": "\nWikipedia has comparisons of Nvidia, AMD, and Intel GPUs, providing tabular information on different tech specs of the graphical processing units.\n", "parsing - Good tools to parse repetitive unstructured data": "\nOpenRefine can be used to parse semi-structured data into a table like structure, where it can be operated on in a manner similar to a spread sheet and exported.\nThe site features a tutorial on converting a list on wikipedia into a table which may be a good starting point.\nThe operations involved can also be exported incase you need to perform the same cleanup operation again.\n", "Graph visualization/analysis tool?": "\nThere are actually quite a few applications for visualizing and analyzing graphs:\n\nGephi and Cytoscape are two well-known open source\napplications that support large and complex graphs.\nIf you're mainly interested in visualizing graphs, have a look at\nGraphviz, which is an absolute classic.\nYou can also use R or commercial tools like Mathematica if you're \nmore interested in the statistical and analytical aspects (see also this \nquestion over on Stats SE).\n\n", "data request - What is the most comprehensive resource for querying french and english books having an ISBN code?": "\nA quick Google search turns up several APIs.\n\nGoogle Books\nAmazon\nLibraryThing\nhalf.com\nisbndb.com\nGoodreads\nBookfinder\nOpen Library\nWorldCat API\n\nYou can limit your queries to these resources by knowing that French ISBNs start with a 2 and English ISBN's start with a 0 or 1.\nEdit\nAs far as getting ISBNs for older books\n\nReprints of older books will be assigned ISBNs by the publisher. That\n  is why books in the public domain (like most 'classics': Shakespeare,\n  Chaucer, etc.) are issued by different publishers and have different\n  ISBNs. - Source\n\nI would assume this goes for any books that existed before ISBN was in wide use. ISBN became a standard in 1970 but in France, ISBN codes have been mandatory only since the 3rd December of 1981 (link is in French).\n", "finance - Where can I find open data on historical forex rates for financial reporting purposes?": "\nI've used the Yahoo Finance API in the past, but apparently it doesn't exist (although it works fine). There is no documentation.\nFor a site with documentation, I'd suggest Open Exchange Rates, which contains forex values that you are looking for, via an API.\nThere are many API methods and in particular for your case, you can request a time series.\nFrom their site:\n\nYou can access historical data snapshots, where available, in the format \u2018/api/historical/yyyy-mm-dd.json\u2019 (for example: /api/historical/2011-11-21.json.) There will soon be an 'available dates' endpoint available too.\n\nYou'll need a code to cycle over dates, here is an example of my python 2.7 code that collects 2013 precipitation and weather in Zurich.\nimport requests\ndef get_precip(gooddate):\n    urlstart = 'http://api.wunderground.com/api/REDACTED_KEY/history_'\n    urlend = '/q/Switzerland/Zurich.json'\n\n    url = urlstart + str(gooddate) + urlend\n    data = requests.get(url).json()\n    for summary in data['history']['dailysummary']:\n        print ','.join((gooddate,summary['date']['year'],summary['date']['mon'],summary['date']['mday'],summary['precipm'], summary['maxtempm'], summary['meantempm'],summary['mintempm']))\n\nif __name__ == \"__main__\":\n    from datetime import date\n    from dateutil.rrule import rrule, DAILY\n\n    a = date(2013, 1, 1)\n    b = date(2013, 12, 31)\n\n    for dt in rrule(DAILY, dtstart=a, until=b):\n        get_precip(dt.strftime(\"%Y%m%d\"))\n\nupdate I noticed for the free account you can only make 1000 API calls per month (link). Not so open after all.\n", "weather - Data download for Chinese meteorological satellites": "\nTry http://satellite.cma.gov.cn/portalsite/Data/Satellite.aspx?currentculture=en-US\n\nYou can then download a \"quickview\" by clicking the little \"picture\" icon:\n\nThe full resolution data seemingly require payment.\n---Information below is OBSOLETE. ---\nTry FENGYUN Satellite Data Center. Getting the data from it can be a little tricky.\nFirst click one of the icons:\n\nIn the next page, remember to select one of the \"products\" before clicking \"next\":\n\nI haven't really used the system. It seems you need to pay for the data. However, \"quickviews\" are free:\n\n", "data request - How can I download the complete Wikidata database?": "\nThe wikidata dump is already available. As of now, the last (mostly) complete dump is from 5 May 2013 and it includes the dump of pages in the important namespaces (pages-articles.xml).\n", "tool request - Is there any world-wide real time catastrophe information system?": "\nAs far as I know there is no system in use that would alert people before the disaster.\nTo all the other questions, quite a lot of countries have adopted Sahana Eden, that does all the tasks related to the Management of the Disaster, including all the information you have putted there and a lot more...\nIt has been deployed in large disaster scenarios and it is \"as good as it gets\", also it complies with UN Standards!\n", "economics - How can I access open data on the Indian rupee value, inflation and sensex index over time?": "\nTrading Economic provides information about India inflation over the years (apparently registered users can export this data).\nGoogle finance provides the Rupee value over th years and you can download this as a csv file.\nGoogle finance also provides SENSEX data.\n", "tool request - Are there any good libraries available for doing normalization of company names?": "\n[Note: I'm the co-founder of OpenCorporates, which along with our reconciliation service for OpenRefine, has been kindly mentioned by several of the answers, but I've tried to cover some of the general issues here, using our experience, rather than suggesting we've got all the answers]\nThis is a really difficult problem because in general it requires more than just string algorithms, and also because of the underlying question of what you are matching.\nSo first, what will you be matching the normalised strings to? If you're matching them to companies (legal entities), OpenRefine is really great for that as long as it's for jurisdictions that we have in OpenCorporates (we have about 30 of the US states, for example), or it's matching the names against entries in Wikipedia, using Freebase. I'm not sure of the matching algorithms that Freebase use, but we do all the usual things such as you've got with your ACME examples.\nWith the Walmart examples, it's rather more difficult. In part this is because sometimes these normalisations would lose information that is helpful in matching to entities. Take the hyphenation issue for example, and say you were searching for Wal-Mart Limited. If you search OpenCorporates on the web interface (which is pretty liberal in what it returns, and for the sake of this example, let's say we're limiting to companies in the UK, and this is what you get:\n\nCREDITS INVESTMENTS LTD (United Kingdom, 16 Feb 2007- )\nDOS-MART (UK) LIMITED (United Kingdom, 18 Feb 1999- )\nGEORGE SOURCING SERVICES UK LIMITED (United Kingdom, 2 Jun 2000- )\nWAL MART LIMITED (United Kingdom, 5 Jan 1999-17 Sep 2002)\nWAL-MART LIMITED (United Kingdom, 12 Apr 2006-31 Mar 2009)\nWAL-MART LN (UK) LIMITED (United Kingdom, 3 Feb 2000- )\nWAL-MART STORES (UK) LIMITED (United Kingdom, 26 Apr 1999- )\nWAL-MART STORES LIMITED (United Kingdom, 18 Mar 1994-10 Jul 2001)\n\nLook at the fourth and fifth entries and you can see the problem. One has a hyphen, one doesn't. It's also worth looking at the first three entries, and these were returned because their previous names matched Wal-mart. There are also lots of other cases where normalisation brings false positives.\nSo although we do some normalisation, we also score against the non-normalised search term in our reconciliation API. We also are increasingly using other attributes of the companies, scoring current companies higher than dissolved companies (company names are often used by unrelated companies over time), unless a date is supplied in which case we return the company that was called that name at the given date.\nFinally there's the question of what do you want to match? With Walmart, maybe it's 'obvious', but in general it isn't, even with something like \"Tesco\", the world's second biggest retailer (after Walmart), which could match several unrelated entities around the world, including the US.\nThat's why when we're matching a dataset using OpenRefine and OpenCorporates we do a first pass, limiting to a jurisdiction, passing dates in where we can, and then automatically reconciling to those entities which have a high score and where there's just one high scoring result returned by the API (there's some cool filtering in Google Refine to do this), and then progressively go down the scores, sometimes using the ability to do live reconciliations, to match the ones with no match, or with ambiguous results.\nBecause we use the service internally we're constantly finding 'edge cases' (that aren't so edgy), and improving the matching. People who've used it and the proprietary DBs reckon OC is very good. However, we can see there's massive of room for improvement. We're now doing more normalisation for non-US/UK company forms (e.g. GmbH and SA/SARL), and playing around with transliteration, and when we launch company hierarchies, there's obviously potential for assuming that by Walmart/Wal-mart you mean the company at the 'top' of the Walmart tree.\nHope this helps.\nChris  \n", "geospatial - Where can I find high-precision cartographic data of French rural areas?": "\nData.gov provides geospatial data for areas around the world through NASA satellite imagery and other shared services.  You can search for mapping data in France or anywhere else at http://geo.data.gov/geoportal/.  You can also see a view of a new geospatial catalog on Data.gov below.  \nI've already entered the parameters for France, so you can see the data available.  Just draw the bounding box where you need it to find any geospatial data that is available in that area.\n", "tool request - Extracting tables from multiple PDFs": "\nI have had great luck with https://github.com/jazzido/tabula\nOnce the PDF is loaded into the system, it takes manual selection of the table to get the data, but I really prefer it over rolling my own computer vision system, as I've found tabula to be highly accurate, and I can't say the same of a 100% automated system.\n", "Tools for merging similar datasets continuously": "\nOpenRefine (formerly Google Refine) offers nice tools for data cleansing, e.g. correcting slight spelling variations. You can also script all transformations on the data and re-apply them later for updated datasets.\nIs this what you're looking for?\n", "data request - What is the best source for finding what businesses have opened/exist/closed in a given geographical area?": "\nThere are several pay services, including ReferenceUSA and Nexis.\nIf you're interested specific geographic areas (as opposed to everywhere in the US), you can check with local governments (usually county) for FOIA-able occupational license data and/or data about property records, usually available via tax assessors office, which will often contain ownership information and flags for commercial properties.\nA service at Florida International University called TerraFly has tons of geocoded business records. I'm not sure what all their sources are, but it might be worth your while to reach out to the researchers there. I think they have some API access.\n", "crowdsourcing - What are the available tools for managing crowdsourced data-cleaning tasks?": "\nPyBossa is an open platform for crowd-sourcing. You write a bit of HTML/JS that is the microtask. They have examples including PDF transcribing. Features include user registration, user credits, statistics.\nhttp://crowdcrafting.org/about\n", "trust - Open Data Conflict Resolution": "\nI will answer this in the more general term (leaving the consumer out). Invariably when data is aggregated and coalesced from plural data sources you will have conflicts in records. Just think of all the wacky occupants you receive in your mailbox or the wacky places you supposedly lived when you do a FREE background search.\nThese data sources need to periodically perform a validation/reconciliation process on the aggregated/coalesced dataset. Typically, the process involves:\n\nA primary table which holds the 'best values' for a record.\nA secondary table which uses the primary key in the first table as a foreign key, which holds the conflicting values of the first table.\nA process which consists of a:\nA. Validation - eliminates secondary records deemed to be invalid.\nB. Reconciliation - chooses the values that are 'best' at the moment.\nThe process is repeated periodically (or on demand) on records that are updated or new records are added.\n\nNote: The not-best-value records in the secondary table, until deemed invalid, are retained, so that they can be reapplied when other records are updated or added.\n", "census - API for information about Brazilian cities/states": "\nBoth the Brazilian Community CKAN instance http://br.ckan.net/dataset and the Brazilian Government's official Data Portal (CKAN-powered) which you link to, have APIs.\nThere's also the World Bank API: http://data.worldbank.org/\n", "Metadata standards and best practices for data dictionaries for CSV files/data": "\nI'd suggest using JSON Table Schema: https://specs.frictionlessdata.io/table-schema\nIt's:\n\nJSON-based\nSuper simple\nExtendable\n\nHere's a rough outline:\n\n  # fields is an ordered list of field descriptors\n  # one for each field (column) in the data\n  \"fields\": [\n    # a field-descriptor\n    {\n      \"id\": \"field unique name / id\",\n      \"label\": \"A nicer human readable label for the field\",\n      \"type\": \"A string specifying the type\",\n      \"format\": \"A string specifying a format\",\n      \"description\": \"A description for the field\"\n      ...\n    },\n    ... more field descriptors\n  ]\n\n", "tool request - Cost of ownership - CKAN for city/local government or small federal agency": "\nFiguring out the cost for running CKAN boils down to two different categories: the cost of running the software itself, as well as the cost of building and maintaining your open data catalog going forward.\nThe cost of the software itself is simpler to answer: CKAN itself is free/open-source, and available as a hosted solution. \n\nIf you go the open-source route, you'll need to work out the cost of your employee's time, cost of server resources, bandwidth, storage, etc. Your employee time may be minimized by using CKAN's deployment services to get started. As for hardware, the system requirements for CKAN are minimal (it can run on a single server) and probably won't be a significant factor in your costs. This can grow significantly as your data catalog grows, your catalog usage grows, or as the service becomes critical enough to warrant making your CKAN highly-available. You'll need to know your own data size and usage patterns to accurately predict this.\nIf you go the hosted route, this is simpler to figure out. CKAN publishes their hosted pricing online - it runs from from between $400 - $3500 at the time of this writing.\n\nThe cost to run an effective open data implementation within your government can vary widely. It could be near-zero for a site that appear to be \"abandonware\" but if you want a continually updated site with a growing repository of useful content, you'll need to spend time building and curating it. This goes far beyond the scope of IT; having a champion within the organization with a knowledge of the data produced will almost certainly help, rather than making your CKAN instance \"yet another tool being forced on me.\"\n", "language - American English SMS Text Message Corpora": "\nThe largest English corpus I've found (over 10,000 messages) is the National University of Singapore's SMS corpus -- select the corpus with \"all\" messages -- however, closer examination reveals that relatively few of the messages originate from US participants.\nA corpus of SMS spam messages has been created which are written in English. There are over 1,000 legitimate messages and only a few hundred are spam-related.\nDr. Caroline Tagg created a corpus of SMS messages (although I believe they are primarily in British English), but I cannot find the corpus online. However, her paper contains hundreds of messages from the corpus.\nI created a text message corpus of SMS messages in American English that contains over 4,900 messages, a few hundred of which are related to illegal drug use. Now available only on archive.org.\n", "usa - What is the appropriate way to timestamp/determine recency of given data set?": "\nThe way I read the question it seems like you're asking whether \"old\" data can still be considered recent. \nLarger data sets or data that is published at some frequency may not always be current. It takes time to scrub data for publication. Technically, the data is considered current if you're using the most recent published data. In other words, if a data set is published end of year, you will have \"current\" data up until the new data is published. \nHowever, if the data is used by an application, typically the users/shareholders of that application understand the publication cycle of data being used.\n", "usa - What is the best way to request machine readable data from a FOIA request?": "\nA common strategy used by journalists I've worked with is to first FOIA a data schema or other explanation of how the data is managed by the government body. This allows you to be much more explicit in constructing your actual request.\nAnd as rcackerman noted, you may not actually have to start with a FOIA -- but ask them what they have and how they have it, so that when you do file a FOIA for actual data, you can be precise.\n", "companies - Non US-centric databases on boards of directors and government agency memberships?": "\nFrom corporate boards of directors, OpenCorporates is a fantastic resource for this kind of thing, if a bit intimidating to wade through.  They do have lots of US data, but also UK, and many other jurisdictions.  It's all scraper-assembled, so not quite as clean as you get from LittleSis, but you can definitely find corporate officers.\nI'm a little fuzzy on what you're asking for with respect to governments.  Do you want a list of government employees?  Or a list of corporate actors who serve in government capacities (on advisory committees and the like?)\n", "data request - Where can I find open listings of zipcodes in Indonesia?": "\nThis website claims to contain a complete [list of?] postal code[s] in Indonesia\nIt is not in a good machine readable format, but the html uses <pre> tags with makes much more easier script the data off there.\n", "usa - Is there a centralized schedule of data release dates for U.S. federal agencies?": "\nOn Data.gov we haven't looked at trying to aggregate a forward-looking schedule. In general we encourage agencies to release data as quickly as possible. Some data releases occur on a very regular basis, but there is often some fluctuation based on the internal approval processes.  I expect as agencies respond to the actions to comply with the new Open Data Executive Order http://www.whitehouse.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- it will be easier to create such a list.\n", "geospatial - Is there an open API for world terrain data?": "\nThe Google Elevation API should allow you to access elevation data world-wide and allows you to give location as latitude/longitude. \n\nWhat Can You Do With the Elevation API?\nThe Elevation API provides elevation data for all locations on the surface of the earth, including depth locations on the ocean floor (which return negative values). In those cases where Google does not possess exact elevation measurements at the precise location you request, the service will interpolate and return an averaged value using the four nearest locations.\n\nIt does have restrictions on its usage unfortunately - \"the Elevation API may only be used in conjunction with displaying results on a Google map; using elevation data without displaying a map for which elevation data was requested is prohibited.\" - https://developers.google.com/maps/documentation/elevation/#Limits\n", "data request - Are there open complete usenet archives?": "\nYou should check out Exploring the USENET Archive: Early Thoughts and here is archive.org's archive.\n", "data request - Database of ships?": "\nFrom this answer on Get the Data: http://getthedata.org/questions/262/list-of-ocean-going-oil-tankers-and-owner/ (provided by Kit Wallace). Note none of this detail seems to be open data (as per Open Definition).\n\nThere are a number of commercial sources such as http://www.ship-info.com/ or restricted sites http://www.equasis.org/ sites with copyright data http://www.digital-seas.com/ but also some amateur sites (which I cant find now).\nhttp://www.shipais.com includes data about the ships it plots - eg. http://www.shipais.com/showship.php?mmsi=256933000 but I'm not sure where that comes from now. Worth investigating.\nThe ITU holds public details about MMSI numbers in their MARS database which does hold some category data , but its limited - here is the data for tanker with the above MMSI and the database is only searchable by MMSI, name or callsign.\nThe best source though you need to register and search by individual boat is equasis which contains full ownership details. According to the website\n\nFrance and the European Commission shared the cost of developing and running Equasis until 31 December 2001 when the maritime authorities of the United Kingdom, Spain, Singapore and Japan also agreed to support Equasis financially. The budget of Equasis is agreed and provided by the Equasis MoU members. It is anticipated that the use of this website will remain free for the foreseeable future\n\n\nAlso:\nMarinemapper exports KML files, and seems to have links to a lot of information about the vessels themselves.\nVesseltracker also provides data about a variety of ship types, although it's commercial and non-open.\n", "data request - Database of fictional characters?": "\nA past alternative to using DBpedia as suggested by Nicolas Raoul was Freebase before it was shut down.\n", "creative commons - CC-BY vs MIT or BSD licenses regarding re-use?": "\nA good reason to not use MIT and BSD licenses for data is that they were written for software, so they're not a great fit. And, CC-BY was written for creative works, not data.\nI don't know where you got the impression that BSD doesn't allow sublicensing, but it certainly does.  It also allows adding additional licenses, as long as they are license compatible.\nCC-BY says that attribution needs to be given in the form specified by the licensor.  If you want lighterweight attribution requirements, just say so in your CC-BY license.\nRather than choosing A, B, or C, you might want to start by writing down the goals that you're trying to achieve with your license.  Given a set of goals, people might be able to suggest a license or family of licenses which is a good fit.\n", "usa - Is there an API or global source for US ballot information?": "\nSeveral sources of this kind of information:\n\nBallotpedia - lots of information, but in wiki pages, so not well-structured/API-able.\nThe Ballot Information Project - an NOI project; a fair bit of data, structured (with an API, I think?)\nThe Voting Information Project - this one has big sponsors (Google, etc.), and has an API as well.\nProject Votesmart - Used to be the place to go for this kind of thing, but now charges for their data; still, they have a lot of stuff and could be a fallback, depending on whether the other sources have what you're looking for.\n\n", "usa - Is there a list of Chemical Weapon Industry Facilities/Funders?": "\nAccording to wikipedia:\n\"the United States had destroyed 89.75% of the original stockpile of nearly 31,100 metric tons (30,609 long tons) of nerve and mustard agents declared in 1997.\"\n\"The primary remaining chemical weapon storage facilities in the U.S. are Pueblo Chemical Depot in Colorado and Blue Grass Army Depot in Kentucky.[28] These two facilities hold 10.25% of the U.S. 1997 declared stockpile.\"\nAnother good place to start would be the EPA: http://www.epa.gov/envirofw/geo_data.html\nYou can sort out the type of regulated sites you are looking for and extrapolate which are likely candidates.\nHere is a list of Biosafety Level 4 Facilities which are those facilities designed for work with extremely dangerous pathogens.\n", "Are there any international non-governmental data aggregators?": "\nThe Data Hub, powered by CKAN, currently lists more than 6000 datasets, though not all of them are Open Data. These datasets come from all kinds of sources, not just governments and statistics institutes.\nThere is also the Linked Data community which collects datasets in the Linking Open Data Cloud group on the Data Hub. Most of these datasets are related to research, but there is also geographic data, media-related data, as well as other user-generated data available.\nYou can also have a look at the related discussion at A database of open databases?.\n", "data format - Standards for self-documenting text files?": "\nYAML frontmatter tends to be the generic standard for documenting text files. Parseable/compatible with JSON, easily human readable, easy to type in. It's used in content management systems, in combination with Markdown, to append metadata to blog posts in text files without the need for fully blown content management systems. \nhttp://www.yaml.org/\n", "licensing - Requirements of the Open Data Commons Attribution License": "\nI am going to give a somewhat different answer than Gisle does above: Regarding legal requirements ask a lawyer.   However otherwise, this largely supplements Gisle's post above.\nThe reason I say to contact a lawyer if in doubt is that copyright law varies significantly from one jurisdiction to another and in fruits-of-labor jurisdictions it isn't clear to me where the natural clear line where copyright protections end would be in a mashup.  What holds in one jurisdiction might not in another, but additionally to the extent that there is a balance between users and copyright holders, this may be different from one jurisdiction to another and from one application to another within a given jurisdiction.  So definitely discuss the matter with a lawyer.\nThe second point Gisle makes though is a good one and that is about norms.  It is important to consider that coordination and cooperation can be helpful, and so doing what the community expects in a way that works with what you are doing is a good way to build some bridges which can be quite useful later on.  I would urge people regardless of the law to engage in the community, and try to work out a mutually acceptable solution regardless of what the law requires.  Different communities have different cultures and to the extent you can work with a community's existing culture, you may get more support, both moral and material.  To the extent the community feels there is an obligation there is often a lot to be gained through fulfilling it.\nA third point I would make though is that as a businessman it is not the worst idea to figure that norms are the essence of licenses.  If the norms are to not worry about parts of licenses, those will be very hard to enforce (politically and possibly legally too depending on jurisdiction), and if norms are not followed even if the legal requirements are, this can result in painful consequences.  So I would say if in doubt, the norms are what you follow.\n", "releasing data - A database of open databases?": "\nThe short answer\nDataPortals.org (previously DataCatalogs.org) provides a comprehensive list of open data portals from around the world. Their (meta-)data is in the public domain and available for download as CSV and JSON.\nThe longer answer\nData that is somehow related is usually grouped in datasets or databases, contained in files (e.g. CSV or spreadsheets) or some kind of database management system, which might be accessible via an API.\nIn the context of Open Data, data portals, data catalogs, or data hubs make it easier to find these datasets or databases.\nA great example of such a data portal is the Datahub, which currently lists more than 4,500 open datasets.\nHowever, there are already hundreds of data portals. A few prominent examples are the official data portals of the US (data.gov), the UK (data.gov.uk), or the European Union (open-data.europa.eu).\nThis is where DataPortal.org comes in: It is a data portal of data portals.\nTo sum it up:\n    DataPortals.org --> Specific Data Portal --> Specific Data Set --> Open Data\n\n", "data request - Open API for nutritional information and/or food barcodes?": "\nThe complete USDA National Nutrient Database for Standard Reference can be downloaded as ASCII text files from here \u2014 no PDF scraping necessary. \ud83d\ude42\nRegarding product barcodes, have a look at Open Product Data, a new project by the Open Knowledge Foundation.\n", "Restrict search to open datasets on CKAN's Data Hub?": "\nThere is a simple answer thanks to the isopen parameter. Here's the query:\nhttp://datahub.io/api/3/action/package_search?q=isopen:true\nNote that q is basically the classic solr query paramemter so if you want to combine this with normal queries you do stuff like:\nhttp://datahub.io/api/3/action/package_search?q=gold%20isopen:true\nOr, slightly more elegantly:\nhttp://datahub.io/api/3/action/package_search?fq=isopen:true&q=gold\nNote that this just checks whether the license is open (as in conforms to the Open Definition).\nStrictly, for full openness as per the open definition the data should be accessible (meaning available (in bulk) and machine-readable). As this is difficult to check automatedly these requirements are not part of the isopen computation in CKAN at the moment.\n", "linked data - Any uses of JSON-LD?": "\nI'm maintaining a list of early adopter in the JSON-LD Wiki\nIf you want a more visual representation, you might wanna look at http://slideshare.net/lanthaler/building-next-generation-web-ap-is-with-jsonld-and-hydra/34\n", "What are the practical limits of releasing open data via bit torrent?": "\nBitTorrent is not a great solution for this. Because each file distributed would need its own network of seeds and peers, you'd effectively dilute the network pool with each file you release, leaving you where you started: you being the one doing most of the distribution for most of the files in the first place. \nIt's probably better to emulate govtrack.us' rsync strategy. Perhaps also with terms that say that anybody rsyncing must run an open rsync themselves (or voluntarily, depending on the size and happiness of your community) This will cut down on redundant downloading, and give you some bandwidth advantages.\nHere's how govtrack does it: http://www.govtrack.us/developers/rsync\n", "documentation - Standards for documenting use caveats?": "\nI don't know that there can be a good standard other than what I was taught when studying environmental computer models, which is to be clear about everything up-front. Obviously this has to be documented somewhere in the expected place (like a README or the like) but the following things should probably be addressed on some way:\n\nMethodology of how the data was collected.\nIntended use, selection methodology\nAny normalization which occurred prior to publication\nAny known assumptions underlying the above three.\nAny other notes that the collectors think might be useful.\n\nThis way people can read and check against their assumptions before they use the data in various ways.\n", "data request - Is there a source for various Scrabble dictionaries?": "\nThe Wordnik API will tell you whether any single word is a valid scrabble word (among other information).\nhttp://developer.wordnik.com/docs.html#!/word/getWord_get_1\nI am not sure that's exactly what you're looking for, but it's the best I've got.\n", "computing - What quantified self products have open data behind them?": "\nThere are some companies that are sharing aggregate data like the Green Button companies in energy: http://www.greenbuttondata.org/greenadopt.html but there are many of them, and it's hard to be specific without further description of the type of data you are looking for.\nSome companies offer such information at an aggregate level, and others, depending on terms of service, obviously offer individualized data for a fee.\n", "usa - How do United States federal agencies release data?": "\nThe processes vary from agency to agency. In general, data is gathered as part of a regular task for a project or program within an agency (this could be the Mars Program or the 2010 census). The data is structured, validated, and organized, and then that data is used for a purpose for the agency (anything from scientific analysis of the surface of Mars to a count of people in a specific city).  The data is generally approved for release outside of the agency through a secondary process that ensures it is valid, understandable, and does not violate either citizen privacy or national security.  It is then posted on an agency site.  \nMost agencies also release their data to Data.gov, which allows people to find the data independent of knowing the specific program, project, or web site to which the agency has released the data.  The existing policies to which U.S. federal data must comply to be released are posted at: http://www.data.gov/data-policy\nThe new Executive Order and Open Data Policy http://www.whitehouse.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government- requires that agencies default to openly posting data gathered in the course of doing their work.  This still allows an agency to not post data that would violate a citizen's privacy or national security, and it would also allow an agency the opportunity to post the data once it is structured, valid, organized, and understandable (pursuant to the Information Quality Act, P.L. 106-554).\n", "How does one parse weather data?": "\nNOAA provides weather data. You can see the general information and visualization at http://www.weather.gov/  Specific data products are found at http://www.ncdc.noaa.gov/most-popular-data  When you click on a dataset you are interested, there is technical documentation and material to guide you in the use of the data. For example, local climatological data can be found at http://cdo.ncdc.noaa.gov/qclcd/QCLCD?prior=N with detailed notes at http://cdo.ncdc.noaa.gov/qclcd/qclcdfaq.htm\n", "data request - Is there an exoplanet API or dataset?": "\nWith a little searching, I found what I was looking for. NASA has an archive of Exoplanets, as well as an API for it. The data are updated weekly.\n\nArchive Home\nAPI\n\n", "data request - Are there datasets prepared for machine learning?": "\nThe University of California, Irvine provides a dataset repository specifically for machine learning purposes. There are currently 239 datasets in the repository.\nThese datasets come in many different formats and topics. The oldest datasets in the repository date back to the late 80s, and there are some datasets that are from 2013.\n", "best practice - Releasing old historical/genealogical datasets as open data": "\nPublishing the Data\nIf you don't want to go down the full CKAN-style route a really simple option if you've got a bunch of CSV files is just to put them online and turn them into \"simple data format\" data packages by adding a tiny bit of metadata in the form of a datapackage.json. Building a catalog out of this is really easy (and can be done with just a bit of JS+HTML!).\nSimple Data Format\nFor details you can see this simple introduction to simple data format for more information. Here's a simple example:\nHere's an example of a minimal simple data format dataset:\nThere are 2 just files, the data file data.csv and the datapackage.json:\ndata.csv\ndatapackage.json\n\ndata.csv looks like:\nvar1,var2,var3\nA,1,2.5\nB,3,4.3\n\nThat is there are 3 fields (columns) and 2 rows of data.\nA simple datapackage.json for this data would be:\n{\n  \"name\": \"my-dataset\",\n  # here we list the data files in this dataset\n  \"resources\": [\n    {\n      \"path\": \"data.csv\",\n      \"schema\": {\n        \"fields\": [\n          {\n            \"id\": \"var1\",\n            \"type\": \"string\"\n          },\n          {\n            \"id\": \"var2\",\n            \"type\": \"integer\"\n          },\n          {\n            \"id\": \"var3\",\n            \"type\": \"number\"\n          }\n        ]\n      }\n    }\n  ]\n}\n\nPeople to contact\nSuggest these type of folks (you may already know them): \n\nFolks at the Open Knowledge Foundation - e.g. worth pinging the okfn-discuss mailing list (there have been several discussions about genealogical data here over the years)\nhttp://www.opengenalliance.org/\n\n(Disclosure: I helped write the simple data format spec and am a member of the Open Knowledge Foundation!)\n", "usa - What new open data do you need from the U.S. government?": "\nSome of the information you are looking for can be found by looking at the tags portion of the web site you can see how people are tagging their questions. Of the More popular tags the one I think you would most interested in is API (at the time of this writing approximately 1 in 7 questions are tagged with API). I think the most important thing in an API is accessibility, and from my experience accessibility can be broken down into a couple different areas. \n\nHow much effort does it take to get the data into a native data structure within my program?\n\nAre there third party libraries like python-twitter for Twitter?\nIs the data in a standard format like JSON or XML? (coming from a python background JSON is preferable to XML)\n\nHow well documented is the API and how easy is it to find and navigate the documentation?\n\nDoes the documentation have broken links, was it's last update in usenet post from 1995?\nAre there code examples in the documentation that I can simply copy and paste into my editor and get instant results? (The less time I have to spend figuring out how something works the faster I can develop... Duh :-)\nIs deprecated functionality annotated? Are alternative methods provided for deprecated functionality? \n\nIs the data provided by the API complete and/or meaningful?\n\nAre there specific error messages? (i.e. You entered an incorrect date format is a much better error message than Your input is invalid) \nI run into a lot of situations where an API has a list function of some kind and the list function will only return a portion of the expected results. (i.e. if query example.com/api.php?list=letters and get back anything other than ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] there's a problem. If there is some reason for not sending an API user all 10 million baby names from your baby name database explain that to them and offer an alternative way to get the data.\nAre there explanations for all the properties found in the data? (i.e. an API response gives me a something like {'license':None, 'data':[1,2,3]} does that mean that there is no license associated with the data or does it mean that the API providers don't have a license on record but one may exist. Ambiguity is bad.)\n\n\nTo sum this up I don't think there is a problem with the amount of data or variety of data but rather with accessibility.\n", "What are the most useful formats in which to release geospatial data?": "\nYou can find a list of GIS file formats on Wikipedia. Here is a decent overview of open source GIS servers from the gis.se site, these are the servers people who use open data will most likely be using, so target the formats that those servers use. I would consider some kind of open vector/raster format (I like geoJSON for personal projects because it works well with openlayers, I also know people who use netCDF and JPEG2000). As a side note I've used this tool before to convert between different formats.\n", "usa - What is the governance of the US Government's Project Open Data?": "\nFrom the FAQ (emphasis is mine):\n\nWho can participate in Project Open Data?\nAnyone \u2013 Federal employees, contractors, developers, the general\n  public \u2013 can view and contribute to Project Open Data.\n\nIt is possible to modify the content of the website as well as to contribute to the various tools offered on github by forking the projects. If your pushed changes are approved, they are integrated in the \"official\" app.\nAs for the governance, later on in the FAQ:\n\nWho is in charge of Project Open Data? \nUltimately? You. While the\n  White House founded and continues to oversee the project, Project Open\n  Data is a collaborative work \u2014 commonly known as \u201copen source\u201d \u2014 and\n  is supported by the efforts of an entire community. See the \u201chow to\n  contribute\u201d section above to learn more.\nAt the onset, the General Services Administration is here to provide\n  daily oversight and support, but over time, it is our vision that\n  contributors both inside and outside of > government can be empowered\n  to take on additional leadership roles.\n\n", "usa - What is the plan for the US Government's Data.gov?": "\nOn the Data.gov team, we'll be blogging and posting here as well about some of the upcoming changes.  The intent is to have all of you give us feedback as we start to evolve ideas, rather than just seeing what happens at the end of the design process.\nMy blog above shows the catalog upgrade using CKAN 2.0 (it's the demonstration site, so don't worry if you see only some of the data there): http://geo.gov.ckan.org/dataset  We'll show you a demonstration shortly of the harvesting of agency JSON files as a new way to federate data to the catalog, and later redesigns of some of home page, dataset pages, and other Drupal components on the site.\nWe are rebuilding Data.gov from the inside out, and have spent time this last year working with the government of India, and now Canada and Ghana as well to open source the Open Government Platform as the underlying code under Data.gov.  You can find it on Github: https://github.com/opengovplatform/opengovplatform-beta/wiki\n", "legal - How should I categorize municipal legislation?": "\nI may have a bad case of hammer-seeks-nail, as I'm currently working fairly full time on a federal legislative markup system with CATO called Deepbills that tags federal legislation in a way that tracks what code it modifies. But here's my thought, based on my experience with how that works combined with my years of poring through the DC code:\nRather than a system that applies a (possibly) subjective classification to the proposed legislation based on reading, what about instead using a system of classification/tagging against the existing code and then tracking what the proposed legislation modifies? The organization of the Chicago municipal code already provides a very loose taxonomy of sorts:\n\nTITLE 1 GENERAL PROVISIONS\nTITLE 2 CITY GOVERNMENT AND ADMINISTRATION\nTITLE 3 REVENUE AND FINANCE\nTITLE 4 BUSINESSES, OCCUPATIONS AND CONSUMER PROTECTION\nTITLE 5 HOUSING AND ECONOMIC DEVELOPMENT\nTITLE 6 RESERVED\nTITLE 7 HEALTH AND SAFETY\nTITLE 8 OFFENSES AFFECTING PUBLIC PEACE, MORALS AND WELFARE\nTITLE 9 VEHICLES, TRAFFIC AND RAIL TRANSPORTATION\nTITLE 10 STREETS, PUBLIC WAYS, PARKS, AIRPORTS AND HARBORS\nTITLE 11 UTILITIES AND ENVIRONMENTAL PROTECTION\nTITLE 12 RESERVED\nTITLE 13 BUILDINGS AND CONSTRUCTION\nTITLE 14 RESERVED*\nTITLE 15 FIRE PREVENTION\nTITLE 16 LAND USE\nTITLE 17 CHICAGO ZONING ORDINANCE\nTITLE 18 BUILDING INFRASTRUCTURE\n\nCertain sections are related to certain issues and may be very specific if you drill down. Titles 16 & 17 relate specifically to land use and zoning. If you look at 16-18 it regards open space, 16-18-40 is the related fee schedule and 16-18-120 specifies the Paulina Street Corridor and outlines where exactly it is.\nSo any proposed legislation which referenced 16-18 could be land-use related and classified as of interest to anyone doing new development or renovation (but not replacement); referencing 16-18-40 indicates that it's cost relevant. Similar logic works for many other sections -- 9-52 is bicycles.\nThe advantages of this as a taxonomic basis is that you'll make your classification something that could more easily be done via machine. It's somewhat limited, but a second level of taxonomy or tagging done against the code could provide a more flexible alert system. Anything modifying title 9 is transportation, 9-52 is bicycles, 9-68-020 is residential parking permits. 9-68-022 is one of several Wrigley areas which is firmly defined.\nBy using a looser initial taxonomy and then applying attributes to code sections you could even use a geographic area definition in an automated way. Given an address XYZ you can then see when it falls within areas impacted by legislation: if it falls within the area described by 9-68-022 then you know any modifications to that section would impact that person.\nThat example is probably more specific than you care about, but the same would apply to any broader neighborhood definitions.\n", "Categories and varieties of Open Data licensing?": "\nHere are the three options listed on opendatacommons.org, the licenses FAQ also has a bunch of really good information.\n\nPublic Domain Dedication and License (PDDL) \u2014 \u201cThe PDDL places the data(base) in the public domain (waiving all rights)\u201d\nAttribution License (ODC-By) \u2014 \u201cAttribution for data/databases\u201d\nOpen Database License (ODC-ODbL) \u2014 \u201cAttribution Share-Alike for data/databases\u201d - The community norms for Attribution-Sharealike are share your work, give credit where credit is due, let others know, use open data formats and don't use DRM.\n\nThere is an here is an interview of Steve Coast (OpenStreetMap's founder)\n\nSteve Coast: Licensing is incredibly important for the community to\n  trust that the data won\u2019t be closed off. So we need to make sure that\n  data from OpenStreetMap will always be free and open. It\u2019s also\n  important that we are able to stop anyone from trying to close it off\n  or derive from it without giving back to the community. We have a\n  multi-year process to re-license based on advice from multiple sources\n  that Creative Commons is not applicable to data. We wish it were, and\n  it probably will be in the future but it wasn\u2019t clear when we began.\n  Until that happens we have a process to move to the Open Database\n  License, which explicitly covers data and not just creative works like\n  photographs or text. The ODbL was in fact started as a result of\n  investigations around the needs of Science Commons and we just helped\n  it to its conclusion.\n\nHere is a guide (incomplete) from opendatacommons.ors on how licensing applies to data from different fields and different countries.\n", "data request - Open dataset on manned space missions": "\nThe Johnson Space Center has a website with biographies of Astronauts and Cosmonauts that you should be able to extract much of your requested information from:\n\nhttp://www.jsc.nasa.gov/Bios/\n\nI find it easier to extract from HTML, but the 'time in space' text might be more difficult to extract from the free text than the PDF that Jeanne linked to.\nupdate : oops ... you asked for time in space of the missions, not the astronauts ... Johnson also has a general Manned Space Flight website, which has information about each of the missions (shuttle, ISS, skylab, etc.), but they're each formatted differently enough that it's a bit cumbersome.\nIf I were you, I'd probably e-mail the contact for those two websites, and ask if they had the information available in more easily parseable form.\nAlso, you might be able to get some information from Wikipedia, such as their list of shuttle missions and list of human spaceflights.\n", "data.gov - How should governments build community around their datasets?": "\nSome general advice:\n\nInvest in community managers and evangelists. The worst thing you can do is create a forum or invite feedback and then not have anyone with a mandate to respond to it in an official capacity; I'm seeing this happen right now with the newly launched project-open-data in which they've invited contributions and have no one to actually respond or accept those contributions\nCreate time-boxed campaigns. Governments and policy mandates come and go, so rather than creating an open-ended \"chat with us about anything\" that may trail off, create 3 or 6 month challenges that you can budget for a high degree of feedback and engagement. This is also helpful for testing your own ability to engage long-term because you can explain your outcomes to superiors/oversight in the short-term. A platform like Challenge.gov can be helpful for this... assuming you actively engage with participants during the challenge periods (many agencies on Challenge.gov are really bad at this and it turns into a black box of disappointment when the winners are announced).\nWrite publicly about your experience in managing the platform and performing community engagement. Have a blog that you post to (at least) weekly about how you are helping actual people do stuff. Use first-person pronouns and first names; don't make it a series of press-releases. The Github blog is a good example of this kind of engagement. If you can't regularly write about your own experiences running the platform (whether because of interest, mandate or oversight), you probably won't be able to relate to the experiences of people using it.\nCreate in-person \"networking\" events. Embrace opportunities to meet with the public, on their level. Which means either attending the meetings of existing community/developer groups, or hosting your own (though I'd recommend starting with the former). Chicago's OpenGov Meetup group often has government employees attending (even when they aren't making a presentation!) who can put a public face on things and help further relationships that may otherwise seem transactional or distant when their interactions only take place online.\n\nThe advice above is tool and platform agnostic. Specifically, I'd recommend:\n\nPut Disqus (or equivalent) on everything; never let a comment go unresponded to (no page should ever have only 1 comment, ever!)\nStart a development blog (as described above)\nContinue engaging in spaces like this one\n\n", "documentation - Standards for documenting gaps in data?": "\nI do not know of standards in this area, but I do know that many data owners document missing data or other known issues in a dataset.  This is generally documented in either the site from which the dataset is linked or in the metadata of the dataset itself.\n", "usa - What are the sources of data on Data.gov?": "\nThe list of all organizations that are contributing data (or metadata) to Data.gov can be found in two different places. \nOne place is on the Federal Agency Participation link at the bottom of the page.  Clicking on an agency or sub-agency name takes you to a page with all of the raw datasets and tools submitted by that agency.  \nThere is also a listing of all organizations (which includes non-Federal entities) at the top of the Catalog page, and shows a description of the organization as well as the listing of all datasets and resources from that organization on Data.gov.\n(Disclaimer: I am the Evangelist for Data.gov.)\n", "data request - Is there a list of hot-spots and free wifis in Germany?": "\nThere is a wiki at http://freewifiwiki.net/index.php?title=Germany which is a listing (not a database).  There are links at the bottom of that site to additional sites in German and English appear to augment this overarching listing.\n", "data request - Downloadable archive of weather conditions for Europe?": "\nThe European Centre for Medium-Range Weather Forecasts (ECWMF) has a rather impressive data collection available via batch scripts, downloadable files or even tailored formats. The first two are freely available (under specific conditions) for non-commercial research but registration is necessary.\nJust to give an example, the GRIB dataset lists 4115 parameters ... including your requested temperature, humidity, precipitation and location (latitude/longitude).\nFor more casual data retrieval, you might try weather services which focus on specific areas, such as Yr from Norway.\n", "releasing data - Alternative to GTFS (General Transit Feed Specification)?": "\nTransmodel is a not very widely used format for schedule data (alternative to GTFS).\nFor real time data (alternative to GTFS-realtime): SIRI is an XML protocol used most heavily in Europe.\nYou'll want to consider what formats developers are most aware of and any possible performance issues.\n\nTRANSMODEL has been adopted as the European experimental standard ENV 12896 in 1997.\n  (La derni\u00e8re version r\u00e9vis\u00e9e du document TRANSMODEL (version 5.1) est une norme europ\u00e9enne disponible aupr\u00e8s de l\u2019AFNOR. Elle a \u00e9t\u00e9 \u00e9labor\u00e9e en 2006, et sa traduction fran\u00e7aise a \u00e9t\u00e9 publi\u00e9e par l\u2019AFNOR en janvier 2012 sous la r\u00e9f\u00e9rence NF EN 12896.)\n\n", "usa - Where can I get bulk access to IRS 990 filings for US non-profits": "\nResource.org has gathered reports dating back to 2002 and says that they process new data monthly. Bulk data can be pulled from here.\nFor those who prefer a simple search, the Economic Research Institute (ERI) has a database of forms dating back to 2003. \nIn at least one case, a journalist on the NICAR-L list reported back that there was a form in the ERI database which was not in the Public Resource set, so proceed with due caution.\n", "tool request - What does CKAN stand for and what does it do?": "\nCKAN stands for Comprehensive Knowledge Archive Network. CKAN is a self-described data portal platform that allows an organization to manage, publish, and share data and for others to find and use that data. \nIn general, data portal platforms provide a structured solution of software, policies, and guidelines that let an organization (often a government entity) share data. The services embedded in these platforms may include data management, content management, data publishing, data discovery, visualization, and workflow. Other examples of such products include Socrata (http://www.socrata.com/), Junar (http://www.junar.com/), DKAN (http://drupal.org/project/dkan), and the Open Government Platform (http://www.opengovplatform.org/).\nCKAN installation guidelines are detailed on the site at http://docs.ckan.org/en/latest/maintaining/installing/index.html\nYou can run it standalone (as noted above) or in a hosted instance. CKAN provides a hosted service, which lets you essentially run the software in an instance on their servers (more at http://ckan.org/datasuite/services/hosted-slas/).\n(Disclaimer: I am the Evangelist for Data.gov which participates in OGPL. Data.gov also utilizes CKAN and Socrata.)\n", "us census - How to normalize the data when mapping crime reports?": "\nI asked a data analyst at the Bureau of Justice Statistics who provided this answer:  \n\"I would say that the answer really depends on what information they are trying to show. There are many different way to normalize crime data and even multiple different ways of doing population based rates.\nFor example,  I've even seen some people playing around with creating rates using the \"flow\" of people through the area, where the denominator for the rate calculation is the number of people who pass through a given area during the day--for example at an airport which has no population per se, but does have counts of the number of people who go through the airport during a certain period of time.\nYou can also create rates for specific crime times as a proportion of all crime in a given area, which helps to identify areas where particular crimes are more likely to occur than other types of crime. This is often done in types of hot-spot mapping where the interest is to identify across a given area where whether burglary (for examples) is more common than other types of crime and how that differs by city block.\"\nDirect contact information and help is at AskBJS@usdoj.gov and they are happy to work directly with folks in this community on such issues.  In the future, as this private beta goes public, I'll invite such experts to answer directly in this forum.\n(Disclaimer: I'm the Evangelist with Data.gov)\n", "data.gov - Have genetic algorithms been applied to Open Data?": "\nNot sure if this answers the question, but there are two aspects: (1) using algorithms to analyze the holdings in a data catalog or a large (big) dataset; and (2) gathering data around genetics and genomics.\n(1) There are many tools and programs underway on data analytics. Check out a large solicitation from NSF and an interagency big data initiative that references useful sites at the end.\n(2) There is a growing set of data on Data.gov related to genetics and genomics, with particular recent emphasis on agriculture (Agriculture.Data.gov). And, open access to federally funded research data is a new directive from the White House in addition to the Executive Order on Open Data.  Research.Data.gov is starting to categorize and organize data related to that latter directive.\n(Disclaimer: I am the Evangelist for Data.gov)\n", "Are there best practices that government API producers should follow more so than non-government API producers?": "\nYou don't identify what kind of areas you're looking for advice on. But I'll highlight several that I think are particularly relevant for Government sources:\n\nWhere feasible, data should not just be made available via an API, but also available for download. This supports other kinds of uses. I think this is important as one goal of Open Government Data is to drive innovation and creation of new businesses. This is easier to do if the entire dataset is available. For example third-parties can offer value-added services over that data.\nAPIs should be accessible without usage restrictions or API keys. This avoids having any barriers to entry. Scaling may be an issue, but it'd be better to offer an open, unsupported API and then offer alternatives for higher-volume usage (and this is the kind of value-add that a third-party can offer)\nData must be clearly licensed. And that licensing information should be included not just in the API documentation but discoverable from the data itself. E.g. linked in API responses. Users should be crystal clear on how they can re-purpose the data. This is important for all Open Data, but particularly so for government.\nUse of Open Standards. Again, this is a benefit in all cases, but government providers should avoid using proprietary formats or data protocols that might impede usage.\n\n", "What would be particularly useful basic APIs for the US Federal Government to offer?": "\nGetting the USPS to open up basic information about ZIP codes, either through an open API or even better as a bulk download, would be a big help to a lot of us dealing with geo problems.\nThe ZIP code API you link to is a great example of how not to do it. It has so many restrictions on its use that it's not possible to use it with most open data sources - \"User agrees to use the USPS Web site, APIs and USPS data to facilitate USPS shipping transactions only.\". You have to phone the postal service before they'll even enable your access!\nThe US Census does a great job of giving us their best guesses at ZIP code locations, but it's missing a large proportion of rural areas, and the boundaries aren't very accurate. \n", "nonprofit - How does the Sunlight Foundation relate to Open Data?": "\nFrom the about page:\nThe Sunlight Foundation is a nonprofit, nonpartisan organization that uses the power of the Internet to catalyze greater government openness and transparency, and provides new tools and resources for media and citizens, alike. We are committed to improving access to government information by making it available online, indeed redefining \u201cpublic\u201d information as meaning \u201conline,\u201d and by creating new tools and websites to enable individuals and communities to better access that information and put it to use.\nIn practical terms, Sunlight's open data work has several facets.  We do policy advocacy around encouraging openness in government (of which the POIA stuff is an example, as is the DATA Act), work with government agencies to figure out how best to expose data they've decided to open, and host an annual transparency-oriented conference of which open data is a major component.  Our Labs team, the technical arm of the organization, also both consumes and produces open data via our tools, which try to make government information more accessible to the public.\nAs for who works here: here's a staff list.\n(Disclosure: I'm a developer at Sunlight.  Sorry if this post comes across as overly self-promote-y.)\n", "data request - Is there a better public version of USA's Social Security Death Master File?": "\nAs noted, there is no official public version of the file, because at this time the NTIS only provides it to subscribers.\nAs for parsing the file, I made a schema compatible with csvkit's in2csv utility. The schema can be downloaded from https://github.com/JoeGermuska/ffs/blob/master/us/ssa/death_master_file.csv \nOnce you've installed csvkit and downloaded a copy of the schema, the command would be\nin2csv -s death_master_file.csv ssdm1 > ssdm1.csv\n\nwhere ssdm1 might vary based on which of the ZIP files you retrieved from ssdmf.info\nFor more on the in2csv script, see readthedocs\n", "Are there publicly available sources listing uses of open data?": "\nData.gov has a developer showcase with links to 300+ applications that have been made with their data.\nupdate: As does uk.data.gov (per D Read).  \nAlso, if we get into articles, every NASA mission keeps a list of peer-reviewed publications to justify their continued funding.  (eg, SOHO, STEREO).  The astronomy community calls them 'telescope bibliographies' and currently has a draft circulating for comment (until 24 June 2013) on Best Practices for Creating a Telescope Bibliography\n  There's also been discussion of ADS building tools for people to manage publication lists, but that's still a ways off.\n(disclaimer -- I help to manage the SOHO & STEREO web servers.  I also remember seeing a chart of Hubble papers that did/didn't include any PI team members, and a few years into the mission, the rate of publication without a PI overtook those with.  It was one of the arguments for opening data, but I'm not having luck putting my hands on that presentation)\n", "standards - Capturing development/aid project perfromance in IATI format": "\nYou attach a result to a specific activity, so in this case the Legal and Procedural Change and Communication Activity. The  should be nested within that activity. You can have many results, which are just siblings.\nA single result should look something like this:\n<result type=\"2\">\n  <title>Stakeholders reached by public outreach efforts</title>\n  <indicator measure=\"1\">\n     <period>\n       <period-start iso-date = \"YYYY-MM-DD\" />\n       <period-end iso-date = \"YYYY-MM-DD\" />\n       <target value = \"14100\" />\n       <actual value = \"43632\" />\n     </period>\n     <baseline year = \"YYYY\" value=\"0\" />\n  </indicator>\n</result>\n\nA few points:\n\nvalues should not include commas\nthe start and end dates could just be taken from the start and end dates of the activity\nthe baseline year can just be the start year of the activity.\ncheck that you are happy with these assumptions:\n\nresult type=\"2\" refers to the Result Type codelist - states that it is an outcome\nindicator measure=\"1\" refers to the Indicator Measure codelist - states that it is a unit\n\nfor the All Activities indicator, I would attach that to the parent project. Where you have \"pending\" for the actual element, you should leave the element out for now.\n\n", "Reporting non-country specific administrative spending in IATI standard": "\nI think the best way to do this is to create a new activity with these classifications:\n\nrecipient-region:\n\ncode: 998\ntext: Bilateral/unspecified\nsee IATI regions codelist\n\nsector:\n\ncode: 91010\ntext: Administrative costs\nbut maybe have a look to check there's not a more appropriate code: IATI Sectors codelist\n\n\nIf there are administrative costs related to a particular project, you could just include that in the description of the transaction.\n", "licensing - Checklist for soliciting non-identifiable data from FOSS-users for open data project": "\nDo you believe that these data can be licensed? If so, then that would imply that you should ask the user to verify that they are authorized to accept your licensing terms.\n", "data request - Is there a list, database or API that contains the URLs for United States city and town websites?": "\n(Disclaimer: I work for the U.S. Treasury but am writing in my personal capacity.)\nI do not know of an API that has what you are looking for off the shelf, but I would recommend you look into what information you can already access or request from the official system governing registration in the .gov domain. The .gov domain registration process for cities appears to be managed by General Services Administration (GSA) through the .Gov Domain Name Registration Service. (Note: GSA also manages Data.gov.) Cities appear to have to apply using a separate form. Several points of contact are given on the welcome page of the site, including registrar@dotgov.gov and a toll-free number: 1-877-REG-GOVT.\nYou may also consider suggesting that this API be made available under the question asked on this site, \"What would be particularly useful basic APIs for the US Federal Government to offer?\"\n", "legal - How should I adapt a Schema.org microdata format for legislation?": "\nI'm not aware of any list of metadata in microdata format especially for legislation. Certainly there doesn't seem to be one at schema.org, although some parts of GovernmentOrganization would be applicable.\nConcomitant with the existence of a governmental entity is, of course, the jurisdiction with which it's associated. For some uses, AdministrativeArea will serve, particularly where \"state\" is used as the primary administrative level below the country level. (Of course, even with the United States, you'd have to mislabel such entities as Puerto Rico and DC or omit them.) Purely geographical and/or geophysical schemes don't adapt especially well to identifying jurisdictions either.\nThe AkomaNtoso schema was created to provide a common structure for \"parliamentary, legislative and judicial documents.\" (Version 3.0 is being prepared and should be released this summer.) As such it provides markup for all aspects and phases of the legislative process. You could make use of the metadata elements in your own schema, insert them as RDF or RDFa (as described here), or make use of them as microdata, as described at [schema.org]{http://schema.org/docs/datamodel.html).\nThe <meta> element consists of eight different types of metadata, as shown here:\na list of meta elements http://www.akomantoso.org/docs/akoma-ntoso-user-documentation/images-akoma-user-documentation/figure-6-2013-structure-of-meta-container\nAnd then you'll have to figure out not only how you want to classify the legislation but by what mechanism you're going to select the appropriate category.\n(Affiliation disclosure -- I belong to the OASIS technical committee for LegalDocML, which is working on version 3.0 now.)\n", "usa - What finance data sets would be particularly useful to release?": "\nWorld Bank Donor Data - how donor funding is being spent by project/country/donor/implementer etc.\n", "What are the standards for data in terms of Open APIs?": "\nEvery standard is intended to serve a slightly different purpose.  Even if we're talking messaging standards over HTTP, you have both REST and SOAP.  (and before I get all of the SOAP haters commenting ... there is a ton of bad SOAP implementations, but not everything is documented oriented and meshes well with REST)\nBefore we had SOAP there were standards like WDDX for encoding your structures as XML, then XML-RPC ... so standards evolve over time, and API providers may not change their systems to support the flavor of the week.\nIf you're working within a single discipline, there may be a set of standards, but again, as they each have a purpose, the odds of there being only one is quite slim unless that discipline's data is both homogeneous, and there's only one good way to search or visualize it.\nFor instance, in Astronomy, the International Virtual Observatory Alliance has over 40 standards.  They all serve slightly different standards, for search query formation, serialization of results, etc.  And that doesn't even include all of the work on FITS (Flexible Image Transport System, a file format with dozens of registered extensions) or WCS (World Coordinate System, used with FITS and other astronomy data)\nI gave a talk at ASIS&T a few years back on standards and protocols in earth and space sciences and we're starting to recycle, converging, and otherwise simplifying things ... but it'll never completely converge, especially as people add new 'unifying' standards.\n", "government - Where can I find a list or directory of foundations, investment firms or individuals interested in financially supporting Open Data initiatives?": "\nThere are a lot of organizations that fund open data projects for various reasons. A few that have provided funding in the last year or two are:\n\nGovernments: Ministries and organizations that are looking to create a specific solution.\n\nUS government: Grants and business opportunities from the US government can be found at https://www.fbo.gov/ and http://www.grants.gov/ Challenges are found at http://challenge.gov  These change almost daily, so you can set up alerts for topics you are interested in.\n\nFord Foundation (such as http://www.webfoundation.org/2012/08/announcing-ford-foundation-ogd-grant/)\nIBM (http://www.ibm.com/ibm/responsibility/initiatives/grant_programs.shtml)\nSunlight Foundation (such as http://sunlightfoundation.com/participate/)\nMicrosoft (such as http://research.microsoft.com/en-us/collaboration/focus/cs/seif.aspx)\nGoogle (such as http://www.google.org/projects.html)\nOmidyar Network (http://www.omidyar.com/)\nWilliam and Flora Hewlett Foundation (http://www.hewlett.org/grants/grantseekers) \nOpen Society (http://www.opensocietyfoundations.org/grants)\nKnight News Foundation (you noted): https://www.newschallenge.org/open/open-government/evaluation/ \n\n", "Ethics of publicizing public data": "\nThe ethics of posting mug shot photos online has been widely discussed. Some links:\n\nLA Times: Tampa Bay mug shot site draws ethical questions\nPoynter: Archived Chat: The Ethics of Posting Mug Shots Online\nSource: Matt Waite on the ethics of a news app: Tampa Bay\nMugshots\n\nAs it's the most recent, and as it's written by Matt Waite, who built the Tampa Bay mug shot site in the first place, I'll quote this bit from the \"Source\" article:\n\nSo before you write the first line of code, ask these questions:\n   - This data is public, but is it widely available? And does making it widely available and easy to use change anything?\n\nShould this data be searchable in a search engine?\nDoes this data expose information someone has a reasonable expectation that it would remain at least semi-private?\nDoes this data change over time?\nDoes this data expire?\nWhat is my strategy to update or delete data?\nHow easy should it be to share this data on social media?\nHow should I deal with other people who want this data? API? Bulk download? Your answers to these questions will guide how you build\n  your app. And hopefully, it\u2019ll guide you to better decisions about how\n  to build an app with ethics in mind.\n\n\n", "usa - Appealing a \"Mosaic Effect\" restriction?": "\nThe answer depends very much on the legal framework of the jurisdiction where the data is released.  The question seems to be US-specific.  I do not know the legal framework in the USA, but close examination of relevant US privcy laws (which I understand are very fragmented and difficult to understand) should tell you about what means of appeal there exist in the USA (on federal and on state level).  The privacy laws of your jurisdiction should give both tell you what means of appeal there exists, and on what grounds you may make an appeal.\nHowever, since this is an international forum, and by means of example on how privacy laws work in the specific jurisdiction I am familiar with, I would like to point that in the EU and EEA, this regulated by the Data Protection Directive (Directive 95/46/EC) where each member state must set up a supervisory authority, to make decisions about, among other things, the release of data that may affect privacy.  Decisions by the supervisory authority which give rise to complaints may be appealed against through the courts (art. 28).\nIn Norway (EEA member state) the \"supervisory authority\" is known as \"The Data Inspectorate\", and there is also the \"Privacy Appeals Board\", which is the primary means of appeal for decisions made by \"The Data Inspectorate\" (decisions made by the \"Privacy Appeals Board\" may be appealed against through the courts, so there are also a secondary means of appeal available).\nAs for case law in Norway, the Mosaic Effect is often cited as grounds for not releasing data.  What usually happens is that the \"The Data Inspectorate\" requires the data to be aggregated to the point where the Mosaic effect can longer be used to indentify individuals before the data is allowed to become public.  However, aggregating data also removes information and therefore makes the data less useful, so a requirement to aggregate is often appealed against.  EU/EEA law requires the supervisory authority to weigh the privacy risks against public utility when making a decion.  In other words, if the privacy risks are low (e.g. the Mosacic effect will not expose sensitive personal data), and the public utility is high, a decision may be made to allow the data to be released without aggregation despite the Mosaic effect.  Vica versa, data where the privacy risks are high, and the public utility is mariginal, the data may not be released at all, or only released as aggregates.\n(I am a member of the \"Privacy Appeals Board\" in Norway, and has during my term of service heard several appeals where the Mosaic effect has been relevant.)\n", "usa - Open and proprietary data in determining federal funding eligibility": "\nThere are two aspects I see to this question: (1) access to proprietary data, and (2) ability to crowdsource the verification of any data.\nFor the first, proprietary data from private companies is sometimes made available as open or restricted data. Some companies' business model is based on selling this data, and some companies will offer at least a small portion of their data openly. In only very specific cases, does the government ask for data directly from companies (incorporation data, financial regulations, etc.).  However, there are some interesting examples related to your question about supermarkets:\n\nBaltimore open data on supermarkets and locations\nA blog on the value of supermarket data for consumer confidence in the UK\nAn interesting post and comments on this topic\n\nAs to the second, there are some great examples of crowdsourcing the validation of open data. USAID did so for food security and Google Earth did the same for crowdsourcing land grabbing in Ethiopia.  These both were well received and validated lots of data that would have been very difficult and costly to gather traditionally.\n", "Data about biases in city service requests": "\nI understand your question and it does relate to open data. It seems like you have a piece of open data: municipal service requests (i.e. \"fix the pothole in front of my house!\"). Your followup question is a good one: given the number of service requests, are these people just cranky, or are there actually more needed items to be fixed in a certain area?\nI searched a while for data on the responsiveness of public works departments in municipalities. Not surprisingly, this data is not collected yet in an open fashion that I could find. It is alluded to in annual reports, but even there, the numbers are sparse.\nSome alternate possibilities:\n\nTry to get the total number of requests historically, before and after the open data request system opened up.\nGet the number of \"potholes fixed\" (public works improvements?) with and without an actual service request (this is harder I would imagine, and you will need to clarify what projects are and are not included).\nGet the number/type of service requests and overlay demographics (maybe old people call more, or maybe rich people, soccer moms, etc). A lot of this will depend on how the request is geo-located (by zip, by block group, etc)\n\nAs an aside, I think it might actually be a good thing to have some sort of \"responsiveness\" metric for public works departments; maybe you could make one. Every department seems to claim that it is responsive, but I was unable to prove this by looking at their annual reports since they didn't use numbers.\n", "government - Is there a roadmap for opening all the data for a city or municipality?": "\nGreat question! Cities follow many different paths, but some best practices are starting to emerge.  Two particularly helpful guides/roadmaps are published by:\n\nOpen Data Field Guide from Socrata\nOpen Data Handbook from the Open Knowledge Foundation (available in multiple languages)\n\nBoth provide a nice how-to guide and future plans for cities and localities with open data.\n", "Open data community ideation tools?": "\nAnother avenue I'd offer for your consideration is the \"Data Jam\" model of generating great Ideations and then the companion \"Datapalooza\" showcase for spurring creation of actual products.  Boston and the NC Research Triangle are in the middle of such an experiment.\nAlso, here is a draft handbook for how anyone familiar with a Hackathon can host their own Data Jams and Datapaloozas.\n", "government - Is there (or should there be) a standard way to categorize procurement data at the municipal level?": "\nI think it would be foolish to try and replace the NAICS system. NAICS is the federal government's categorization system, and in my experience, it is also in use at the municipal level in the United States. Here's a longer description:\n\nNAICS was developed under the auspices of the Office of Management and Budget (OMB), and adopted in 1997 to replace the Standard Industrial Classification (SIC) system. It was developed jointly by the U.S. Economic Classification Policy Committee (ECPC), Statistics Canada , and Mexico's Instituto Nacional de Estadistica y Geografia , to allow for a high level of comparability in business statistics among the North American countries.\n\nIt's easy to hate on a system that only gets as specific as \"Custom computer programming services\", but in reality, it was developed with a lot of effort by a bunch of smart folks.\nIn my opinion, there are two challenges when trying to compare prices across governments:\n\nNAICS is not always specific enough, especially when it comes to technology. \nUnits are not standardized. \n\nWhile I would love to see a data standard that fixes these issues, think it's inevitable that there is always going to be a large amount of discretion/manual comparison involved. Because of this, I'd be much more interested in a system that allows a user to view aggregated pricing data from multiple cities, filtered by query or NAICS code. It would pull from multiple sources like the City of Chicago link in the original post.\nAs far as NAICS goes -- maybe we could create additional subcategories for codes that are way too broad, such as \"Custom computer programming services\"? Or taking this idea further, would there be a way to create a superset of NAICS that extends it so it never goes out of date?\nEDIT: There's a discussion going on about this at https://github.com/dobtco/NAICS/issues/1\n", "data request - Seeking real-world networks that have an approximately linear structure": "\nThe road system of New Zealand is obtainable in a computer readable form from OpenStreetMap. There are APIs available to query for just the objects you want, in this case the roads, see http://overpass-turbo.eu/s/cG for an example.\n", "documentation - Self-documenting RESTful APIs: examples with WADL?": "\nCode generation is often considered to be an anti-pattern for REST APIs: the goal is to allow clients and servers to evolve independently as much as possible. Generating client code from a WADL document, as you might do from SOAP, will make the client brittle to server-side changes. It'd be better to \"bootstrap\" the client by consuming the WADL at run-time rather than compile time.\nHaving said that, here's a couple of quick examples of WADL used on some public APIs (I've not checked the data licensing, so aren't necessarily \"Open Data\" APis):\n\nLaunchpad: https://help.launchpad.net/API/Hacking\nD&B direct: http://dnbdirectapps.com/docs/1.0/rest\n\n", "What are the most common ways that users find out about new data sets?": "\nHow to stay up to date with UK government data releases:\n\nOffice for National Statistics release calendar\nParliamentary releases mailing list  \nPlanning alerts mailing list\nPress releases\nRSS feeds\nTwitter\n\nFor example:\n\nThe Office for National Statistics release calendar is excellent because it allows you to see weeks in advance what data is going to be published. http://www.statistics.gov.uk/hub/release-calendar/index.html\nParliamentary releases mailing list lets you select exactly which committees and types of reports you want to be alerted to, and if you want them immediately or daily. https://subscriptions.parliament.uk/accounts/UKPARLIAMENT/subscriber/new?\nPlanning data has a great email alert, excellent for local data journalism.\nhttp://www.planningportal.gov.uk/inyourarea/\n\n", "What examples are there of Linked Data/RDF being used for open data applications?": "\nI would love to answer this, but I have no idea what \"strong linked data\" means.  These techniques have been used in a lot of products, the dbpedia.org system has been used by a number of systems, ranging from Watson (maybe IBM is considered too academic) to Siri (I don't think Apple is an academic group).  Schema.org and Facebook's Open Graph Protocol are also big users of linked data vocabularies and web linking schemes.\nGoing to government, there's been a lot of work using linked data in various ways.  The Brits are the lead, a number of their open sites, based on the Ordnance Survey maps among others, use linked data.  Within the US, we have demonstrated a lot of uses at hackathons, in some of the competition winning apps, and in some the info sources available on line.  \nThere's an article Jeanne Holm, George Thomas, Chris Musialek and I wrote that covers some of this - IEEE Intelligent Systems and some thoughts about what we are doing at data.gov \nSo my fast summary is that like many technologies this is being used as a component in many apps, it is not by iteself proposed or fielded as a be all and end all - but without the URIs, the data doesn't make it to the Web, and then we cannot exploit many of the powerful things that open data allows.  \n(I have a column in the soon-to-be-released issue of Big Data called \"Peta vs. Meta\" that says a bit more about how the schema.org stuff is used - see also schema.org/Dataset)\n", "Are there best practices about data lifecycle management involving citizens?": "\nOn Data.gov we have a several different ways for people to provide feedback, most of which are publicly viewable.\n\nSuggest a new dataset and see what others have suggested (all the dataset suggestions are tracked to completion, although not all requested datasets exist or can be released)\nComment on or rate a dataset (see an example for earthquake data)\nAsk a question in our Developer's Community (these are moderated for spam)\nSend an email\nHost or participate in events (see a listing on our community page)\n\n(Disclaimer:  I serve as the Evangelist for Data.gov)\n", "What companies, projects, and researchers are using the Consumer Financial Protection Bureau (CFPB)'s API of data on consumer product complaints?": "\nOur team at Beyond the Arc makes extensive use of the CFPB database.  You can see our latest analyses on our blog.\n", "best practice - Displaying Trust and Data Provenance?": "\nMy issue would be what the purpose of displaying the provenance is.\nAs I've suggested in some of my questions on here, some of my concerns are about tracing the issues that might be in the data, and sometimes you have to go back and look to see how it's been processed and what it's derived from to tell what the possible issues might be.\n(eg I've run into at least once case where the folks calibrating the data hadn't considered big vs. little endian when they converted from 68000 to PPC processors ... the issue wasn't caught 'til they went from PPC to intel and realized that all of the PPC processed data (years worth) was defective. ... but architecture the processing was run on isn't always captured when people talk about provenance)\nPersonally, I'm of the opinion that for the sake of open data, you shouldn't just share the provenance of the data set in question, but you should try to describe that data's relationship with all of the rest of the data that you serve.  (rule #4 in \"5 star data) After all, it might be that the data that someone found has some more processed form that would be better for their needs ... or formatted / packaged differently.  ... and you're not going to get that from only tracking provenance, as that only goes in one direction.\n(disclaimer : years ago, I gave a talk at the AGU on the need for a model to discuss relationships between data (warning: 18MB PPT file) ... unfortunately, I've gotten bogged down with other stuff for the past few years ... but the DataCite schema has RelatedIdentifier and a decent list for relationType to get people started)\n", "data request - Are there any open mappings of train station identifiers in the UK?": "\nThere are a number of codes used to identify train stations. There is a good summary of the various codes here:\nhttp://nrodwiki.rockshore.net/index.php/Identifying_Stations\nThat site includes links to reference that that you may be able to use in addition to NAPTAN, see:\nhttp://nrodwiki.rockshore.net/index.php/Reference_Data\nFor example this site correlates CRS, NLC, TIPLOC and STANOX codes and has a MySQL dump available:\nhttp://trains.barrycarlyon.co.uk/data/locations/\nI think that covers what you're looking for.\n", "metadata - How can I track updates on the release of new open data sources across the world?": "\nThere are two additional options here. \n\nIf you know of an open data portal for a country or topic you are interested in, many allow you to subscribe to their data updates or releases.\nIf you do not know that a source exists and are looking for one, several aggregator sites are referenced in a recent answer to your related questions on international aggregators for NGO data.  Additionally, on Data.gov we track official government open data portals, and I can certainly look into creating a subscription feed there.\n\n", "data request - Wikipedia dump files in SQL format": "\nNo, there are no SQL versions of the XML dump files.\nThe page Data dumps/Tools for importing on meta.wikimedia.org describes how to work around that: You can either use ImportDump.php to import the XML file directly (apparently suitable only for small wikis), or you can use a tool like mwdumper to convert the XML into SQL and then import that.\n", "Evidence for the economic impact of open data?": "\nThere are several studies on the economical impact of Open Data. The most recent I know of is a study done in 2011 on data held by all public bodies in the European Union called Review of Recent PSI Re-Use Studies Published [docx] (PSI stands for Public Sector Information), the study is also know as the Vickery study. One of its main findings is that the EU's current usage of PSI results in \u20ac30 billion of economic activity and that opening up more PSI could increase this to \u20ac70 billion.\nMentioned in the Vickery study are amongst others:\n\nMeasuring European Public Sector Information Resources (MEPSIR), 2006\nCommercial exploitation of Europe\u2019s public sector information (PIRA), 2000\n\n", "Tales of woe from fixed-format (non-delimited) ASCII data distribution": "\nI gave a talk many years ago (2008) at the joint AGU/SPD meeting on problems trying to read catalogs, but I don't know that it's directly applicable, as many of the cases were due to manually maintained files that were expected to be read by humans, not machines.\nOur community does have a standard for documenting the files in a language-independant way (example), but some of the problems that I ran into was that the documentation was just flat out wrong -- the documentation was written years before, and someone changed the table format without updating the docs.\nI did keep some other documentation of the issues that I ran into with trying to read catalogs, but I instead formatted it as a checklist of recommended practices, not as specific 'tales of woe'.\n", "What does OpenRefine offer that other data-parsing tools don't?": "\nI'm a programmer and I use OpenRefine all the time.  Some of the advantages it has over breaking out Python or some other language include:\n\nresults of transformation expressions are previewed interactively with live data\nquick, interactive, filter facets which allow for easy browsing of instances/rows which match a variety of filters\nexploratory analysis of data to do quick visualization via facets and explore interactions among columns\nreconciliation of text data against reference data services containing strong identifiers (Freebase, OpenCorporates, any SPARQL or RDF, etc)\nsimple linking of reconciled entities to other info sources like Wikipedia, MusicBrainz, IMDB, etc\ncomplete provenance/undo history of all modifications\ncombination of machine smarts and human review for tasks like clustering of names.  \nwide variety of input & output formats including both file formats and online repositories like Google Spreadsheets & Fusion Tables\none click selection of record boundaries to produce a grid of data from a JSON or XML API is great for exploring new API endpoints\n\nAnd I'm finding more cool stuff all the time.  Just the other day I discovered that the scatterplot facet rotates 45 degrees allowing me to select any area on the diagonal of an OCR character accuracy vs OCR word accuracy scatterplot to investigate in more detail.\nDisclaimer: I'm the project lead for OpenRefine, but most the good stuff was done by the original author David Huynh.\n", "calendar - Are there any regular Open Data conferences?": "\nConferences:\n\nTransparencyCamp, by the Sunlight Foundation, annually since 2009.\nOpen Knowledge Conference (OKCon), annually since 2005. Open data has been central since its inception - in 2012 this expanded to be the Open Knowledge Festival (OKFestival).\nOpen Government Data Camp, by the Open Knowledge Foundation, annually since 2010. From 2012 the camp has been merged with the Open Knowledge Conference.\nEuropean Open Data Week, annually since 2012.\nEuropean Public Sector Information Platform (ePSI) Conference, annually since 2012.\nEuropean Data Forum, focused on Linked Data, annually since 2012.\nHealth Datapalooza, focused on U.S. open health data, annually since 2010\nNational Day of Civic Hacking, annually since ~2010\nInternational Open Government Data Conference, started in 2010\nOpen Data Day is an annual event since 2009\nOpen Data Exchange started in 2013, and will become an annual event.\nThe Computer-Assisted Reporting conference (often referred to as NICAR) is held annually by Investigative Reporters and Editors and focuses on obtaining and using open data in a journalism context.\nIEEE Big Data, a journal on big data which includes calls on open data\nOpenSym, OpenSym includes a track specifically for open data\nOpen Data Science Conference began in 2015, held several times each year (i.e. 2-4) on both the east and west coasts of the U.S. as well as internationally (e.g. Kiev, Bangalore, Tokyo, etc.).  Emphasis on data science but also touches on open ideas, software, and data.\n\n(people might want to expand this community wiki)\n", "What is the major difference between Open data and Linked data?": "\nData can be open but not linked, or linked but not open.\n\"Linked data\" refers to data that is machine readable, semantic data, that a machine can 'understand'.  The \"semantic meaning\" comes from the links, hence the names.  \"Open Data\" refers to data that is accessible to anyone (e.g. without monetary cost to access) with a permissive license on reuse (e.g. public domain or CC0).\nDevelopment in linked data usually focuses on tools that provide meaning to data as microdata, RDFa, or RDF, and ontologies that provide meaning of terms.  Open data focuses instead on tools that allow users to access the data conveniently, focusing on tools such as RESTful APIs, and formats that allow a user to query and subset data such as JSON or XML.\nTim Berners-Lee suggested this five star rating system to help think about linked data:\n\n\u2605 make your stuff available on the Web (whatever format) under an open license\n\u2605\u2605    make it available as structured data (e.g., Excel instead of image scan of a table)\n\u2605\u2605\u2605   use non-proprietary formats (e.g., CSV instead of Excel)\n\u2605\u2605\u2605\u2605  use URIs to denote things, so that people can point at your stuff\n\u2605\u2605\u2605\u2605\u2605 link your data to other data to provide context\n\nIn Tim's system, five-star linked data has to be open (first star, also open format, third star); The linking is really in stars four and five.\nBy contrast, most open data systems provide the first three stars (e.g. a RESTful API providing data in JSON format), but don't necessarily hit the fourth and fifth star.  From this we might surmise that all linked data is open data, while all open data is not linked data.\nOne could arguably claim that they provide linked data (URIs, links to other data) behind some proprietary firewall for internal use only, and thus it is not open data.\n", "data request - IPA phonology database": "\nMight want to double check the license, but the baseline standard is the CMU Pronunciation dictionary, which is freely downloadable and also ships with many NLP libraries, like NLTK (python).\nFor out-of-vocabulary words, I've had great success with Sequitur G2P, which is both trainable and under the GPL.\nedit: note that CMUDict (and many other speech processing pipelines) represent pronunciation in ARPAbet. I apparently don't have enough points to post more links, but google \"FAVE ARPABET\" and you'll get a handy cheat sheet.\nedit 2, in response to OP's edit:\n\nConverting from arpabet to IPA is deterministic, so again, wikipedia is your friend as long as broad transcription is acceptable (see note below) \nDepending on the language, you may not need a pronunciation dictionary. german, japanese and korean are examples of languages that have a deterministic mapping of grapheme to phoneme. english orthography is a hideous mutt of historical accident, so sometimes there's really just no way to tell how a word will be said without just memorizing it. french is horrible, too. i'm not sure about arabic. i'd ask people who do automatic speech recognition in your target language (googling should bring you some researchers' homepages)\n\n\"note below\": 99.99% of the time, in real-world engineering usage, it is. IPA transcription can get insanely narrow, describing phonetic attributes things like aspiration, specific articulatory gestures, etc that don't \"exist\" in a speaker's conscious knowledge of their language because they're not phonemic, meaning that they can't be used to signal the difference between words with two different meanings\n", "usa - Does any US Government agency (like FDA) publish a list of approved food products and ingredients?": "\nThe USDA maintains a National Nutrient Database with \n\nnutrient information on over 8,000 foods using this new and improved\n  search feature. You can now search by food item, group, or list to\n  find the nutrient information for your food items.\n\nThe Nutrient Data Laboratory gives food composition and allows you to browse foods by nutrient. \nFDA 'approval' of food products is somewhat less straight forward- see What does FDA regulate? section on food. \n", "geospatial - Looking for Open Data Source to Correlate Address to Latitude/Longitude (geocoding)": "\nOpenstreetmap has an API which gives coordinates for an address, see Stackoverflow for an example\n", "data request - Is there a list, database or API that contains the all the product information in India": "\nFor the ISBN's, you may find interest in Worldcat. It allows searching with ISBN codes, and by language. Hindi is one of the searchable languages. They also offer an API, but to perform unrestricted search requests you have to be affiliated to a Library, otherwise it's free. The other alternative is also to write your own API that scraps the results off web pages.\nAs for the UPC codes, there is the UPC database that has millions of items (though, not sure how many Indian UPC codes there are). Otherwise you can also use the Google Search API for Shopping, which is deprecated, but offered until September 2013. To search by UPC codes, you have to use the gtin parameter in the URL. \nIf you are interested, read this article about the GTIN - Global Trade Item Number, which explains how it is related to UPC. If you are not, I'll make it short, the GTIN-12 format is UPC.\n", "data request - English news dataset for sentiment analysis": "\nCould you explain more about what you need the data for? I'm not aware of any pre-built data sets, but you could attempt to construct your own. You'll need to break the problem into two parts though.\nThe easiest route to identifying the entities is the OpenCalais API, which despite its name is a closed-source service, but has generous usage limits. You can also look at the American National Corpus, which contains a large number of automatically-tagged entities in an open data set.\nYou'll then separately need to figure out the sentiment associated with each entity, which is still an AI-complete problem to do totally accurately, especially in an example like yours where it would require understanding the meaning of the sentence. Most sentiment analysis techniques look at the frequency of particular words or small sequences of words, you can find a good overview of the algorithms here, along with some datasets matching words with their sentiment. \n", "Where can I find data on sales of celery varieties in Europe?": "\nEurostat is a European institution which collects data from its member country's statistical institutions and gathers data itself, amongst others stats on the production of vegetables. Here are the stats for celery and celeriac production across all EU countries. A summation across all countries for the years 2000-2011 shows the following production in 1000 tonnes:\n\ncelery: 3194.9 \nceleriac: 3817.2\n\nIt is interesting to see though that the production data provided by Eurostat on The Netherlands differs in some cells from the Statitistics Netherlands (CBS) data.\n", "data request - Is there a global database of all products with EAN 13 barcodes?": "\nThe Open Product Data project is a comprehensive source for open barcode data. As of May 2014, they have close to a million products in their database. The data is accessible online, through an Android app (source code), and available for download under the Open Data Commons Open Database License (ODbL).\n", "data request - Geolocation - UK places with 100,000 people within a 30 mile radius": "\nFrom this Q&A at GIS SE (Worldwide population density data not grouped by country) (updated by me to reflect link changes):\n\nOne of the best gridded data sets is CIESIN's Gridded Population.\nSee Gridded Population of the World (GPW), v3 for more details. The best resolution is 30 Arc seconds (The global data set has resolution of 2.5 arc minutes - Deer Hunter).\n\nTo find the places, have a look at this question at GIS SE: Algorithm for finding population for a given center point and radius in US\nIn general, I would advise looking for geospatial data at GIS Stack Exchange before going here. The chances are quite high the question has already been asked and answered there.\nEDIT: through gracious assistance of ldodds, UK-specific reverse geocoding facility (at Ordnance Survey) has been identified:\nhttp://beta.data.ordnancesurvey.co.uk/datasets/os-linked-data/explorer/search\nwith documentation available at http://beta.data.ordnancesurvey.co.uk/docs/search. This would be the last stage in the processing pipeline.\n", "usa - Ownership of US county property tax/assessment data?": "\nIn the USA there are no sui generis/fruits of labour provisons protecting data.  While compilations of data may be protetected if the author has used creativity with respect to which facts to include, in what order to place them, and how to arrange the collected data so that they may be used effectively by readers (re: Feist vs. Rural telephone), this (very mariginal) copyright protection afforded collections of data in the USA will not be relevant when you create a new collection reusing current and historical property tax data.\nIn addition, most states in the USA regard works (which may be collections of data) that has been compiled by the agencies of government or its subdivisions, the property of the people. For detail, see Wikipedia about copyright of US state governments.  This is probably not relevant to you, as transformatory use of state tax data is not under any circumstance breech of copyright in the USA, but gives you extra assurance that it is legal for you to reuse and aggregate this data for the purposes you describe.\nBut to make sure (never rely on legal advice on the Internet), you may contact the agencies of government or its subdivisions that releases the data you plan to aggregate, and ask if they claim ownership to it.\n", "Does there exist an authorative definition of an open dataset?": "\nYes, there is: the Open Definition defines openness for data (and content). The Definition was produced in 2005, heavily based on the Open Source Definition, and revised minimally since.\nThe key part of the Open Definition states:\n\nA dataset [work] is open if its manner of distribution satisfies the following\n  conditions, which simultaneously delimit the characteristics of a suitable\n  open license:\n1. Access\nThe work shall be available as a whole and at no more than a\n  reasonable reproduction cost, preferably downloading via the Internet\n  without charge. The work must also be available in a convenient and\n  modifiable form. The license may require the work to be available\n  in a convenient and modifiable form.\nComment: This can be summarized as 'social' openness - not only are\n  you allowed to get the work but you can get it. 'As a whole' prevents\n  the limitation of access by indirect means, for example by only allowing\n  access to a few items of a database at a time. An example of 'reasonable\n  reproduction cost' is the cost of a blank DVD required to\n  distribute a complete database.\n2. Redistribution\nThe license shall not restrict any party from selling or giving away\n  the work either on its own or as part of a package made from works from\n  many different sources. The license shall not require a royalty or\n  other fee for such sale or distribution.\n3. Reuse\nThe license must allow for modifications and derivative works and\n  must allow them to be distributed under the terms of the original work.\nComment: Note that this clause does not prevent the use of 'viral'\n  or share-alike licenses that require redistribution of modifications\n  under the same terms as the original.\n...\n7. No Discrimination Against Persons or Groups\nThe license must not discriminate against any person or group\n  of persons.\nComment: In order to get the maximum benefit from the process, the\n  maximum diversity of persons and groups should be equally eligible to\n  contribute to open knowledge. Therefore we forbid any open-knowledge\n  license from locking anybody out of the process.\nComment: this is taken directly from item 5 of the OSD.\n8. No Discrimination Against Fields of Endeavor\nThe license must not restrict anyone from making use of the work in\n  a specific field of endeavor. For example, it may not restrict the work\n  from being used in a business, or from being used for genetic research.\nComment: The major intention of this clause is to prohibit license\n  traps that prevent open material from being used commercially. We want\n  commercial users to join our community, not feel excluded from it.\nComment: this is taken directly from item 6 of the OSD.\n\n(Disclosure: I helped draft the first version of the Open Definition and have helped curate it since along with other members of the Open Definition Advisory Council)\n", "data request - Is there an open movie and/or music database available for commercial use?": "\nI'm chosing to answer this from the perspective of \"what open datasets are there for movies/songs\".\nIts worth noting that IMDB and MusicBrainz offer commercial usage agreements, assuming you're happy to pay.\nMusicBrainz is an excellent starting point for music metadata. The Core data, which covers the artists, releases, songs is all in the public domain under a CC0 license. It is only the additional \"supplementary data\" that is published in a CC-BY-NC license.\nMore information on that here:\nhttp://musicbrainz.org/doc/MusicBrainz_Database\nOpen Data for movies is more scattered. Wikipedia contains a lot of data on movies, actors and directors, all of which should be available in Dbpedia.\nDbpedia is available for use under an Open license.\nI think all other sources of movie data will likely require you to pay for some kind of commercial usage. \n", "usa - Is there data on the types of cars bought and turned in during the \"cash for clunkers\" program?": "\nThe stats seem to have been quoted on a number of sites. The official name of the program was the \"CARS program\". You can find additional statistics on the transactions here:\nhttp://www.nhtsa.gov/Laws+&+Regulations/CARS+Program+Transaction+Data+and+Reports\nThe \"Final Paid Transaction Database\" appears to be what you're looking for. It's available as a Microsoft Access database or a CSV file (described as 'text' on the site.) Here is the list of columns in the CSV file (as generated with csvcut -n)\n  1: vendor_id\n  2: dealer_name\n  3: address_line1\n  4: address_line2\n  5: address_line3\n  6: address_line4\n  7: city\n  8: state\n  9: ZIP\n 10: area_code\n 11: phone\n 12: invoice_id\n 13: invoice_num\n 14: invoice_date\n 15: sale_date\n 16: disposal_status\n 17: disposal_facility_nmvtis_id\n 18: disposal_facility_contact_info\n 19: sales_type\n 20: invoice_amount\n 21: trade_in_VIN\n 22: trade_in_vehicle_category\n 23: trade_in_make\n 24: trade_in_model\n 25: trade_in_year\n 26: trade_in_vehicle_drive_train\n 27: trade_in_mileage\n 28: trade_in_title_state\n 29: trade_in_registration_state\n 30: trade_in_registration_start\n 31: trade_in_registration_end\n 32: trade_in_insurance_start\n 33: trade_in_NMVTIS_flag\n 34: trade_in_odometer_reading\n 35: new_vehicle_VIN_trunc\n 36: new_vehicle_category\n 37: new_vehicle_make\n 38: new_vehicle_model\n 39: new_vehicle_year\n 40: new_vehicle_drive_train\n 41: new_vehicle_car_mileage\n 42: new_vehicle_MSRP\n\nThe New York Times posted some summary statistics:\nhttp://wheels.blogs.nytimes.com/2009/08/26/the-final-numbers-on-clunkers/?scp=3&sq=cash%20for%20clunkers&st=cse\n", "How can a data governance framework be adapted for Open Data?": "\nIf you're talking data governance frameworks, I think you'll want to look at the Data Management Body of Knowledge (DMBOK) published by DAMA International. The functional point of view in DMBOK is pretty comprehensive. As Federal government guidelines on roles and responsibilities go, the best baseline framework is what the Office of Management and Budget (OMB) and agencies put together in response to the information quality act. OMB's guidelines are available here: http://www.whitehouse.gov/omb/fedreg_final_information_quality_guidelines\nEvery federal agency was required to develop information quality guidelines, but here's an example from the US Department of Transportation: http://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/subject_areas/statistical_policy_and_research/data_quality_guidelines/index.html\nThe guidelines generally ponder a data quality administrator and data quality officials matrixed across the organization. DOT adapted most of that goodness into its data dissemination policy, DOT Order 1351.34. Which you can find using a google search because I don't have enough reputation points to link to it. :-)\n(Disclaimer: I am a contractor at the US Department of Transportation)\n", "data request - US Government API Usage": "\nThere is a growing catalog of APIs from the federal government available at Data.gov's Developer community.  Activity on each dataset will be made visible in the activity stream area (not yet visible as the new catalog was just created last week).  In the meantime, you can see API activity as follows:\n\nEPA Envirofacts API: dataset page and activity stream\nPrevious metrics (temporary solution) until the activity streams are populated\n\n(Disclaimer: I am the Evangelist for Data.gov.)\n", "data.gov - What are the data quality measures for open data?": "\nI think the question, as phrased, is impossible to answer well, but I will try.\nQ: \"How does a consumer know they are getting good data?\"\nA: Let me answer with more questions. How does a consumer know they are getting a good search result from Google? How do they know when the news is of high quality? It depends. As consumers get more interested and informed about something, they do better. The most savvy and informed consumers will compare a data set against a known source. Others have to rely on some degree of trust.\nQ: \"Are there standard frameworks for grading the quality of an open data set?\"\nA: In practice, there are defacto standards for metadata. For example, data.gov uses Dublin Core along with additional attributes. CKAN has many of the same attributes.\nAlso, for each type of data (or subfield) there are often industry standards or at least conventions. Good luck enumerating those!\nA post from the Sunlight Foundation, Government Data Sets - Managing Expectations is a high-level gloss; it breaks down \"dataset quality\" into provenance, data quality, responsibility, maintenance, and documentation.\nThe above article is somewhat naive; the quality of a data set is not an independent thing. As Wikipedia - Data Quality points out, the quality of a data set  depends on the question asked of it. There is no \"one\" measure of data quality. Rather, there is a subjective 'appropriateness' for each question you might ask of a data set. You can't ignore the subjective nature of data quality.\nQ: \"Should there be metrics published around accuracy, completeness, timeliness or validity of the data?\"\nA: There are advantages to doing so, sure, but there are costs too. Who provides the resources to do it? This question cannot be answered well in the abstract. This is a question of leadership and resources. If you want it, act. Advocate for it. Or do (hack, write, whatever) something for your city, country, state, province, or country.\nQ: \"Should there be a minimum set of controls on the part of the publisher?\"\nA: Maybe. This is complex.\n\nPerhaps it is smart to release the data sets you have, regardless of quality, in whatever format you have available.\nPerhaps later, over time, and perhaps with incentives, improve them and/or convert to better formats.\nIncreasing standards of data quality may act as barriers to publishing data; reducing your data inventory. This may be good or bad.\nSome data releases may be criticized in any number of ways; for reasons in or out of your control.\nData may be used in ways that the government or collecting agency does not agree with.\nBeware of publishing data that may be sensitive. It is difficult to anonymize data in the general case.\n\nIn summary, this is a hard problem with many pitfalls. Even well-meaning organizations may need significant prodding to make data releases happen.\n", "usa - Sources of political voting records at the county level?": "\nThe open elections project is trying to collect this type of information from official sources, though it isn't yet available.\nThis is commercially valuable information that typically comes with a licensing fee and can be problematic for republishing. \"Election Data Services\" is one of the usual sources for this data. Also see Dave Leip's political atlas store where a site license for 2012 county-level presidential results appears to be about $200. \nThis data doesn't exist for ZIP codes, of course; they don't match polling districts. \n", "best practice - How to publish real-time open data": "\nYou have a few options for real time (or \"near real time\", which is when you have a delay between the collection & time to serve it, or for those that sample at a lower cadence)\nThere are a lot of considerations when dealing with 'real time' data:\n\nWho is the intended audience?  (and do they already have standards for serving this type of data?)\nIs the data of value over the long term, or only ephemerally?  \nIf the data is of value long term, at what cadence does it need to be at to be of value?  Does that cadence change as the data ages?\nIf the data is ephemeral, what is the maximum age for which it's of value?\nWhat is the maximum latency acceptable for the data?\nHow large is each record / observation?\nHow would the intended audience expect to work with the data?\nHow many people are going to be calling this API?\nIs the data useful individually, or only as part of a larger sensor network?\n\n... I'm probably missing a few things ... but without knowing the answers, I can't say if it makes sense to :\n\nhave an API that queries the sensors in real time\nhave the sensor report in every few minutes and serve the latest value\nhave some sort of data logger and serve the last 10 or 100 values\nlog to a database and have a protocol for requesting data from a given period of time\nappend each entry to a log, and roll the log every day (or hour, or some other time period).\n\nIf you're just looking to put up a few graphs on a website to show how much sunlight you get ... then just use RRDTool or something similar.\nIf you're trying to contribute back to a citizen science project ... you could look into the National Weather Service's Cooperative Observer Program.  I don't know if they deal with solar irradiance though.  DOE does, as they have their Solar Energy Potential map, but I don't know how they get the input for it.  You can also try to find if there are any local mesonets in your area that need a node near you.\nAlso be aware that for some types of real time and NRT (near real time) data, there are APIs for reporting significant events, and interested parties connect and subscribe to the types of alerts they're interested in.  Or individual sensors report back to some central clearing house, and people can then query that for data of interest.  For instance, in astronomy there's the Gamma-ray Coordinates Network, for US earth science data, there's the NOAA Observing System Architecture.\n", "data request - Are there any open datasets with technical specifications for photographic equipment?": "\nI think it's semi-abandoned, but there are various levels of details for 1,300 models here: https://www.freebase.com/digicams/digital_camera?instances\nIf nothing else, the schema might provide a starting point for informing the types of information to collect.\n", "data request - Open database APIs for journal article metadata": "\nThere are several different potential sources of information. I don't think any are completely comprehensive and few would count as strict \"open data\": apart from Open Access titles, licensing is likely to vary between publishers.\nHaving said that you could look at some of the following sources:\n\nSpringer API\nNature Linked Data Platform\nPubMed API\nPLOS API\nArXiv API\nCrossRef Search API -- this would be a good starting point\n\nEssentially there are publisher specific APIs, subject aggregators, and a few cross-industry services. CrossRef as a DOI registry are a good starting point.\nIts worth noting that given a DOI, you can now get structured metadata about the article using content negotiation, i.e. a simple HTTP request. This includes all CrossRef and DataCite DOIs. More information, including examples.\n", "tool request - Keeping track of updates to open data published as CSV": "\nYou might want to consider maintaining your cleanups as a set of operations or diffs which get applied to the source data.  This would help isolate you from changes to the source and allow you to reapply them to a new dataset.\nOpenRefine maintains a history of operations, but you could do something similar with a set of version controlled scripts in your favorite scripting language.  You also might be able to use a set of patch files produced by something like DiffKit.\n", "data request - Sources of topograpical maps for use in LaTeX documents": "\nHave you had a look at openstreetmap?\nhttp://wiki.openstreetmap.org/wiki/Osmbook\nNot tested myself, they use Latex to generate the book.\n", "Best practices for huge explorable linked data directories": "\nI would suggest:\nWhen dereferencing the root URL, point to a metadata document (via RDFa and/or conneg), say http://yoursite.com/meta. When dereferencing this document, provide a description of datasets using DCAT. every URI there, when dereferenced can show metadata (file size, creation dates, etc) and include a dcat:downloadURl link with the actual data. I'm not sure about the paging issue, but all of the above can be done in say, apache + flat files. Of course there are other more sophisticated solutions but I think this pretty much covers most of what you ask for.\n", "Is there a site where USA road traffic historical data would be available?": "\nThere are a lot of datasets related to road safety and general patterns.  Here you can find many of the National Highway and Department of Transportation datasets and ones specific to traffic in various locations.\n(Disclaimer: I am the Evangelist for Data.gov.)\n", "usa - Where can I find U.S. train traffic data?": "\nAs a supplement of open data, the Federal Railroad Administration in the U.S. provides data on incidents, casualties, and a listing of the rail crossings. FRA also provides geospatial data on the location and maps of rail networks.\nIt could be possible to interpolate train traffic between two grade crossings as well. You could experiment with the FRA grade crossing data. The grade crossing file captures day and nighttime movement counts, as well as crossings where there is less than 1 train per day. The specification is available.\n", "usa - Data Source for Speed Limits": "\nThe best source at a broad level is from the Federal Highway Administration.\nThere is a also variety of open data on traffic safety. Some of this has embedded speed limit information as well as other information, such as fatalities and accidents. There are also some state speed zone data zones, such as those for Virginia.\nThere was a question on another Stack Exchange about this as well: \"you can derive an approximate max speed limit by looking at the national speed limit for the type of road in the country of interest\" derived from OpenStreetMap.\nThe DOT National Transportation Library has FAQs about the National speed limit and speed limit laws. Speed limits are governed by state and local jurisdictions. Some roadway data from a subset of states is made available through the Turner-Fairbank Highway Research Center (TFHRC) (or pull a data request). Reference librarians are available to help with specific data requests.\n(Disclaimer: I am the Evangelist for Data.gov. A contractor with Dept. of Transportation also provided information for this answer.)\n", "data request - Open Web Crawling Dumps": "\nIf the crawled data doesn't need to be very recent, the Internet Archive provides 80 terabytes of archived web crawl data from 2011 for research. Unfortunately, they don't say under which license they release the data, so it might not be Open Data as defined by the Open Definition.\n", "Standards for capturing organisational data like budgets, procurement, salaries": "\nXBRL is a standard derived from XML that is gaining momentum for describing financial and business transactions. A good definition and set of practices is represented by the Security and Exchange Commission.  It provides examples, APIs, and other technical information for accessing data from the SEC and for machine readable financial data using XBRL.  The site notes, \"The Commission also has published final rules requiring certain nationally-recognized statistical rating organizations (NRSROs) to provide rating information on their websites in XBRL format.\"\n", "business - Open Address Data for Restaurants": "\nOpenStreetMap has quite an easily accessible database of restaurants (and other places), which you can easily query using their Overpass API. An example query for Overpass's Query Form which gets all restaurants in greater London:\n<query type=\"node\">\n  <has-kv k=\"amenity\" v=\"restaurant\"/>\n  <bbox-query s=\"51.28\" n=\"51.686\" w=\"-0.489\" e=\"0.236\"/>\n</query>\n<print/>\n\nAn easier way of creating a query is to use Overpass Turbo, which allows you to navigate a map to reflect your area of interest, again an example of all restaurants in greater London (click on Run and move to the Data tab on the upper right to see the 'raw' data).\nAnother source is OpenCorporates, which often holds official registrations of companies and whether they are active or not. Many restaurant registrations don't have their address listed, but you might want to use their data to find out whether restaurants are still active or not (see their video on how to use Google/OpenRefine to reconcile names from a certain dataset (in this case OpenStreetMaps) with the OpenCorporates dataset).\n", "releasing data - Preservation of blog posts, articles and essays": "\nGreat question.  I agree that permanent archiving of blog posts and other digital content is an important challenge in open data. It might be helpful to break this down into parts:\nPersistent identifiers\nHaving a persistent address at which potential users/machines can reference your content is crucial to good archiving, and most of the issues you list refer to this (losing your domain name, provider going bankrupt, etc.)  One promsing way to address this is by registering a PURL or Persistent uniform resource locator.   PURLs are widely used for major web ontologies like Dublin Core: http://purl.org/dc, to make sure these resources have consistent links.  Users must register and they are relatively easy to set up (for instance, I have registered http://purl.org/cboettig as a partial redirect to each of the pages in my online research notebook. If you are familiar with DOIs for published literature these are largely analogous technology.  \nArchiving content\nMaking sure your link always resolves is not the same as guaranteeing your content continues to exist.  Robust, geo-politically distributed archiving services like CLOCKSS or LOCKSS are probably the gold standard here, but not accessible to individual authors.  Having your own distributed backup copies on public repositories is still a good idea.  Depositing copies in an appropriate repository, such as fig*share* for scientific research content, is one way to achieve this level of archiving.  \nTools like Git/Github can help archive the version history of your content, not just the most recent version.  \nGood metadata\nHaving good machine-readable metadata on your site will help search engines index it accurately and can help users/machines actually make use of it.  Consider identifying author, titles, dates, tags, and other such data using RDFa, though even vanilla HTML5 has quite a few semantics available.  Some examples here\nYou've already mentioned good licensing, which is key in making your posts useful as open data.  See that the license information is properly embedded in machine-readable metadata as well. \n", "usa - Which, if any, U.S. Federal Government agencies offer a Service Level Agreement (SLA) for their APIs?": "\nSLAs suggest a quality of service that costs money to maintain ... in the business world, you have to pay to get an SLA.\nThe only time that I've been involved with something approaching an SLA would be an MOU (Memorandum of Understanding) between two federal agencies.  In some cases,  the agency requiring a given level of service helping to pay for the upkeep of the network and processes that go into making the data available ... in others, it's just an agreement that the connection to retrieve the data exists, with no guarantees on availability.\n(Disclaimer : I work for the Solar Data Analysis Center, but I'm not the 'Joseph' listed on their website; I'm also not involved with these MOUs, other than knowing that they exist (and which hosts not to block if they start acting up))\n", "What's the difference between Open Data and Big Data": "\nThey are not the same at all. Datasets are Open if they are available under a free license to everyone. Datasets are Big if... well, they are. Typically big beyond where common software can handle them in real time. \nFor example Facebook and Google work with Big Data that is not Open. \nMost Open Data sets are actually an example of Small Data: The datasets themselves are not huge, but there is a large number of them that can be correlated to increase their value. \n", "data request - Finding an index of food prices": "\nIt sounds like you want Consumer Price Indices for Food and Beverage for various metropolitan areas. The Bureau of Labor Statistics calculates just such indices: http://download.bls.gov/pub/time.series/cu/cu.txt\nMetropolitan Areas\narea_code   area_name   \nA101        New York-Northern New Jersey-Long Island, NY-NJ-CT-PA1  \nA102        Philadelphia-Wilmington-Atlantic City, PA-NJ-DE-MD\nA103        Boston-Brockton-Nashua, MA-NH-ME-CT\nA104        Pittsburgh, PA\nA207        Chicago-Gary-Kenosha, IL-IN-WI\nA208        Detroit-Ann Arbor-Flint, MI\nA209        St. Louis, MO-IL    \nA210        Cleveland-Akron, OH\nA211        Minneapolis-St. Paul, MN-WI \nA212        Milwaukee-Racine, WI\nA213        Cincinnati-Hamilton, OH-KY-IN   \nA214        Kansas City, MO-KS      \nA311        Washington-Baltimore, DC-MD-VA-WV       \nA316        Dallas-Fort Worth, TX       \nA318        Houston-Galveston-Brazoria, TX  \nA319        Atlanta, GA     \nA320        Miami-Fort Lauderdale, FL   \nA321        Tampa-St. Petersburg-Clearwater, FL     \nA421        Los Angeles-Riverside-Orange County, CA \nA422        San Francisco-Oakland-San Jose, CA  \nA423        Seattle-Tacoma-Bremerton, WA    \nA424        San Diego, CA   \nA425        Portland-Salem, OR-WA   \nA426        Honolulu, HI        \nA427        Anchorage, AK       \nA429        Phoenix-Mesa, AZ        \n\n", "usa - What level of government/governing agency determines the terms of use for transportation map/gis data in the U.S.?": "\nThe national governing body for geospatial data in the US is the Federal Geographic Data Committee.  Data provisioned through the FGDC is part of the overall Data.gov corpus of data.  Data provided by federal agencies (such as NASA and NOAA) are provided without charge and without restriction. (Note the data policy that states \"Data accessed through Data.gov do not, and should not, include controls over its end use.) \nHowever, data provided by other organizations, such as the Milwaukee city data you referenced, are governed by that entity and may vary. Non-federal data is clearly marked as such on the site in both the metadata and with a banner across the right side of the dataset in the search results.\nThe new Open Data Policy from the US government explains more about the definition of open licensing.  \n(Disclaimer: I am the Evangelist for Data.gov)\n", "data request - Open database of domain registration information?": "\nYou could try a service like http://www.whoisxmlapi.com/reverse-whois.php or http://www.domaintools.com/\nThe short answer is: it's complicated. The whois system (which is used to query domain registration data) is decentralized similarly to the DNS system - individual registrars keep the whois data for their clients so there isn't a meaningful way to query a central database. Additionally, the whois protocol is designed to be incredibly simple: query for a resource and receive the associated data. Think of it as a dictionary... You can easily search a dictionary for the definition of a word, but you'd need a different tool to search through all the definitions to find words with similar meanings. Companies like the ones mentioned above do the heavy lifting by aggregating the individual whois data and then providing a means of searching the meta data. Since not all whois responses are the same however, most results by reverse whois searches are considered \"best guess.\"\nAs with everything, there are caveats to the explanation above, but without getting too unduly complicated - there you have it.\n", "tool request - How to publish open data on my website? (Or: from CSV to RDFa)": "\nIf you already have a nicely formatted CSV then why not publish that? You can publish in both formats if you really want to do the RDFa too.\n", "licensing - License for data that precludes government/surveillance use": "\nSuch a license would require that the publisher provide one on one individual approvals for data use and analyze or have the potential user express all the impacts of the use of their application or analysis.  This essentially would not be open data.\nTrying to stay with the theme of this group, there is a license that could apply:\n\nCC BY-NC and BY-NC-ND which restricts commercial use\n\nAn example of the licensing used by Thomson Reuters and many of their partners might provide insights as well.\n", "usa - Additional Detail from IPEDS Data Source": "\nThe entry for IPEDS on data.gov (https://catalog.data.gov/dataset/integrated-postsecondary-education-data-system-ipeds-data-center) provides a point of contact. Recommend emailing the dataset POC directly.\n", "Census County Commuter Flow Data?": "\nHere is a link to county to county commuting flows:\nhttp://www.census.gov/population/metro/data/other.html\nFor older data, you can take a look at the raw files from:\nThe 1990 Census: http://www.census.gov/population/www/socdemo/jtw_workerflow.html\nThe 2000 Census: http://www.census.gov/population/www/cen2000/commuting/index.html\n", "usa - Are there good examples of requiring open data in RFPs?": "\nClear and simple, I can do ... re-usable I'm not so sure of.\nNSF in 2011 put a requirement on all grant applications to submit a 'data management plan', explaining what data would be produced and made available by the project.  Note that it doesn't actually require the data to be 'open', and it's entirely possible that the scope of the project may be such that the PIs believe that there isn't any 'data' produced.**\nThere's also been questions as to what instructions NSF has been giving to the grant review panels about data management plans, and how much weight they have in the final scoring of the grant proposals.\nNASA has various 'data policy' statements, depending on the field.  The heliophysics policy states:\n\nTwo overarching principles also essential to achieving the goals of current Heliophysics programs are:\n\nEmbracing NASA's open data policy that high-quality, high-resolution data, as defined by the mission goals, will be made publicly available as soon as practical ...\n\n\n... while the NASA Earth science policy contains:\n\n\nNASA commits to the full and open sharing of Earth science data obtained from NASA Earth observing satellites, sub-orbital platforms and field campaigns with all users as soon as such data become available.\nThere will be no period of exclusive access to NASA Earth science data. Following a post-launch checkout period, all data will be made available to the user community. Any variation in access will result solely from user capability, equipment, and connectivity.\nNASA will make available all NASA-generated standard products along with the source code for algorithm software, coefficients, and ancillary data used to generate these products.\nAll NASA Earth science missions, projects, and grants and cooperative agreements shall include data management plans to facilitate the implementation of these data principles.\nNASA will enforce a principle of non-discriminatory data access so that all users will be treated equally. For data products supplied from an international partner or another agency, NASA will restrict access only to the extent required by the appropriate Memorandum of Understanding (MOU).\n\n\nMy understanding is that all NASA missions require a 'PDMP' (project data management plan)\n** although, that then gets us into the question of what is 'data', which is much too long and off-topic for this post.\n(Disclaimer : I work for a NASA heliophysics archive; I haven't served on a grant review panel for years and never for NSF)\n", "Spend transaction data formats": "\nI think it was best said in John King's wrap-up for the public hearing on public access to federal data last month at the National Academies.  To paraphrase:\n\nIf you require people to do stuff for which they get no benefit, they're going to spend the minimum effort in doing so, and you'll get a crappy result.\n\nSo ... rather than focus on the format ... focus on what benefit they could get out of it.  I've been a big proponent of tool building to support data formats -- create some great tool that people want to use, but to use it, they have to put their data into a proscribed format.  Maybe you could give them a tool to do the reformatting, so that they're the ones maintaining it should their fields change.\nRight now, their different formats support their existing systems.  They actually need those systems to get their job done.  The extra reporting is just an extra burden that's been placed on them.\nTalk to the various IG (Inspector General) and finance departments ... maybe there are some common analysis that they all do that's challenging with their existing tools.  If it's more cumbersome than the data reformatting, you have a potential way in.  If you can't find some way that you can actually improve someone's life (with that someone being in the power to make the change), you're just going to be treated like another unfunded mandate. \n", "tool request - Is there a Git for data?": "\nI recently stumbled on this article by the Open Knowledge Foundation regarding the design of a graphical interface to diff tabular data called daff.\nIt can also be tested and forked on GitHub.\n", "usa - Linking FCC documents from ECFS to the Federal Register": "\nI've found the Federal Register Ruby gem to be useful:\nresult_set = FederalRegister::Article.search(:conditions => {\n  :agencies => \"federal-communications-commission\", \n  :docket_id => \"12-375\", \n  :type => \"PRORULE\"\n})\n\nTo provide programmatic linking, a mapping of \"NOTICE OF PROPOSED RULEMAKING\" (ECFS) to \"PRORULE\" (Federal Register) is needed.\nThis seems to work well enough, but a potential pitfall could be proceedings that have multiple documents of the same type (e.g. multiple NPRMs). In those cases, I'm not sure how to distinguish documents.\n", "data request - Bioequivalent drugs in the US and EU?": "\nA useful subset of the bioequivalent drugs would be approved generics. Don't know if you're also looking for unapproved (yet) but these would be a bit harder to find as a complete collection. Keep in mind that the pharmacokinetic test routines differ slightly between the US and Europe.\nKeeping to the approved generics, the FDA is required to maintain and post a list quarterly. The data is in PDF but at least it's selectable text (not just scanned images). For Europe, the EMA has a search page for generics (etc). Search results can be downloaded in excel format.\n", "usa - Availability of APHIS Data as API or bulk dump": "\nIt's likely not want you are hoping for, but that website is an API, just an undocumented poorly designed one. You'll want to make easier to work with by writing a screen scraper. \nIt looks like at least one other person, maybe you, is working on one at ScraperWiki\nhttps://scraperwiki.com/scrapers/aphisacis/\n", "data request - A database for dog, cat and other pet names?": "\nI've compiled some resources for a blog post, I'll just post the relevant content here:\nHundenamen aus dem Hundebestand der Stadt Z\u00fcrich\nThis one is from the city of Z\u00fcrich, Switzerland, where I live. I've seen a recent Twitter post about this dataset, so that may have planted the idea that dog names can be open data.\nData goes back to 2015, and each year is one CSV file. To get an idea of the dataset size, I choose the complete year of 2019. 7647 records. It may be hard to find trends in so few dog registrations. Additionally, the Paw Patrol trend is slowly making it here to Switzerland. Since it started in North America, I'll go to look there.\nAnchorage Dog Names over Time\nOnly 16k total names between 2017 and 2019. That's not enough dogs when there are so many possible names. And starting in 2017, I may not get a good before snapshot.\nSeattle Pet Licenses\n\nA list of active/current Seattle pet licenses, including animal type (species), pet's name, breed and the owner's ZIP code.\n\nThis might be a good dataset because records go back to 2000 and are updated through 2019. I can get snapshots before and during the PAW Patrol era. But I counted dogs registered in 2019 and it was 11k. In 2018, 7k. Still not enough.\nNYC Dog Licensing Dataset\nThis could be it. Recently updated. 24.1 MB CSV file. 345k total rows going back more than 10 years. 79k dog registrations in 2019. Explore the data here.\nthe fine print:\n\nEach record stands as a unique license period for the dog over the course of the yearlong time frame.\n\nWhat does this mean for my data? It means that dog names are assigned at least once per year. If I count unique dog names over multiple years, I'll be over counting. \nand\n\nEach record represents a unique dog license that was active during the year, but not necessarily a unique record per dog, since a license that is renewed during the year results in a separate record of an active license period.\n\nThis means that dog-names within a given year may actually be duplicate as well. If this was a real project, in order to fully trust my data, I would first count how many names are repeated. To do this, because there is no column dog ID which would uniquely identify a dog, I would have to create a surrogate key based on the columns such as AnimalBirthMonth, AnimalGender and BreedName, and perhaps also the geographical data Borough and ZipCode.\n", "best practice - Let's suppose I have potentially interesting data. How to distribute?": "\n\nSuppose that I have some sort of specialized data, perhaps that I've collected myself or been a part of the collection. And suppose that nothing prevents me from handing this data out to people. In what method should I go about distributing/storing this data so that others will be able to find it and use it, whenever this time may be?\n\nTargeting specialised repositories as per @Joe's answer is indeed an excellent way to go about disseminating data, but what if no such specialised repository exists or you do not wish to target only one specific community in particular?\nA methodology to expose Open Data using generic principles is the 5-star Open Data scheme originally proposed by Tim Berners-Lee here.\nThe core rationale of 5-star Open Data is that you make your data more easily accessible, processable and interoperable with each successive star:\n\u2605 Put your data on the Web in some format with an Open Licence. People can access it through their browsers and spend some time to figure out how they can download/access/process/use it. (Avoid problems for your client like this.)\n\u2605\u2605 Put your data in a machine-processable format. For example, having a table in Excel is better than having a snapshot printed in PDFs or images because people can download it and start running experiments over it. (Avoid problems like this.)\n\u2605\u2605\u2605 Use non-proprietary formats. For example, providing data as a CSV is often better than as an Excel file because CSV can be directly processed by a wider range of (free/open source) tools and programming languages. (Can't find anyone complaining about Excel on here yet but, e.g., this is a similar problem.)\n\u2605\u2605\u2605\u2605 Use URIs to denote things. For example, let's say you provide a bunch of pollution measures for cities and somebody would like to specifically reference the pollution measure for London. Assigning a URI for London in your local data provides a global unique identifier for that city that people can reference and point to. There are, for example, related proposals for embedding URI fragment identifiers in CSV files. (Avoid problems like this or this.)\n\u2605\u2605\u2605\u2605\u2605 Link your data to other data to provide context. So you have created a URI for London in your data and people can point to it. However, which London are you referring to? London, England or London, Ontario? If you link your local URI for London to the Wikipedia page about the London to which you refer (or, even better, to the DBpedia URI for the specific place to which you prefer), this provides context as to what you mean. (Avoid problems like this.)\nThe shift from \u2605\u2605\u2605 to \u2605\u2605\u2605\u2605(\u2605) is quite an ambitious one and technical proposals are still being made on how best to achieve this, but five star Open Data is great because now your data are available on the Web under open licences with open structured formats where everything of importance is given a URI that can be referenced and linked across the Web, allowing for future discovery and re-use. A common methodology to create five star Open Data (again proposed by Tim Berners-Lee) is Linked Data, which assumes RDF as a common interoperable data format. But if that all sounds too much, getting as far as \u2605\u2605\u2605 data is still great.\nAgain, you can check out this description of 5 Star Open Data for more information and a related question here. \nA useful resource for the generic cataloguing of Open Datasets is the CKAN project, where the related DataHub repository is a great place to list and publicise your dataset. You can check out a bunch of 5-star Open Datasets here.\n", "Should data APIs require registration and API keys?": "\nThe other answers so far are all terrific. I'll reiterate one point, and make a new one:\n\nThe openness of an API is always important, but when complete, quality bulk data is available some of these access issues become a lot more tolerable. An API is not a substitute for bulk data. The federal government has become very API focused, and many of them have throttling, API key registration, and even attribution requirements. The need for accompanying, complete bulk data is a point I hope the community will continue to press.\nThere's an important political aspect to API key registration, which is demonstrating (especially internally) that the API is a success, and worth continued investment. I work at the Sunlight Foundation, and this is one of the reasons (in addition to abuse, contacting devs, etc.) we require registration of an API key. This is both quantitative (measuring hits), and qualitative (it's nice to have logos of big organizations on the sidebar of our API homepage).\n\nOf course, government agencies are in a fundamentally different situation than non-governmental organizations. They're funded directly by taxpayers and are the original producers of information that literally belongs to the public. \nWhen agencies consider the benefits of using API keys versus providing open access, the scale should be heavily tilted towards open access to the people's information. Providing free bulk data access in addition to any APIs (like the Census does), and providing key-free API access (like the Federal Register does) are models I strongly encourage.\nUpdate: See FederalRegister.gov's API case study for their rationale for not using API keys:\n\nIn our view, API keys can create an unnecessary barrier to rapid experimentation with our public data. We are able to track our API usage via logging mechanisms on our servers and already have infrastructure in place to mitigate any sort of excessive requests. The benefits of using a simple REST-ful API format are that any user can easily try it in their browser (no SOAP that requires complicated XML to be POSTed around, no special headers, etc). The response to our no keys policy from the development community has been extremely positive (http://news.ycombinator.com/item?id=2839137).\n\n", "data request - Is there an open database of elementary, middle, and high schools in the United States?": "\nUnfortunately, the key links in the popular answers for this question are all currently broken. It is still true, as David H answered, that the Common Core of Data is the official source, but the links to data.gov are broken.\nNow, the only relevant thing on data.gov is the page about the School & District Navigator which links to an interactive map that doesn't itself offer data.\nHowever, the Department of Education has a simple interactive tool which helps get access to specific CCD files, \"fiscal\" or \"non-fiscal\", at the state, district, or school level.\nAlong the way, I also found public schools as GIS data from the DHS Homeland Infrastructure Foundation-Level Data (HIFLD) website.\nThe Department of Education also created a polished \"Developer Hub\" referencing supported and legacy APIs, but the dates on blog posts for that are all at least 3 years old right now, so it's hard to know what's actively supported -- and in any case, none of the APIs are for K-12 schools.\n", "government - Data on income information for India, China, and West/East African nations with GIS coordinates": "\nSpeaking as a non-professional, I have seen no such detailed datasets.\nYour research should not depend on clueless strangers, though. My first hit on Chinese statistics was: http://www.stats.gov.cn/english/statisticaldata/ , and I'd think they are worth exploring. Same for India.\nGathering detailed income data is fraught with difficulty on many levels: the ones with access to the high-fidelity data (local tax authorities) won't talk for a host of reasons, apart from confidentiality (quite often they won't be bothered to help other government agencies), while aggregation over villages/counties/whatever lowest-level administrative units is not something in high demand from national and supranational decision-makers. I leave aside the obvious problems of tax evasion, non-response, and outright fabrication.\nThus, for countries with less than well-funded statistical agencies, one is forced to look for proxies.\nThe usual proxy is electricity consumption; while the utilities are loath to give outsiders access, as a rough proxy, one can simply look at the night lighting: http://geology.com/articles/satellite-photo-earth-at-night.shtml.\nI'd recommend against turning to land use data - they can't tell you much about city-dwellers or income levels. You can ask folks at GIS SE for details, though.\nAccess to sanitation is another proxy (albeit a non-linear one). World Bank, local authorities, utility companies, food inspection agencies can possibly serve as sources, I'd guess.\nWhatever you do, please remember to compile a list of references, prior and related studies, validate and cross-check your data. Ideally, you would do an on-site survey for that (assuming you have got some money to spend). Beware of systematic bias creeping in (it will!) without you noticing or telling users of your data.\n", "Crowdsourcing Data Submission": "\nIndeed, quite typically, for transport data, it can be very interesting to gather open data and users feedbacks (tweets, posts, ...) to improve quality and restore confidence towards public services.\nThere are some tools mixing Open Data and crowdsourcing: OpenStreetMap, OpenEcoMaps\nAt some midpoint between Open Data and crowdsourcing, you have FixMyStreet: The data released by the users can be used to hydrate future open data DB's. \nBtw, see this blog post: http://blog.okfn.org/2011/05/23/can-crowdsourcing-improve-open-data/\n", "data request - Open dataset for a 65-million year temperature history of earth?": "\nThe Zachos et al. article has supplemental data section, but there are no actual datasets in this section, only sources for \u0394O-18 and \u0394C-13 isotopic data. The article itself says that the temperature estimates are given for an ice-free world ocean. If you want to cite the data from Figure 2 in the article without all the caveats (and then some) of the original, your readers will be either fooled or offended.\nA sample caveat from the supplement:\n\nSampling Biases: One of the limitations on reconstructing long-term secular variations is the highly uneven distribution of deep-sea stable isotope data in both space and in time. The global signal for some key intervals is based on data from just a few records (emphasis mine - DH). In general, these spatial biases increase with age, moving toward the Atlantic, and shallower water depths. In other words, the Pacific, and abyssal portions of the oceans tend to be under-represented in existing stable isotope records. These biases do not pose a problem for our temperature/ice-volume reconstruction of the late Neogene oceans which were thermally homogeneous. Such biases, however, are a concern for establishing the mean climate-state of some \"warmer\" time intervals when the thermal gradients within the deep-sea were greater.\n\n", "data request - Open alternative to weatherbase.com": "\nHow about NASA GISS Surface Temperature Analysis? Although they have station-oriented timeseries, global coverage seems quite good and they provide monthly average temperatures globally for a ridiculously long time: random sample of a station dataset in Africa (1946-2012):\nYEAR    JAN    FEB  ...   DEC\n1946   24.8   25.9  ...  26.4\n1947   26.1   27.3  ...  25.3\n1948   25.8   27.4  ...  25.7\n...\n2010   25.9   28.0  ... 999.9\n2011  999.9  999.9  ... 999.9\n2012  999.9  999.9  ... 999.9\n\nMajor cities tend to have one or multiple stations with identical name nearby. Documentation of their data processing is naturally very good. (Meh: the sample station I picked seems to be plagued by missing values [999.9] recently.)\n", "legal - Open data for international treaties": "\nA searchable directory of international treaties can be found here: http://www.worldtreatyindex.com/index.html\nLinks to download the raw data of the complete database can be found here:\nhttp://www.worldtreatyindex.com/multi.html\n", "releasing data - Recommended BitTorrent tracker/index for dataset release?": "\nConsider whether posting your .torrent file to a BitTorrent index site is the best solution for you. If your objective is to publicise your dataset you may be better off simply posting your .torrent file to a website or forum that focuses on open data or, more specifically, the topic to which the data relates.\nYou should also bear in mind the large amount of upload capacity you'll need for this initially. Depending on the level of interest, you might find it difficult to retain seeders, and so could find yourself uploading the file repeatedly. I assume you've looked into paid cloud storage (such as Mega), but bandwidth limits will be a potential issue.\nIt might also be worth confirming that you are using the maximum compression possible. Compression software commonly defaults to a moderate compression ratio as a trade-off for quicker compression time.\n", "data request - Open Seed for Crawl": "\nYou can download the top 1 million (ZIP) sites from Alexa.\n", "parsing - What data source for cloud coverage available with forecast and how to parse it?": "\nOpen GIS-ready cloud data are available at various levels of detail, both in time and space.\nIt is quite tempting to work with satellite photos of cloud coverage; unless you are a professional, don't do that - there are a bunch of hidden snags you have to know about.\nFor current data on (points) airfields and airports of the world, your best bet is METAR, where you can learn type of clouds and cloud coverage, as well as visibility figures. There are many applications (in Python and Ruby, among others) that access and decipher METARs (you can do that as well, after a bit of training).\nIf you need in-depth point forecasts of probability that cloud cover will be less/greater than a given threshold, you can use NOMADS (may be overkill for your purposes, though).\nFor wide-area coverage with forecasts up to 8 days, the ideal stuff comes from GRIB, easily downloadable and parseable with command-line utilities and GUI programs. These are forecasts of most probable weather, though, unlike the extensive data from NOMADS ensemble.\n", "usa - Real-time gunshot detection data?": "\nThe City of Oakland tacitly agreed to make the shotspotter data open, and the firm representative was eager to do so, but they've not followed up by doing so. \nThey did release it in bulk in Washington however, not real time.\n", "tool request - Anyone have a good way of comparing two large and unstructured lists (~2k entries each) for commonalities between them?": "\nFollowing the discussion in the comment section, I suggest that you have a look at OpenRefine. For a 4,000 rows dataset (two set of 2000 rows each) Refine allow a mix of manual and script cleaning (using fuzzy match). Here is the steps I will follow (based on what I understood) to clean this dataset:\nPrepare your data\n\nIn a separate tool, merge the two set in a single document with a column for each source\nLoad the file in OpenRefine\nUsing the transpose function merge the two fields in one, in the windows option remember to tick append column name (so you can track from which source your data come from) and separate\nUsing the split function, split your new column based on the pipe |\n\nSo now you have a field with your source name and an field with your value, now we will be able to start to clean those value:\n\nInvoke a text facet to list all the value available, and click cluster to do fuzzymatch comparison and search for similar record\nPlay with the different clustering algo (including Levenshtein, metophone) ... You have to manually select the right matches, discard the other, you are in total control of the algorithm. Do not hesitate to explore the different algorithms available, some are more conservative than other or will match on different parameters.\nOnce your done with the semi automatic clean up, finish the work manually using the facet windows to list all value available in your list. When you want to correct a value click the edit option in the facet windows to update all matching rows.\nOnce done export your data, there is various format available.\n\nThis is a very high level process and hopefully you will find it useful. If you want more details, you can explore this tutorial for the split and clustering function, or dig through existing step by step doc.\ndisclaimer, I am part of OpenRefine team\n", "data request - Open datasets for product reviews": "\nAmazon has an API for this, and then there's always web-scraping.\n", "usa - Public access laws used for real-time data?": "\nGiven most local laws and regulations allow for a 2-3 week response time, I imagine you'd have to request the records before they exist, and word your request in such a way that it can \"never\" be completely fulfilled.\n", "tool request - Wikipedia table to JSON (or other machine-readable format)": "\nYou can use Google Spreadsheets ImportHTML formula as detailed in this Liberating HTML Tables (using Google Spreadsheet) tutorial on School of Data by Tony Hirst - it includes a specific walk-through for Wikipedia.\nThe essence is to do:\n=importHTML(\"\",\"table\",N)\n\nIn your case you could try:\n=importHTML(\"http://en.wikipedia.org/wiki/List_of_television_stations_in_the_United_States_by_call_sign_%28initial_letter_K%29\",\n   \"table\", 3);\n\nIn your case you can tweak 3 to be the table you want to grab and obviously you can repeat this formula for multiple tables.\nThere a bit more on this and issues with links in this answer on ask.schoolofdata.org (also from Tony Hirst).\n", "tool request - How to use the DOI system as an individual?": "\nI'm not aware of any groups that will let you create test DOIs unless you're somehow affiliated.\nHowever, the California Digital Library is a member of DataCite, and they operate EZID to allow other groups to mint DOIs.  They have a pricing schedule for groups to get access.  I'd suggest contacting them (see the link on the pricing page). \n", "best practice - Load data from HTML tables into OpenRefine?": "\nIf you're doing this interactively, most browsers will format tables as TSV when they're selected and cut.  Pasting this into the clipboard dialog of Refine's project creation dialog will allow you to import the data as TSV.\nIf you've got a bunch to do or need to do this repeatedly, I'd use Google Spreadsheet's importHtml(url,\"table\",N) function which will fetch the Nth table on the given page.  Refine can import directly from the resulting Google Spreadhsheet, so you can skip the export step.\nIf you just wanted little bits of information from lots of different tables, you could use \"Add column by fetching URL\" and then hand parse the interesting data out using Refine's parseHTML() with the necessary CSS selectors, but that would be pretty painful and not recommended.\n", "data request - Database of English words pronunciation": "\nWikimedia Commons currently offers more than 20.000 sound files with English pronunciation, around 1.500 of those with British English pronunciation. All of them are published under an open license.\nUnfortunately, there are currently no dumps of the media files available. However, there is a page that explains how to reuse the content outside of Wikimedia.\nAs an alternative, there is Forvo. Their audio files are licensed under the more restrictive (and not entirely open-data-compliant) CC BY-NC-SA license. On the other hand, they do offer an API.\n", "releasing data - What is a ready to use wordpress CMS template for serving open datasets?": "\nData.gov has open sourced it's code, which combines WordPress for the front end with CKAN for the open data catalog.  The code is available via Github--for commenting, downloading, or submitting modifications.  \nThis code will continue to evolve with user-driven updates to the functionality needed.\nThis will be part of the U.S. ongoing contribution to the Open Government Platform, as well, which is a collaboration between India, Canada, Ghana, and the U.S. and currently is available as a native Drupal, and a Drupal + CKAN capability. The source code is being made accessible via Github.\n(Disclaimer: I am the Evangelist with Data.gov)\n", "usa - Linking results from the FCC's TV Query API to the FCC's TV Stations Profiles API": "\nThis is the sort of issue that can arise when trying to combine data from multiple sources, in this case two apparently distinct APIs.\nThe Developer page gives some brief instructions as you point out, but importantly there seems to be no method for retrieving a list of all valid Facility IDs. This is an oversight in my opinion, as a \"Facility Details\" request, for example, requires a Facility ID as a parameter. It appears the only way to get valid Facility IDs, other than your method, is to use the \"Facility Search\" API method, which allows you to search for stations using \"call sign, frequency, city, state, channel, or Nielsen DMA\", and returns a Facility ID among other data.\nYou haven't stated whether you're working manually or programmatically, but as a programmer if I were doing this and needed to get a list of Facility IDs, I would probably use the Facility Search to return data for each of the 50 states, then combine these. This would be relatively straightforward in code - loop through the state codes for each of the 50 states, as per the example given for New York:\nhttp://data.fcc.gov/mediabureau/v01/tv/facility/search/NY.json\nFor each state, scrape off the Facility IDs, which ought to result in a complete list of valid Facility IDs for the USA. This assumes that there are no facilities that are somehow independent of states, or have no state recorded.\nHaving said all that, I would probably take a minute to send a quick email to developer@fcc.gov, asking whether it is possible to retrieve a list of all Facility IDs via some unpublished API method. The amount of support you can expect for using an essentially free and thus unsupported service may vary of course.\n", "What criteria determine a good name for an open data product?": "\nUnique.  I can't believe we still have groups naming projects 'GAIA' (as if image processing software, satellites, and other existing projects isn't enough).  But even satelites like 'TRACE' and 'SOHO' are problematic because they're common enough in English.\nI'd also look to see if there are standard prefixes or suffixes in the discipline that are significant (eg, you gave two examples that ended in -X). If there are common acronyms for the given type of data you're releasing, you may want to follow the conventions in your field ... but you also have to be wary of it being too similar and someone thinking it's just a typo for some other product.\n", "licensing - Why should I care how a (structured) dataset is licenced?": "\nIANAL but I believe most open licences are exclusively designed to provide reuse rights that were taken away by copyright (e.g. CC). Some licences also give rights that were taken away by the 'database right' (e.g. ODbL, OGL). (I don't have much knowledge about click-wrap agreements.)\nI'm not sure that you can say that most data is factual, and therefore copyright does not apply. Track names for CDs are creative works. Weather readings and locations of items on a map have been defended vigorously against copyright (e.g. before database right came along in '96), although you'd have thought that because they are collected by strict rules, they'd fail the 'creative judgement' test. That the case of the football fixture list had to be referred all the way up to the European Court suggests it is not an easy judgement and that you have to be extremely careful. The telephone directory case also seems hard-won.\n'Database right' does not apply to an individual item of data, but the threshold is somewhat lower than the entire database. As soon as you extract a substantial quantity from the database you become liable. So anything more than one item might invite a letter from the lawyers.\nSince reusing data without a licence is such a mine-field, anyone who doesn't have a massive legal fund might as well regard it as a no-go area. That's why there is a big push by governments to provide open licenses for their data, to encourage transparency and reuse of these valuable national assets.\nIt is important to note that you can use data for your own internal purposes, whatever the licence. It's only if you want to republish the data or derived data that you need to study the licence.\n", "usa - What open data institutes in the US are working with the Open Government Partnership?": "\nYou may have already seen this, but on the OGP website there is a Country Commitments page for each country including the United States. The \"Efforts to Date\" tab has some broad information on the government's, well, efforts to date. It appears that the main objective so far has been to increase the utility of the datasets available at data.gov.\nData.gov looks to be a well-designed site with loads of information. For example you can dig down to Open Data sites at the state and county level. I suspect if you browse around you might find something approaching the information you're after.\nAs an aside, it seems that the OGP does not directly sponsor or financially support the work of individual groups or companies, etc. Rather they are an organisational body with the goal of spurring on governments to increase transparency. If you're interested in how the OGP's budget is sliced and diced, it is available for download. \n", "api - Is there a resource to look up the Standard Industrial Classification codes that companies file with the SEC?": "\nOn the NICAR-L mailing list, Tim Henderson pointed to an HTML listing, at http://www.sec.gov/divisions/corpfin/organization/cfia.shtml\nMatt scraped the data and posted it as a Google document.\nMatt Jacob pointed to another potentially useful resource, a listing of what the SIC codes actually mean.\n", "Which real-time open data APIs do you know?": "\nYou can find a wealth of government APIs at the Data.gov developer page.\nAs far as real-time nature of the data feeds, the APIs vary in their update frequency. For example, flight status from the FAA updates every 10 - 15 minutes: \n", "data request - Is there any free weather database that one could use for correlations in business intelligence software?": "\nTwo resources:\n\nhttps://developer.forecast.io/ (free for low use, then paid after that)\nhttp://www.ncdc.noaa.gov/cdo-web/#t=secondTabLink\n\nThe NOAA data is used by a developer in Chicago to explore the correlation between weather and crime.\n", "data request - Where can I download those bible verses in JSON or XML or SQL formatted file?": "\nDisclaimer: I am the author of below github projects.\nTwo very good (and complete) sources are:\nhttps://github.com/scrollmapper/bible_databases :  \nThis has many database formats including all three formats you are inquiring of. \nhttps://github.com/scrollmapper/bible_databases_deuterocanonical : \nThis is a newer continuance of the bible databases, but with secondary books (ie, the Deuterocanonical ones). This is just sqlite at present but easy to convert. It is also based in a Django project. \n", "data request - What sources exist for sales tax information": "\nThere are only pay-per-request API's for sales tax data. The Sales Tax Clearing House offers an interface to pull sales tax information. With a list of zip codes, you could automate the process of pulling in the tax data. I'm guessing they probably wouldn't like that, so it would probably be a good idea just to send them an e-mail requesting a data dump.\nThis question has also been asked on StackOverflow.\n", "usa - Need clarification: if state or city gov releases data in non-open formats (i.e. book, microfiche), *must* they now also release it in CSV format?": "\nThe Open Data Policy and Executive Order are for federal datasets and does not mandate the same for state or local governments, although such policies does influence more local policies.  \nSee http://www.data.gov/opendatasites for a large (but not comprehensive) listing of Open Data sites at the international, state and local levels.\nFor state-level open government or 'sunshine' laws, recommend http://www.rcfp.org/open-government-guide.\n", "usa - What is the significance of Census ACS columns with line numbers ending in \".5\" or \".7\"?": "\nBefore seeing this, I actually verified this answer just earlier today with Paul Overberg, one of the leading Census journalists around: the \"fractional line numbers\" represent \"headers\": that is, they are labels that group together subsequent rows but which don't have values themselves.\nHis example was the \"Median income in the past 12 months --\" line in this:  (from http://factfinder2.census.gov/bkmk/table/1.0/en/ACS/11_1YR/B07411)\n", "geospatial - Where could I find open data about ATM locations (in Paris)?": "\nOpenStreetMap's database has the ATM tag. Through the Overpass API, you can quickly access the Points of Interest: Map of ATMs in Paris (Overpass API). Click on the points to see additional metadata  (mainly the name of the operating bank).\n\nBonus: there is a good thread on XAPI call for all ATMS in OpenStreetMap's own Q&A plattform.\n", "Who are non-schema.org data standards for?": "\nSchema.org is an ontology (\"data standard\") specifically for marking up HTML so that search engines can more easily extract structured data from otherwise unstructured data. Schema.org is very popular for marking up products (reusing the GoodRelations vocabulary), articles (reusing the rNews vocabulary), reviews, etc. If all you care about is search engines, then Schema.org is the only vocabulary you need to care about.\nHowever, HTML is just one way to share information. As you mention, you can provide an API or bulk downloads. Popolo (I'm its editor) is currently targeting those channels. If you're curious about why standards matter in those contexts, I can provide a longer answer.\nThat said, even if you only use Schema.org in your HTML, you'll be offering partial support for Popolo, because Popolo reuses terms from Schema.org. It also uses terms from predecessors of Schema.org that Schema.org subsequently adopted. For example, Popolo's Person class has significant overlap to Schema.org's.\nAs for adding Popolo terms to Schema.org, that's definitely a possibility. For example, I think it would make a lot of sense to add the dissolutionDate property from Popolo to Schema.org's Organization class, so that companies can dissolve like in real life.\nLast point: unless you plan to share data with other developers through HTML, as far as I know, there's not much to gain from using vocabularies besides Schema.org in your HTML. If you want to share data with other developers, you should offer bulk downloads or an API, instead of requiring people to parse your HTML (even if that HTML has beautiful semantic markup).\n", "best practice - Examples of metadata for non-uniform collections": "\nI don't know of a single all-encompasing standard.  You basically need to look at what attributes are common to all the collections you're tracking, even if they don't necessarily seem similar at first glance.\nIf you're dealing with data, then DataCite is generic enough to describe collections of data without getting into specifics for each scientific discipline.  (There are other more specific but still collection-level descriptions available; eg, SPASE for space physics data).\nFor physical objects, you may need to look into why you're interested in the items.  For a museum, they might track collections by who donated them, or where they came from (archaeological dig, etc.), while someone who's operating a store would track that as a supplier and might have information such as lead time needed for orders, what type of things they manufacture, etc.\nYou also run into strange issues of what exactly is a collection.  I deal with cataloging data for the most part, and there are issues with what the proper aggregation should be for cataloging.  See Laura Wynholds's 'Linking to Scientific Data: Identity Problems of Unruly and Poorly Bounded Digital Objects' for a bit of a background.  (and see Renear, Sacchi & Wickett's 'Definitions of Dataset in the Scientific and Technical Literature' to see we can't even agree on what the collective noun actually means)\nTo explain this with physical objects --  a car is a collection of parts; to the factory that needs to assemble the car and the dealership that's selling it, they might look at the car as one thing, or an aggregation of multiple components.  Even something as simple as a movie can be broken into multiple scenes, thousands of individual images plus audio tracks, etc.\n", "data request - How many software developers are there in the world, per country?": "\nThe databases at the International Labour Organization (specifically ILOSTAT and LABORSTA) are tantalisingly close to what you're after. For example, go to LABORSTA and select Employment, then select Employment for detailed occupational groups by sex (SEGREGAT). This allows you to select a country and view a breakdown of detailed occupational groups which includes \"computing professionals\". Not quite \"software developers\" but perhaps a reasonable proxy if you're interested in comparing across countries. Note that the main LABORSTA statistics also break down by occupation but sadly the list of occupations lacks detail.\nILOSTAT is the whiz-bang successor to LABORSTA, but it seems the data is no more detailed than described above. Sadly, in neither case does there seem to be an option to download bulk data, rather you are compelled to view results on a per country basis.\n", "real time - Programatically request recent close prices for a list of stock tickers": "\nWhile the Google Finance API is officially no longer available,  it's still active in returning requests in XML.\n// Dow Jones\nhttp://www.google.com/ig/api?stock=.DJI\n\n// NASDAQ\nhttp://www.google.com/ig/api?stock=IXIC\n\nYou will need to consult a programming site like StackOverflow  if you have a programming related question.\n", "data request - UK supermarket product nutrition": "\nWe're building an open crowdsourced database for uk food products.\nWe have calories, ingredients, photos.\nYou can use it and contribute to it as well (and let your user contribute) according to the OdBL licence :-)\nhttps://uk.openfoodfacts.org and https://world.openfoodfacts.org\n", "geospatial - Examples of scraping from \"real-world\" data sources using OCR, etc?": "\nThe best example I have heard of is Real-time traffic monitoring using mobile\nphone data (PDF).\nThe idea is to derive road traffic velocity from the position data that the mobile phones within cars \"generate\" when moving from one base station to the next. The frequency of these base station handshakes approximates the travel velocity of the car. Practical application is the detection of congestion without the need for dedicated hardware.\n", "data request - Where are the concentrations of digital companies in the UK?": "\nThe original article from the National Institute of Economic and Social Research (there's also a direct link to the PDF) has more detail on the geographic distribution of UK technology firms in the form of hotspot maps by Travel to Work Area. This is among a lot of other well presented information including breakdown of technology firms by sector, and growth rates of the \"digital economy\".\nI suspect the articles you cite in your question did not look beyond the summary of key findings. Always look to the original published report as a primary source.\n", "usa - Accessing CATO Deep Bills with Ruby": "\nSince Deepbills is based on the official XML that Congress publishes, I think it'd be pretty easy to add support for passing through Deepbills' extra tags and attributes to the Ruby gem I made for working with Congress' bills, us-documents:\nhttps://github.com/unitedstates/documents\nIt's a tool for stripping a lot of tags out of Congress' bill XML, and giving you semantic-less HTML that can be integrated and styled much more easily than the original XML.\nAdding support for Deepbills might be as simple as adding new names to the whitelist of tags to preserve.\n", "usa - How and where can I get data on US census block data by city and state?": "\nTry NHGIS, it has built a resource that offers what you are looking for.\nThe race question is available at the block level from the 2010 Census Summary File 1 release; but the income question is available only through the American Community Survey (ACS) Summary File release which goes down to the block group level. If you want the information together, I would recommend using the ACS as a source.\nOtherwise, if you have ArcGIS 10.1, you can just download this. It may work in previous versions, but it's a roll of the dice.\nHere is what's available in the pre-joined Census data:\nCondensed Codebook\nFull Explanation of Each Variable\nUpdate:\nThere are new resources for:\n2012(Data)\nand\n2013(Data, Variables)\n", "data request - Metropolitan Railway Datasets": "\nThe authoritative source for Chicago transit data is http://www.transitchicago.com/data/.\nSteven Vance did quite a bit of preparation and organization around Chicago transit data and posted it on his blog in 2010. I don't see any note that indicates the data has been updated since then, but it's still probably useful as an overview to what data there is and what you might need to do with it.\n", "usa - How to flag incorrect links to data on data.gov?": "\nThe datasets that you might be looking for are all school districts and/or the School Universe Survey. To provide feedback on any of the datasets on Data.gov, you can contact the dataset owner listed on the page (contact email, which in this case is jane.clark@ed.gov) or contact Data.gov at the contact link at the bottom of every Data.gov page.\nThe URL you are linking to is for a new concept site on Data.gov (Next.Data.gov) for which we are eliciting feedback on a set of new approaches and user interface. This site is not fully operational, but in this case the link is clearly incorrect and I'll get it fixed right away. The contact listed on this page is Marina Martin at    marina@marinamartin.com.\n", "usa - Is there a tool to match zip codes to cities?": "\nThere is no simple answer to this question, because ZIP codes do not represent geographical areas. They represent postal delivery routes, which are sometimes simply a bank of PO boxes in a specific post office, and are sometimes an organization like a University which has its own internal mail processing services.\nTherefore, not all ZIP Codes can truly be located in a city, and even those for which a geography can be reasonably defined, they are not necessarily located within a single city.\nProbably the closest approximation you could get would be to start with the Census Bureau's ZIP code tabulation area (ZCTA) gazetteer file, which can be downloaded here. Then, using as many of the state-based \"place\" shape files as you need, load those into a GIS tool. Personally, I'd use PostGIS, but you might also be able to use QGIS, or if you can get your hands on a copy, ArcGIS (commercial software).\nYou could then:\n\nlook up the ZCTA for a given ZIP code \nextract the INTPTLAT and INTPTLONG values, which are the lat/long for the \"centroid\" of the\nZCTA\nuse the GIS tool to identify the place geography which contains\nthe ZCTA's centroid\n\nYou will probably have some ZIP codes which aren't in the ZCTA dataset, and it's not really precise, but it's probably good enough for a lot of cases.\n", "media - Raw Data Feed for TV Listings": "\nDigital TV channels should have an Event Information Table encoded in them, but the time window is quite limited (generally about 12 hrs).\nTribute Media used to offer free TV listings to the public, but when the popularity of MythTV and other PVRs took off, they couldn't support the amount of traffic that they were getting hit by.\nAs Tribue wasn't interested in dealing with lots of individuals, some folks banded together to form SchedulesDirect.  They distribute the Tribune / Zap2it data to individuals, at a moderate fee (currently $25/yr), but there are restrictions on redistributing the data.\n", "Is \"open\" data free as in speech, free as in beer, both, or something else?": "\nThe Wikipedia page for Open Data states it better than I can. That is, open data should be:\n\n... freely available to everyone to use and republish as they wish,\n  without restrictions from copyright, patents or other mechanisms of\n  control.\n\nTo answer your question, truly \"open\" data should certainly be free as in speech. I would say, however, that open data need not necessarily be free as in beer. In your example, even though a fee might need to be paid to access the data, as long as the end-user is able to use the data as they wish (including for commercial purposes) then I would agree this still could be considered \"open data\". In practise, data that is made available for public use (by government, say) is generally also free as in beer, but I don't see this as a requisite.\nThe idea of open data is, in my view, synonymous with the notion of open source software. Just as open source software can be sold if desired by any party, I have no problem with open data being used in commercial ways. If a business is able to add value to open data (or software) then they should be able to charge for their product. The original source will always be available to anyone wishing to use it.\nOf course, the above view is utopian in some ways. I suspect a lot of \"open\" data these days is in reality controlled in some way via partially restrictive licensing, copyright, and so on.\n", "data request - Weed out inaccurate information from 2 million records": "\nYou might want to take a look at https://github.com/open-city/dedupe. I haven't used it, but looks like it should solve your problem.\n", "time series - Where can I find data sets for machine monitoring?": "\nWith some clever thinking you can find anything.\nI searched 'vibration data filetype:csv'.\nIn the results was some very complete data set: see here for example\n", "data request - U.S. Supreme Court record items": "\nTo my knowledge, there is no complete online archive of historic US Supreme Court documents. \nThere are links to recent indices of online material from http://www.supremecourt.gov/default.aspx, and the court actually redirects seekers of online merits briefs to the American Bar Association's \"Preview\" publication A direct index of cases is available for 2012-13 and 2013-14; for earlier material, PDFs of Preview can be downloaded, but only back to the 2007-08 session.\nThe FindLaw site has online briefs dating to 1999.\nI don't have access to WestLaw, but the SCOTUS site suggests that it may have historic briefs. If so, I would not assume that they have them as far back as 1942.\nOtherwise, I believe you'd have to visit a law library or use a commercial document retrieval service, which would send a person to the court to make copies of the relevant documents for you. This page lists libraries which are repositories for printed briefs as well as listing services which can retrieve documents for you (for a fee).\n", "data request - A hierarchy of all sellable products and services": "\n[Update: As of Jan 25 2015, the page linked in this answer no longer provides the download page or the POD site. A search on the website shows something related to accessing this POD but after quite a bit of search, it doesn't appear that any of this data is actually available.]\nThe most promising open data source at least for sellable products seems the Product Open Database (POD), which tries to publish a very comprehensive product database, indexed by the barcode number.\nThe POD Download page contains the database itself and a DB scheme overview. Attributes include the GTIN (the barcode number), product name, brand name, country, size and weight of package. They even have thumbnails of product pictures and brand logos.\nThe terms of use... erm. Good news: These Terms of Use grant you a worldwide, royalty-free, non-exclusive licence to use the Datasets and pictures subject to the conditions below. Please read them carefully. Bad news: The license is not determined yet. But the explicit use of an \"Open Data\" badge gives hope...\n", "data request - Solid Waste Production, Globally? Spatially Resolved?": "\nI haven't found anything quite like what you want, but you might be able to build it from a bunch of separate datasets.\nMany datasets have solid waste production/consumption on the level of buildings.\n\nhttps://data.weatherfordtx.gov/Government/Solid-Waste-Operating-Statistics/mvy8-6q2t?\nhttps://data.snostat.org/Government/Solid-Waste-Transfer-Station-Data-2012-/xn5u-y9xd?\n\nHere's one with aggregates for the US.\n\nhttps://data.oregon.gov/Environment/Materials-Discarded-in-the-U-S-Municipal-Waste-Str/3g88-w2ag?\n\nI found all of these by searching on OpenPrism. You'll probably find more if you look through more of that search's results.\n", "nonprofit - Open Data for having transparency of expenditures in running of an Orphanage?": "\nIt's an interesting question. Salaries are the large expenditure. Will the staff be upset if their salaries are public?\nI think transparency in a non-profit is essential, especially for charities. If the staff don't want their salaries public, perhaps share the average or median salary as part of an expenditure breakdown.\nIf you are interested to increase the number of donors, consider fund-raising for specific projects by using crowd-sourcing. One example is http://www.indiegogo.com, where the donations can be matched to specific projects. For donation level you can also have small gifts of appreciation. You can link each amount to specific costs in the organization (i.e. $100 buys one month of ABC for XYZ). This setup allows for giving in levels ($10,$100,$1000+) that appeals to a broad audience (but mostly local).\n", "tool request - Crawling user data from different applications": "\nI'm going to assume that, given someone's name, you then want to scrape data from sites that they are posting on or otherwise have information on (i.e. Twitter, Facebook, etc.). With that being assumed...\nI have been doing this type of thing for quite some time, and have not found any \"out of the box\" scrapers that will do this. I've been writing my own using a combination of Ruby and Python, which ultimately may be the way you have to go.\nNote: be sure to check the TOS of each site, especially social sites. Some don't like their data going outside of their private walls.\nA Google search will come up with a few tools that require you to learn XPATH or Regex. Both are relatively simple, once you get the hang of it (which took me some time).\nIf you clarify what type of information you're looking for I'll be able to give you a better answer.\n", "government - What cities provide open data on rental building bylaw infractions?": "\nThe City of Chicago publishes building violations: https://data.cityofchicago.org/Buildings/Building-Violations/22u3-xenr\n", "data request - Find reusable images of city X with width>2100 pixels": "\nIf you don't need programmatic access (i.e. an API), you can use the Google Advanced Image Search. \nYou can also get there by following these simple steps:\n\nGo to search.creativecommons.org\nEnter your search term (e.g. berlin)\nSelect your license requirements (use for commercial purposes and/or modify, adapt, or build upon)\nClick on Google Images\nOn Google Images, click on the Size filter, select Larger than\u2026 and then 4 MP (2272\u00d71704).\n\nThat should do the trick.\nUpdate: If you want to share the URL of the image search results, please have a look at Google image search URL that can be shared?\n\n", "api - Individual bicycle data within a bike hire scheme": "\nCapital Bikeshare has this. CitiBike seems to as well.\nAlso, CityBikes looks really cool.\n", "best practice - Source of open trend data": "\nHere's the best open data source for trend data I could think of: Wikipedia page view statistics. Derived datasets:\n\nWikitrends, a daily/weekly/monthly updated list of 10 most popular (as in page views) articles. Bonus: not only absolute, but also relative change top 10 lists (called uptrends/downtrends) are offered.\nWikpedia:5000, a weekly updated list of the 5000 most accessed articles.\nTrending articles on Wikipedia finally seems pretty close to what you seek: popular articles in multiple language wikipedias during last 1, 3, 6, 12 or 24 hours. In my region of interest it shows the typical pattern of expected and surprising keywords.\n\n", "usa - Voter Registration data in one place?": "\nIt's unclear what you mean by \"voter registration data\".  \nIf it's demographic statistical data, the Census Bureau has reports from 2012 and earlier, and there are more reports at the Election Assistance Commission.  These reports cover the whole US.\nif, instead, you are looking for actual voter lists (names, addresses), then it's unlikely to ever be available at a national level (for some states, the sale of the lists generates revenue). \nHowever, some recent data (only) for a few states has been put online by Tom Alciere:\n\nConnecticut\nDelaware\nColorado\nFlorida \nOhio\nOklahoma\nUtah\n\nThese are ugly sites, presumably just automatically produced to generate advertising traffic. But for about half the states, the full dataset has been made available to download without restrictions, and the others are easy to crawl.\n", "Are there public transport data for Germany freely available?": "\nAgain, OpenStreetMap to the rescue: it has a whole tag scheme related to public transport services. Pointers:\n\n\u00d6PNVKarte (German, real domain [\u00f6pnvkarte.de] contains an umlaut, openbusmap.org is just a proxy domain) has a nice rendered world map, showing airport, train stations, rails, buses, subways, trams, ... worldwide (with varying degrees of coverage, of course). The corresponding page on OSM Wiki gives background information.\nMore general, the article Public transport on OSM Wiki lists all important tags for extracting relevant data from a data dump.\n\n", "data request - I need a KML file for Northern Ireland BT postcodes": "\nTabular data about post code data is at MySociety.org:\n\nONS Postcode Directory (ONSPD), February 2012 edition (thanks dvdoug!). This include full UK (including Northern Ireland) postcode locations.\n\nThose files don't include the headers, which is annoying. This page has a version from a different vintage which had a header added. If it's 47 columns, hopefully they match, but it's taking an age for me to download.\nIt looks like Northern Ireland has some more complicated licensing rules for commercial use, which may have contributed to that data being harder to find when the Wikipedia pages you're looking at were originally created.\nIt looks like someone has approximated shapes for the BT postcodes which you can see on this Open Street Map page. Getting those in a useable format is pretty obtuse, as best I can tell.\nTo conclude, I'm sorry that the original text of this answer was misleading: the source I identified doesn't actually have what you need. You may have to contact NISRA; maybe more requests from the public will chip away at their data publishing process.\n", "data request - Postal codes and city districts worldwide for download": "\nGeoNames does indeed provide the relationship between parts of cities and their postal codes. A few examples:\n\nParts of London with the postal code EC1\nPostal codes of districts in Austria called Innere Stadt (Inner City)\n\nYou can either use GeoNames' Postal Codes Lookup Tool, or you can download the database dumps in text format for many countries.\nThe data is licensed under Creative Commons Attribution 3.0.\n", "usa - What is the difference between US Census definitions of \"Urbanized Areas\" and \"Urban Clusters\"?": "\nLook at the National Center for Education Statistics Urban-Centric Locale Codes developed by U.S. Census https://nces.ed.gov/ccd/rural_locales.asp. You might be able to modify that approach to accomplish what you want. They use principal cities to delineate between \"urban\" and \"suburban\"\n\nCity - Inside an urbanized area and inside a principal city  \nSuburb - Inside an urbanized area and outside a principal city  \nTown - Inside an urbanized cluster and outside an urbanized area  \nRural - Outside of an urbanized cluster and outside of an urbanized area\n\n\n", "tool request - Twitter crawlers for tweets, retweets and social network": "\nHave you looked at: http://datasift.com/ it is where I would start. \nhttp://gnip.com/ would be another good option.\n", "doi - Persistent publishing of data. My nations DataCite does not cover my field; alternatives?": "\nFor either users or producers of open data, the Register for Research data Repositories (RE3) can be a useful resource.  In their own words:\n\nThe goal of re3data.org is to create a global registry of research data repositories. The registry will cover research data repositories from different academic disciplines. re3data.org will present repositories for the permanent storage and access of data sets to researchers, funding bodies, publishers and scholarly institutions. In the course of this mission re3data.org aims to promote a culture of sharing, increased access and better visibility of research data.\n\nAmong other information, the registry specifies what repositories provide the ability to identify particular datasets through DOI, URN, or otherwise.  It allows to search by discpline.  For example, searching exclusively for open access atmospheric science repositories gets 37 results.  Limiting it to those that have persistent identifiers reduces the number of results to 7, such as the World Data Center for Climate.\nThe registry aims at both producers and users of open data.\n", "weather - Systematic bias in NCDC GSOD climate dataset?": "\nWithout all of the original data, and its metadata, any answer here can only offer a guide as to how to start answering your questions.\nYour first question is: \"where is the problem?\"\nYour second question is: \"is GSOD biased?\"\nBoth of these must start with further statistical analysis.\nAnd you need to analyse the metadata for the datasets you are comparing.  Go through the definitions side by side methodically, and look for overlaps and for differences.\nCompare your available data at its most temporally disaggregate: more detailed than what you've done so far - monthly means can hide so much of relevance. If possible, try to recreate the monthly means yourself from the individual readings: that can often highlight issues that cause this sort of discrepancy: e.g. the way that missing data or outliers are handled.\nIt would also be very helpful to set out your prior.  And it would be helpful to analyse what's happening in the other months too.\nAs for using linear regression as an analytic tool in this case, do remember that it's a really blunt, unsophisticated tool; it will more often mislead than give useful information. Remember, courtesy of Andrew Gelman, the criteria for its applicability:\n\n\nValidity. Most importantly, the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient. . . .\nAdditivity and linearity. The most important mathematical assumption of the regression model is that its deterministic component is a linear function of the separate predictors . . .\nIndependence of errors. . . .\nEqual variance of errors. . . .\nNormality of errors. . .\n\n\n", "data request - Recent high spatial resolution images of the Tasman Sea": "\nWMO OSCAR has a (complete?\u00b9) list of current space-borne high resolution optical imagers, defined as instruments with a \"spatial resolution in the range of less than 1 m to a few 10 m.\".  Scroll down to \"Current instruments\".  I'm not an expert in high-resolution optical imagers, but I suspect the problems are as follows:\n\nThe higher the resolution, the smaller the field of view.  At high resolutions, it either takes a very long time to cover the entire globe, or data are only acquired on pre-order and for-pay.  Also, it's not free.  Some examples:\n\nGeoEye has a resolution of 0.41 metre, and covers the globe in 6 months.  Otherwise, data are available for pre-order for a specific location, but the field of view is too small so it likely won't help you.  Competing WorldView has similar properties.\nAWFS has global coverage in 5 days, but the resolution of 56 metre is insufficient.\nCMT has a resolution of 4 metre, and global coverage in 104 days.\nETM+ on LandSat has a resolution of 15 metre, and global coverage in 16 days.\nHiRi on Pleiades has a resolution of 0.7 metre, and global coverage in 26 days.  I don't know if they actually acquire information accordingly, or if this is just a theoretical limitation: it would be an awful lot of data.\n\nIf you browse through the list I linked above, you will find many more examples.\nData may be difficult or impossible to obtain, and most data are not free and open.  In fact, many high-resolution optical imagers are carried on spy satellites.  Good luck.\n\nI don't know practically speaking how to get the data.  Many instruments are commercially operated.  You might be able to get more information by contacting companies such as GeoEye, RapidEye, or DigitalGlobe.  Perhaps your best bet may be to see if any agency can get recent Pleiades data.  If WMO OSCAR is correct, it might just be good enough: 0.7 metre and 26 days appears to be the best thing out there.  Again, data are not free and open, but search and rescue organisations in most countries should have access to the necessary funds.\n\n\u00b9It seems not to include spy satellites, but those might not have open data available...\n", "data request - Where can I find the training logs of (as many as possible) athletes?": "\nTry these:\n\nLongitudinal study of the effect of high intensity weight training on aerobic capacity\nEffect of endurance training on lung function: a longitudinal study.\n\n", "geospatial - Where can I find a longitudinal survey that includes sociographic data, including religion affiliation, over a long period?": "\nHere are some additional studies that may be useful:\nMost likely meets most or all of your criteria:\n\nAmericans' Changing Lives: Waves I, II, III, and IV, 1986, 1989, 1994, and 2002 (ICPSR 04690)\n\nMay meet some of your criteria:\n\nYou mentioned ARDA studies already, but the National Study of Youth and Religion (3 separate waves) may be helpful.\n\n", "data request - Chicago Traffic-Related Fatalities": "\nThis data used to be publicly available through the Illinois Department of Transportation's Safety Data Mart. However, the Department's new Safety Portal is now only accessible to other government agencies. \nFor Chicago data, you can get historical data from the Chicago Crash Browser\n", "data request - Statistics of US/Europe Businesses, Advertising Agencies Specifically": "\nGut feel, you will have to dig this data up country by country.\nThis list should be fairly up to date for the UK.\nhttp://www.ipa.co.uk/framework/sections/agency/agencies.aspx?display=list&menu=open (something like import.io or scraperwiki will help you turn the page into data - I don't think they have an API)\nI found this http://eaca.adforum.com/search/agency/idCountry/783 which has a variety of search options for agencies. Again, it is a site not a db. So you would have to use a scraper of some variety. There is still no guarantee of accuracy or completeness.\n", "data request - Are there any open datasets for soccer statistics?": "\nRecently, the paper Linked Soccer Data was published. In it the authors describe how they combined various football-related datasets, such as http://fussballdaten.de/. Some of the data they covered can be viewed through this demo application.\nThe paper also mentions other relevant sources of football data, including the openfooty API.\n", "real time - Realtime Data - Why and Who?": "\nLearning:\nThis is huge and can be applied to anything.\nImagine trying to learn how to play poker.  You can only play one hand every hour.  How long will it take you to learn how to play poker?\n\n\"Not too out of date for my use case\"\n\nIf the company is smart and whose goal is to make a profit, they will change their \"use case\" or even completely revamp the point or meaning of the use case so they can derive insights like what their competitors are doing, and much more outside the scope of this response to, for lack of a better word, win.\nWith a realtime source feed this doesn't even cost much money.  I actually am 100% broke at the moment ($0) but I could program a few things on my computer or even (lol) go to the public library, rent out an amazon web server for 25 cents an hour that I remote desktop into, and start gathering insights with the correct programming and know-how.  Then, with correctly applied statistics, start deriving insights.\nThis concept mostly involves repeatedly testing different products or permutations of ideas (which is essentially what everyone does), but in the accelerating marketplace of the online world (and to the retail world as well since they are beginning to blend), competition is not going to go away.  \nWhomever learns the quickest wins.  If you have direct competitors, whomever has the economic advantage will win, even if it's an accumulation of several different economic advantages. \nYou could say they got that economic advantage in a different way because maybe they:\n\nAlready know more about the market\nInitially had a longer time frame to learn more\nHad better connections/got luckier/hired the smartest people\nAre able to learn much faster than their competitors\n\nThe internet changes VERY rapidly.  So would you agree with me that numbers 1, 2, and even 3 are becoming more irrelevant nowadays?  I didn't list money/resources because, usually, all they let you do is learn more quickly in some form or fashion.  Sometimes there are barriers to entry (example: oh no, we can only do 5,000 API requests per day and can't pay more for anymore?).  Learn more and you'll figure out a way around anything.\nColleges don't even teach the skills that are in most demand today! - unless its a programming language or something.  And if they do, then the teachers most likely don't know much more than the students!  Think about \"social media.\"\nWho teaches that?\nHow many people are looking for experts in it?\nNow look at my poker learning analogy.  \nIf you think money is the biggest problem, then you're not learning quickly enough how to potentially point out trends that could persuade someone to invest, or you're not able to see all the little miniature data points that reveal the hidden picture - the one that could be lowering costs by a factor of 4.  That is, assuming you're able to analyze them.\nYou'd also not see how the real-time feed allows one to quickly test ideas and throw them out or keep them.  What if you wanted to test permutations of ideas?  Assuming sample size is large enough, you'd only be able to test one permutation (of which you'd probably have lets say 3x3x3=27 of) every day or whatever interval.  How would you sell that to inner management?  Think they'd be more easily persuaded if you could get the answer within an hour and do it cheaply? \nWhat about factors that may change during your 27 days of permutation testing that would affect the outcome like market factor forces, seasonality, etc.\nIt could be argued that getting the data over a longer time frame is better because you'd hope all uncontrollable factors would be included in the analysis as an average; but what if you wanted to control one of those factors and, say, just test at 4pm-5pm with all the permutations?  Or any other innumerable cases I could think of.  \nThe factor of time here is most important in my opinion to be honest.  Why?  You may sacrifice a bit of accuracy by not doing the test for a lengthy period, but that sacrifice will lead you to conduct more experiments with better accuracy...ones that you could immediately implement.\nNow you could also argue your sample size isn't large enough.\nMost people assume you need a massive sample size.  Something to consider though is the validity of your analysis increases with sample size, BUT it increases at a quickly decreasing rate.  Something to consider - and apply with realtime data.\nI love massive sample sizes too, but how do you think drugs get approved in clinical trials or medical research studies are done with 20 people? They can't test them out on thousands and thousands of people.  They use statistical methods.  \nExample:  They look at all results of EVERYTHING, even things you may think are completely unrelated they record.  Blood pressure, heart rate, hair growth, skin color change, rashes, change in color preference perceived mood, sleep time, computer time, perceived interest, the combination of each factor and the combination of each factor on each other combination of factors comparing that to known full population averages are...factoring in time..etc..The data in small sample sizes is there (ie. real-time data would be considered a small sample size if you're using little snapshots of it), it's about setting up for it and digging deep.\nMany, many other statistical methods are available like fractional factorial, non-parametric, design of experiments, deriving simulated numbers from your numbers based on standard deviation and probability and seeing how they fit into your model, etc. methods as well.  \nOk enough belaboring on my part.  Here are a few examples too:\n\nIP Addresses & What Their Attributes are at that Time\n\nGEO location to IP services such as Maxmind constantly update their database because IP addresses change so frequently.  For example, for a day or even 3 hours you could have your IP be used as a Tor exit node meaning alot of websites will block you, and do it quickly, or list you as using a \"proxy.\" This is based upon data from GeoIP databases, which somehow will pick up on you being a tor exit node and will change the attributes of your IP quickly and accordingly to that information.\nAn attempt to geolocate a potential customer based on the results of a GEO-IP database and thus discovering potential facts about them using a big enough sample size that you regress into a big database of known facts about that area (like avg level of income) using census data.  =x   This involves the central limit theorum.\nThwarting fraud and identity theft.  If your IP is listed as a \"proxy\" in GEO-IP databases, the merchant will likely pick up on that (by doing behind the scenes requests to the their GEO-IP database provider) and reject your attempt to purchase anything.  Essentially they are assuming your attempt to hide your identity must mean you're also hiding other things.  Plus, statistics from transactions reveals this to overwhelmingly be true anyway.  \n\nImagine you are a thief and stole someone's CC number and are trying to buy stuff online.  Are you going to use your home IP address?  Probably not.\n\nOnline Media Buying\nI can actually add more to this since I buy media online.  Albeit I admit that much of that does involve bidding and you admit the realtime factor being important for.\nHowever, there's MUCH more to it than that.  One thing that REALLY bothers me when I'm buying media is the lack of real time RESULTS being published as well.  Say I want to raise my bid price really high and see what happens - I'm testing the market.  Most systems will only charge you when you do this as if you were bidding slightly higher than the next person, but sometimes they won't, or in my experience they actually just say they do and charge you some strange inflated ratio.\nIn our example lets say I get my results every 30 minutes.  During that 30 minutes my bid price was $10 dollars per click.  A very high bid on average.  I need to know the results of this test to judge all sorts of things about the market (and by market it could mean a specific website, or a specific keyword, anything really) like competition, size, potential of market, conversion rate, etc.\nI get my results back 30 minutes later and, oh crap, I just spent $10,000 because I was blind during the test.  Had I been able to see it realtime, I could of just done it for 30 seconds and known a good outcome.\nYou could argue that I should have gradually raised my bid.  If so, I point you back to the poker analogy up top and remind you of all the permutations I have to test as well.\n\nThis also gets even more complex when you're working with sales reps at a digital marketing firm and the client or your boss wants answers quickly.  This means you have to conduct a lot of tests on a very, very wide permutation of variables in order to be able to give them the information they seek or just to be able to found out what the profitable combination is.  \nI could tell a client or anyone else alot in about 10 minutes with realtime data, using statistical analysis (a skill very few in my field have unfortunately), and trying different permutations...mostly involving macro changes (macro changes let you cover more ground at once - it's essentially a compensation for the inability to do everything real-time).  Without that, it could take a week.  Time is money.\nBeing able to make a move every 30 minutes or every morning won't work well when you have so many permutations to test (bid, landing page, source, offer, etc. etc. etc.).\nAs another example, a company I worked for had a tech team that could only change the landing page every 2 weeks - whereas it would take me 30 seconds to make the change I wanted...but I wasn't allowed to do it.  In fact, I wasn't even allowed to talk directly to the person making the change.  It had to go through 3-4 different and usually unnecessary people.\nImagine being able to make a move every 2 weeks and during those two weeks having to run the campaign and deal with the pressure and constant reminder that the campaign was not profitable.  Back to the poker learning analogy.\n", "releasing data - What's the best way to host map tiles?": "\nTake a look at what OpenStreetMap does. There's a page describing the nature of tile server disk usage. If you go up to zoom level 18 worldwide, you're talking about 91,625,968,981 tiles, which would take around 54000GB of disk space, but would mostly never be viewed.\nSo I'm not sure if it would ever be a sensible approach, but having said that, I heard that MapBox do pre-generate all their tiles when hosting a tile set. I think they go up to higher zoom levels just in the cities or something like this.\nThe approach OpenStreetMap tile servers use, is a combination of on-the-fly rendering and caching. The management of this is done with a specially written apache module called mod_tile\nEither way, if you want to do things worldwide up to a high zoom level, you need something a little more complicated than a filesystem full of 256x256px PNG images. mod_tile stores files in cache as a 'meta-tiles'. MapBox uses a format called MBTiles to store all the tiles in a database file.\nI mentioned MapBox a few times. If you pay them they'll render & host tiles for you. There's various other providers of tile hosting/rendering and other map services\nIn general you'll find the OpenStreetMap tech community have a lot of experience with this kind of thing. You can contact them in various ways. There's even a question & answer site: https://help.openstreetmap.org\n", "government - Besides Vancouver and Chicago, what cities have open data on rental buildings bylaw infractions?": "\nA growing number of city open data sets is federated to http://Cities.Data.gov.  There are quite a few rental building violation datasets accessible: http://www.data.gov/cities/Community/Cities/Datasets  You can find some additional information at http://Counties.Data.gov and http://States.Data.gov.  These include non-Socrata based local governments as well.\nSpecifically, you might look at:\n\nSeattle's code violations\nNew York's building code violations\n\n(Disclaimer: I am the Evangelist for Data.gov)\n", "How can I get the Wikidata inter-language links?": "\nThe official Wikidata dumps are still in a very early stage.\nAt the moment, you might find these processed files helpful, especially the one that ends with links.ttl.gz (currently http://semanticweb.org/RDF/Wikidata/turtle-20130801-links.ttl.gz), which is a Turtle file that provides the inter-language links extracted from the Wikidata dump.\nThe export scripts are available on GitHub, and you might also find the corresponding announcement on the mailing list interesting.\nPlease also have a look at the related question How can I download the complete Wikidata database?\n", "The Healthplan finder API stopped responding, where can I get support or more information?": "\nFirst, verify that you've tested your script on a different system. Then I would proceed to:\n\nCalling HHS at 800-318-2596\nTrying a live chat\nLeaving feedback on this dataset at the Developers' Center\nSending a tweet to @healthcaregov\nLeaving a post/send a message on their Facebook page\n\n", "Open Data Standard for Stack Exchange?": "\nI don't know much about open data standards, but as far as I'm aware, no. Stack Exchange itself isn't based on any particular standard.\nWe provide all publicly-accessible data via an XML-based [data dump, but I believe its schema is simply mimicking the database schema we also expose via the Data Explorer.\n", "licensing - Is data scraped from Govt agencies and in public domain considered open data?": "\nIn order to license the data you have to hold a copyright, which it sounds like you do not.\nIf you believe the data is in the public domain, then you can consider Public Domain Mark \n", "usa - GitHub license for code written by US Government Employee": "\nAs a work of the US government, there isn't any license appropriate for the work, because it's already in the public domain (in the United States). So a license like the Unlicense (or CC0), in which the licensor is entering the covered work into the public domain, doesn't work. Some text that acknowledges the public domain status in the US is helpful (and desirable), but it's not a \"license\".\nThe above situation only applies to the US, unfortunately -- government works are potentially copyrightable in non-US contexts. So, some formal text or license that enters the work into the worldwide public domain is appropriate (and desirable).\nMy favorite example of this is what HHS has done on their ckanext-datajson extension:\n\nAs a work of the United States Government, this package is in the public domain within the United States. Additionally, we waive copyright and related rights in the work worldwide through the CC0 1.0 Universal public domain dedication (which can be found at http://creativecommons.org/publicdomain/zero/1.0/).\n\n", "usa - Survey of illness and symptom data": "\nyou ought to start reading http://asdfree.com and play around with http://www.google.com/trends/  :)\nthe easiest county-level data set to work with is the area health resource file.  this won't have individual cases, but it'll have many small-area counts of different things health-related over many years.\nthe data set probably with most of the characteristics you've described is the surveillance epidemiology and end results cancer incidence data.  it's got data over many years, but only for a handful of (nationally-representative) states\nof the other major health surveys - brfss, meps, nhanes, and nhis - only meps has event-level/icd-9 data, only brfss has state- or sub-state-level data, and only nhanes has biological information.\n", "extracting - What is the best way to get airline schedule data from pdf files": "\nIf the pdfs are very similar each time you check them, then the best path may be to write a custom pdf scraper for each low cost carrier. Check out this tutorial, http://blog.scraperwiki.com/2012/06/25/pdf-table-extraction-of-a-table/.\nYou should also keep your eye on https://github.com/jazzido/tabula, It's not quite there yet, but may be a solution soon.\n", "real time - Obtaining ADSB Mode-S Data Feeds for Aircraft from the FAA": "\nThe FAA does not seem to provide a stream of ADB-S data, but does provide a stream of about airplane location and disposition called Aircraft Situation Display to Industry. Getting access to this stream is not completely straightforward, but you can start with the FAA's page on the program.\n", "data.gov - CKAN API questions": "\nYou've asked three separate questions in one question, I don't think this fits into Stack Exchange's one question, one answer model, should have been three separate posts maybe. But anyway..\n\nIt appears that every API response includes a JSON piece with help information. can this be excluded?\n\nNo :) Not as far as I know. Just ignore it. I agree it seems a little unnecessary.\n\nThe response does not appear to include information on the total number of hits. how can API clients get information on this total number of hits of searches?\n\nThe package_search link you posted does actually contain the number of hits, that's the \"count\": 8511 bit.\n\nDoes the CKAN API support spatial selection criteria as seems to be implemented at data.gov? it appears the catalog.data.gov UI does do some form of spatial filtering (within/overlap is unclear), but the API returns an error when using the ext_bbox parameter: \"Search Query is invalid: \"Invalid search parameters: ['ext_bbox']\"\"\n\nThe CKAN version that catalog.data.gov is using does not support ext_bbox parameters on GET requests, but later CKAN versions do:\nhttp://demo.ckan.org/api/3/action/package_search?q=test&ext_bbox=-180,-90,180,90\nOn catalog.data.gov you need to send a POST request as in the following example:\ncurl -X POST -d '{\"q\":\"environmental\", \"extras\": {\"ext_bbox\": \"-130,30,-50,40\"}}' http://catalog.data.gov/api/3/action/package_search\n\n", "business - Job satisfaction data": "\nThe Health and Retirement Study (HRS) is a longitudinal panel survey which includes job satisfaction questions in one of its modules. It primarily focuses on tracking US individuals 51 years and over.\nThe GSS also has a few questions on job satisfaction.\n", "weather - API for sun radiation / illuminance data?": "\nAs no one's given anything else so far, so might as well make this an official answer.\nI'm not aware of any free APIs specifically, but there are various data sources that might be able to give you what you need:\nThe US Department of Energy's National Renewable Energy Lab offers maps of solar radiation in the US, both annual and monthly averages, although the data is only through 2009:\n\nhttps://www.nrel.gov/gis/solar.html\n\nNOAA offers data from 1991 to 2010 from 1500 ground stations in the US, with nominally hourly cadence:\n\nhttps://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/solar-radiation\n\nOpenEI (Energy Information) maintains a registry that lists 43 'irradiance' datasets and 26 maps (some are the NREL ones I already mentioned):\n\nhttps://data.openei.org/search?ra%5B%5D=Solar+Power&dt%5B%5D=data&sort=relevance&size=25&q=irradiance\n\nSolarGIS might have it, as they sell products that look to have more granular information, but their only free (attribution required) offerings are annual averages of two types of 'irradiation'. (I'm not sure how that differs from 'irradiance'):\n\nhttps://solargis.com/maps-and-gis-data/overview\n\nAnd the company that specifically markets an API (not free) for this data is Vaisala.  If those other resources aren't sufficient, you might have to look at their pricing:\n\nhttps://energy.vaisala.com/en/support/solar-prospecting-tools/how-accurate-are-solar-prospecting-tools/\n\nPS: What is different between solar irradiance and solar radiation?.\n", "government - What are your responses when people tell you they can't open data?": "\n\"Oops! I'm sorry, I can't be your customer.\"\n-- by which I simply mean this: they can't, and so I can't. There are plenty of other people who are interested in having an open and honest relationship with their customers; companies who don't, aren't interesting to me, and I'm frankly unwilling to waste my time with people that I can't trust.\n", "documentation - What do the permit types in Chicago's Building Permit Data mean?": "\nThose are pretty common terms for building permits.\n\"New construction\" means that they're building (or placing) a new building.\n\"renovation/alteration\" means that there's an existing building that's being modified.\nWhat qualifies as a renovation can get a little interesting by area ... I remember one of my co-workers telling me that in Virginia if you leave one wall standing, you can declare it to be a renovation, and avoid having to pay all of the fees required for new construction if you're looking to tear down an existing building.\nSee the city's website for details on the other permit types.\n", "transportation - Is there Open Data on car engine efficiency?": "\nThanks for your question. I'm not an expert in this space, but I asked around and was pointed to two resources at NREL you might look at:\n\nThe efficiency curves available in the FASTSim model that are posted at http://www.nrel.gov/fastsim. I was told by the managers of this dataset that it may be used for commercial purposes, just not redistributed commercially.\nThe Caltrans cleansed dataset will be posted at http://www.nrel.gov/tsdc in the next couple of weeks, which will includes a subset of vehicles where OBD data was recorded along with GPS speed profiles.\n\n", "data request - Where to get IMDb datasets": "\nNot sure if this would classify as a comment or an answer, but it's useful information nonethelss:\nSo in reading this question I HAVE to point this out - ever heard of the paper?: \n\nArvind Narayanan and Vitaly Shmatikov. \"Robust De-anonymization of Large Datasets (How to Break Anonymity of the Net\ufb02ix Prize Dataset)\". \n  The University of Texas at Austin February 5, 2008.\n\nFull text is at:\nhttp://arxiv.org/pdf/cs/0610105v2.pdf \nIt's quite a famous paper and was even on the news when it got published.\nHere's the abstract: \n\nWe present a new class of statistical de-anonymization attacks against\n  high-dimensional micro-data, such as individual preferences,\n  recommendations, transaction records and so on. Our techniques are\n  robust to perturbation in the data and tolerate some mistakes in the\n  adversary\u2019s background knowledge. We apply our de-anonymization\n  methodology to the Net\ufb02ix Prize dataset, which contains anonymous\n  movie ratings of 500,000 subscribers of Net\ufb02ix, the world\u2019s largest\n  online movie rental service. We demonstrate that an adversary who\n  knows only a little bit about an individual subscriber can easily\n  identify this subscriber\u2019s record in the dataset. Using the >>>Internet\n  Movie Database<<< as the source of background knowledge, we successfully\n  identi\ufb01ed the Net\ufb02ix records of known users, uncovering their apparent\n  political preferences and other potentially sensitive information.\n\nI looked at their citations for clues but they only thing they cite verbatim is:\n\nIMDb. The Internet Movie Database. http://www.imdb.com/, 2007.\n\nThis is also quite a while ago too.  However going through the full text of that article, you may be able to glean some clues as to how they got their data and replicate those - so this could potentially help you.\n", "data request - Shaded KML file for US Zip Codes? Doesn't have to be perfect": "\nThe shading of shapes on a GIS map is a design choice; it's not a characteristic of the data. Not all choropleth maps are opaque, even if some that you've seen are. A designer made that choice.\nIn Google Earth, you can apply styles to a layer using the \"Edit > Get Info\" command. On that, choose the \"Style, Color\" tab and change the color and opacity in the \"Area\" section. Getting labels like on http://www.usnaviguide.com/zip.htm is not something I know how to do in Google Earth.\nNote that I find that file you pointed to at filosophy.org to be \"very laggy.\" I think that happens when you have over 50,000 features (shapes).\n(edited in response to comment thread; comments may now seem unrelated)\n", "Content-driven API or RSS feed": "\nMaybe you can find here something; but what would be a business model of an API \"news for free\" provider?\n", "social process - How should I respond if a government official says he won't release data because no one cares about the data?": "\nBoringness of data sets is not a legal reason to withhold data.\nThe only thing that matters is: 1) does your state/country have a public records disclosure law? and 2) What exemptions does it have for not disclosing? i.e. most states exempt agencies from disclosing records about undercover police officers and certain kinds of health records.\nAnd 3) of course, does the government actually collect the data that you are asking for?\nIf so, then they can be compelled to send you the data, as per the state's regulations. If the government doesn't actually collect that data, then no, they don't have to collect it for you. Also, most state laws say that an agency does not have to provide new types of records or queries to you, if those queries aren't generated as part of the government's business.\nFor example, an agency may fight against a request that asks for pension amounts, aggregated by year/department, if they never conduct that aggregation themselves.\n", "social process - How should I respond if a government official says she won't release data because she doesn't think it will be useful?": "\nThe first three I'd lump together ... the others, I'm not so sure.  For the first three, I'd ask them:\n\nWhy did you collect the data?\nIf the data has no use, why do you keep it?\nWhat do you use the data for?\n\nThe next two require more information ... they might be fiscal in nature, and in today's budget situations, it's quite likely that departments are already short-staffed; changing how they do things might have significant costs, especially when the data is actually in high demand.\nThe last one ... well, if it's a federal agency, I'd be inclined to bring up FOIA.  If it's a state agency, I'd bring up the equivalent law, if it exists.  I know a few people who argue against opening up the data because they've had to deal with too many crackpots who want to discuss the data and/or refute it without actually understanding what the data is.  (it's amazing how many times you can explain compression artifacts to people, and they still insist that it's UFOs, and that you're just trying to cover it up)\n", "usa - How do I Get that Juicy Economic Data from BLS.gov (Bureau of Labor Statistics) into Zip Code Format?": "\nI used some BLS data for a recent project. It took me a little while to dig through. But there are essentially 3 ways to get access to their data. \n\nIs from their 'downloadble' sets published as links from HTML. It sounds like you have got to this.\nQuandl have some of the BLS data curated. That have made it nicely searchable, filterable and available under REST.\nAll of the data the BLS publish is here in raw format: ftp://ftp.bls.gov/pub/ the doc folder tells you what each of the sub directories in time.series directory is for. This is the overview. As far as I know this is totality of what they have published at the most granular level. Overview tells us this the cx file and the cx file tell us this.\n\nThe work is in curating this data. There are joins needed through structured keys. This is not hard, but you need to go through the process.\nTo the best of my knowledge, if the data is not here, the BLS don't have it. I would love to be proved wrong and shown where more granular data is.\n", "How do you respond when government cites costs for not releasing data?": "\nFor point one, I would probe more. Custom programming to get data out, a portal, what? \nFor point two, I try to make sure if it's actually true. Try to get the details of the software they are using and then figure out what the capabilities of the software are. Talking directly to the vendor is often the best next step.  This excuse is often given by people who are not well informed about what is technically possible, so I would also attempt to talk to the whoever is responsible. Assuming it is true, then ask them to release the data in whatever format they can. If the data is interesting enough, then the community will figure out how to make an obscure data format open, a la http://treasury.io/.\nFor point three, they are likely already spending substantial staff time responding to FOIA requests. If they make their most common FOIAed documents open by default, they can save staff time.\nFor point four, yes. They will have to do some different things.\nFor point five, this really depends upon their current information management practices. If they have good, computerized systems in place for managing their data internally, it should be easy to automate public data releases. If they don't have those internal systems, then, you might be in for a longer effort which involves finding an internal champion for better internal systems and supporting that effort.\n", "How do you respond when government cites time concerns for not releasing data?": "\nThese seems like objections for a government that hasn't started making data public. If that's the case, then the key thing is go get them to make a start.\nReal time data is great, but there's lots of really valuable data that changes very slowly, can be easily exported. GIS data and budget data are often good targets, but choose something that makes sense in your local context.\nWhen a government is getting started, they don't need a dedicated open data web site. They can just make a new page on one of their existing sites with links. Make sure though, that they have a way of keeping track of traffic and downloads. You'll want to be strategic in selecting data sets that will get used so you can make the case for releasing more data later.\nThe last point seems different than the others, because it seems like you are requesting a particular piece of data, not just advocating for open data. If you get this request, I would try to find out what the software is and talk to the vendor to see what's possible.\n", "How do you respond when government says it should be selling its data, not opening it?": "\nDepending upon the situation, you could respond by saying\n\nThat violates the applicable FOIA law which usually says that records must be provided only at the cost of responding to the request. This might be substantial, but cannot, legally, be a money maker.\nEven if it's legally permissible to charge for the data, the overhead of taking and processing payments is likely to wipe out any profit\nThe data that people are most likely to pay is almost always timely. The data owner can increase their demand by making a staler version available for free and charging for more immediate access. In the private sector, many economic indicators are free, but you can pay for the privilege of getting the data a few minutes before anyone else, ex. https://www.ism-chicago.org/chapters/ism-ismchicago/barometer.cfm\n\n", "How do you respond when government says it needs more proven results to release data?": "\nFirst,\nTell the story of peer cities or agencies that released data and realized benefits. It's hard without a specific target, but by now cities and agencies of very different sizes have released data and seen people run with it.\nCode for America's Brigade are a good resource, for telling these stories for cities, http://brigade.codeforamerica.org.\nBigger examples you could point to\n\nGTFS\nWeather\nOpen Street Map\nThe whole field of geographic market segmentation which largely depends upon census data\n\nSecond,\nThis is what Hackathons are good for. It demonstrates the reality of the premise that if you release these data then people will do things with it, and that the government will get good press for making the data available.\n", "How do I escape a single quote in a Socrata SODA 2 API call?": "\nFrom Socrata Tech Support\n\nThe [Socrata SODA 2 API] expects strings to be enclosed in single\n  quotes. You can use double single quotes to escape it within a string.\n\nhttp://data.cityofchicago.org/resource/xh8b-g55w.json?$where=license_description='Caterer''s Liquor License'&$limit=1\n", "How do I escape an ampersand in a Socrata SODA 2 API call?": "\nEscape the & as hex code %26.\nhttp://data.cityofchicago.org/resource/xh8b-g55w.json?$where=license_description='Special Event Beer %26 Wine'&$limit=1\n", "data request - Historical values for the German \"Sonntagsfrage\"?": "\nJust found the Overview of the Allensbach Institute, which has data at least for the current legislative period. The data can be easily scraped, see for example my R script.\nThis helps a bit, but if you have another answer, I will accept and upvote yours.\n", "data request - Database of names of Japanese and non-Japanese people": "\nFinally, Wikipedia and its sometimes tragical affection to compiling lists on everything and anything becomes handy: \n\nList of most common surnames in Asia#Japan (Wikipedia). Every continent and major country seems to contain a pretty extensive list, sometimes even with estimates on number of occurrences.\nCategory:given names by culture looks pretty extensive: male, female and neutral names cover over 1,000 articles. If they are still incomplete, looking for categories of famous Japanese persons might be complementary.\n\n", "machine learning - Data Set for Predictive Modelling": "\nIf you're already using a Python toolchain, then the easiest option is probably skdata. See the data sets it provides.\n", "data request - Product catalog datasets": "\n\nBestBuy publishes product data through Products API or in RDF/XML dumps (see sitemap here).\nLinked Open Commerce is an attempt to aggregate data from e-shops, including descriptions of products.\nAn older (2009) attempt to publish product data is ProductDB.\n\n", "Data on Android/iPhone apps by user?": "\nauthors of the BAM application for android may have some data you are asking for.\nIt can recommend apps based on other users that have the same set installed.\nhttps://play.google.com/store/apps/details?id=com.bestappsmarket.android.bestapps&hl=en\nE.g., blind users if they connect - they would recommend this way apps to each other.\n", "ethics - Is metadata about data from my clients my data or the clients?": "\nI think you should check first if the agreement you have with your customers allow you to reuse and mix their data and in so in which conditions.\nYou can also look at the standard and guideline promoted by of the Dutch DPA or the European Data Protection Supervisor in term of privacy. \n", "transportation - Stream Airfare data": "\nAirlines distribute fare information through something called a global distribution system (GDS). An example of one such system is SABRE. There are lots of different kinds of GDSs out there, and you can find them with a simple internet search. The International Air Transport Association (IATA) is probably your best resource for finding out more about GDS and trends there.\n", "data request - Northern Ireland electoral wards shapefile": "\nIf you go to the NISRA website then you'll get all shapefiles in both .shp and .tab formats for 2011 census as well as some other geogrpahies.\nBe awere of the copyright.\n", "geospatial - Bathymetric contour data for North America": "\nOpenStreetMap to the rescue! Its cycling map shows contour lines and shaded relief (even in Canada):\n\n(source: opencyclemap.org)\nLike most open data projects (probably Natural Earth, too), they use NASA's SRTM (OSM Wiki) dataset that has global coverage, as far as I know. As you seem to need contour shapefiles, refer to the article Contours (OSM Wiki) for a workflow on how to convert raw SRTM data to shapefiles using GDAL.\nJackpot: Apparently, OpenDEM went through the effort of preparing contour shapefiles worldwide from SRTM with 25m precision. They offer a pretty convenient download for an arbitrary region. (Download manager or wget recommended.)\nHow I found the data: For all my spatial data needs, usually before googling, I have a look at the OpenStreetMap Wiki and search for relevant keywords, in this case: contour, height, SRTM.\n", "language - Frequency Analysis Character Distribution Data": "\nSecond try: Google Ngram Viewer contains raw counts of 1-, 2-, ...-grams of text, retrieved from its book scanning endeavor. The section 1-grams contains counts of the occurence of lettres, numbers and even punctuation. They are provided as tab-separated value files, so the frequencies should be derivable with modest scripting efforts.\nFound via Wikipedia article Text corpus.\n", "geospatial - UN/LOCODE Copyright Status": "\nI believe minopret is incorrect on the use restriction of UN/LOCODE to non-commercial use w/o permission. There is no where on the UNECE site that indicates such a restriction. All the verbage would indicate the contrary. The terms of use simply indicate that the material is w/o warranty and the user will indemify the UN.\nThis statement can be found on the web page: http://www.unece.org/cefact/locode/locode_since1981.html \nUN/LOCODE is freely available to all interested users. It can be consulted and downloaded from the web-site www.unece.org/cefact/locode and users are welcome to propose additional locations; for this purpose a new, automated request procedure has been introduced, as described below: \nSometimes users get confused on what code lists from the UN and ISO are free to use and which are copyrighted and you need to purchase. For example, the ISO 3166-1 codes are free to use, but the ISO 3166-2 subdivision codes are not. \nEDIT: Feb. 15, 2014\nThe ISO 3166-1 Country Codes will no longer be freely available from the ISO website after Feb. 20, 2014. You will need to purchase a subscription for 300 CHF (http://www.iso.org/iso/home/standards/country_codes/country-codes_new-product-info). \nI have placed an archived version (Feb. 15, 2014) of the English and French datasets in TXT and XML format at:\nhttp://www.opengeocode.org/archive.php\n", "How to get daily updates from Wikipedia?": "\nRight now, there is no good solution. The options I can think of are:\n\nJust download the normal dump whenever it becomes available. This means you won't get daily updates at all. For the English Wikipedia, a new dump is generated about once a month.\nUse the adds/changes dumps (already mentioned by ojdo). There are two problems with using this:\n\nIt doesn't include all changes. Specifically, information about moves, deletes and some undeletes are not included.\nIt's an experimental feature and I wouldn't be surprised if it were discontinued in the near future (because a better option is coming, see below).\n\nUse the API (possibly combined with the IRC recent changes feed) to get the text of all new revisions to a wiki. I think this might work for very small wikis, but it's certainly not feasible for huge active wikis like the English Wikipedia.\nUse binary incremental dumps (which is a project I built over this summer). This will do exactly what you want: it will allow you to download only changes since the last dump and it should allow creating dumps much more often (the hope is for daily dumps). The only problem is that this is not live yet, so you will have to wait before using this (I have no idea how long, but I would expect it to go live this year).\n\n", "data request - World gas/petrol prices at the pump": "\nMy best guess is World petrol prices on MyTravelCost.com. They give you a large, configurable bar chart of gasoline prices. However, their source description can be described vague at best:\n\nThe data are drawn from a variety of sources including official government materials, oil companies, online resources specializing in gas prices, and others. These sources provide reliable information about fuel prices in a large number of countries. For the other countries, we provide an estimate using previously published data.\n\nBonus: If you want to access the raw data, simply right-click on the graphic an open the (incredibly long) image URL in a new tab. It contains the raw data for the bar chart like this:\nhttp://www.mytravelcost.com/graph.php\n?data=0.02,0.09,0.15,0.18,...\n&titles=0.02|0.09|0.15|0.18|...\n&outsideGraphTitles=Venezuela|Iran|Saudi%20Arabia|Qatar|...\n\n", "data request - Crowdsourced local consumer prices": "\nGas Buddy, the Price of Weed website (SE does not allow a link) and Craigslist all come to mind.\n", "usa - Government shutdown causing linked open data to go away": "\nArchive.org might have already crawled the site for you. Check out the Way Back Machine: http://wayback.archive.org/web/query?type=urlquery&url=&Submit=Go+Wayback!\n", "usa - Federal Shutdown: Raw SNAP (Food Stamps) participation data source": "\nIs this helpful? \nMonthly Data -- National Level:\nFY 2011 through June 2013\nhttp://webcache.googleusercontent.com/search?q=cache:r9JbtaubWlMJ:www.fns.usda.gov/pd/34snapmonthly.htm\nAnnual State Level Data:\nFY 2008-2012\n\nPersons Participating\nHouseholds Participating \nBenefits \nAverage Monthly Benefit Per Person\nAverage Monthly Benefit Per Household\n\n", "data request - Stemming of long English text": "\nGiven that language is not a fixed thing, I'd hesitate to put much stock in a fixed database of \"definite\" stems. \nHere's the source code for the NLTK (python) Porter stemmer (GPL). It looks like it has no serious dependencies on anything else in NLTK -- just an interface that you could discard and some stuff for unicode compatibility that you could adapt. So if you're using Python (and can work within the GPL), this could get you started pretty quickly without the weight of the entire NLTK library.\nIf you still feel that the stemmer is likely to make mistakes, it wouldn't be hard to add an override that checks a dictionary of 'definite' stems before dropping back to the algorithm. Maybe you deploy it with an empty dictionary and then add corrections as you find errors.\nPS this Stack Overflow post \"Stemming algorithm that produces real words\" looked like it might relate to your question.\n", "data request - Looking for lat/long coordinates of proposed Northern Gateway pipeline route": "\nIt looks like Pipe Up Against Enbridge has the data your after, i.e., http://pipeupagainstenbridge.ca/the-project/map \nMy colleague @HughStimson is contacting them about the data in the hopes that we can get permission to host as part of the GeoDataBC collection (https://github.com/geodatabc). (PS We're looking for contributors!)\nIn the meantime, it appears that the necessary data is linked from that map, e.g.:\nhttp://pipeupagainstenbridge.ca/images/map/pipelineroute.kml\nYou'll probably want to check-in with them about the license / source for the data.\nGood luck! \nPhillip.\n", "What models and data is used for traffic predictions?": "\nIn this MIT project you can find a model about traffic prediction. Also, they mention that they took the data from Minnesota Department of Transportation. So, my guess is that if you want the data from a specific city, you have to contact with the local department.\n", "licensing - Can you use data if no data license is explicitly stated?": "\nThe content of a database is generally covered under copyright law, so broadly speaking\u2026 no. You cannot assume that copying and re-use is implicitly allowed by default.\nAlmost all major countries follow the Berne Convention. In the US (for example), almost everything published after April 1989 is considered \"copyrighted\" by default and protected whether it has a copyright notice or not. You should assume that any works that are not explicitly licensed for reuse may not be copied unless you know otherwise. \n\"Knowing otherwise\" is where it gets tricky. There is a lot of data that simply is not copyrightable. But you have to understand copyright law and the laws governing reuse if you are going to act without explicit license. There are Fair Use provisions which allow a certain amount of re-use of original works. Copyright law doesn't generally protect mere listings of things (like ingredients, formulas, telephone listings, etc)\u2026 but copyright protection may extend to substantial literary expression within those listings. Some countries recognize separate property rights for databases which are somewhat distinct from copyright. Also, only original works of authorship are protected by copyright. Compilations of others' work may not.\nIf you don't see an explicit license, you should assume copyright by default \u2014 and then proceed cautiously to determine whether the work itself is actually copyrightable or if your application is covered under Fair Use.\n", "government - Where can I find normalized data on governmental spending on science?": "\nTry the OECD's Main Science and Technology Indicators\n", "programming - What is a good editor for linked data?": "\nMy currently favoured text editor jEdit has a simple yet effective word completion feature (Menu Edit > Complete word; default shortcut Ctrl+B). It takes its word list from the opened document and includes keywords of the file's programming language (in case it is code). Word-delimiting characters can be user-defined as described on the page Working with Words (section What's a Word?) in the comprehensive jEdit User's Guide.\n", "biology - Is there a list of linked data from resources mentioned in the annual NAR database issue?": "\nYes there are several resources.\nwww.bio2rdf.org is probably the best known.\nTo easily browse linked data try\nwww.distilbio.com\n", "From an entrepreneur's perspective, are there reasons to open data?": "\nExample: I only know of the company Cloudmade because they provide (a now outdated, but fine at the time) download portal of ready-to-use shapefiles derived from OSM data. Though this service might have helped its competitors, they probably earned much more in terms of visibility, which might spawn e.g. development contracts for custom-made solutions. \nThis argument is more verbosely given for open source software in the excellent blog post Yes, You Can Make Money with Open Source. Just replace OSS with open data, and the argument stays valid.\n", "geospatial - Is there a tool to convert geoRSS into geoJSON?": "\nThe most versatile tool for geo format conversion is ogr2ogr in gdal. Here's an online front end that uses ogr2ogr to convert to and from GeoJSON, and it supports GeoRSS.\n", "data request - Where to get school district boundaries?": "\n1) google to find the exact website of the census.gov that contains the information you're looking for.  google's search results still contains all of the shuttered web pages\n2) paste that url into the \"wayback machine\"\n3) choose the most recent blue circle that's pre-shutdown\nExample: Here is the US Census 2017 School District TIGERLine/Shapefile Web Interface saved via Wayback Machine.  \nNote: the Census' FTP server will not save in the Wayback Machine; In very recent past, the team behind the Wayback Machine had started bypassing US .gov server restrictions in order to preserve sites/data, but for reasons unknown, Wayback is still blocked by Census FTP.\n", "data request - Creative commons licensed audio files of basic French vocabulary": "\nMany of the pronunciation files on Wiktionary are from the Shtooka Project, that offer colllections of audio files for basic vocabulary in many languages, licensed under the CC-BY license.\n", "data request - Large bibliographic database of research papers": "\nCiteSeerX and DBPL are two of the most commonly used data sets used in the academic literature.\n", "usa - Is Data.gov down due to the government shutdown or is it down for good?": "\nData.gov is down because of the US government shutdown. You can see almost identcal language on the census.gov website, which explains that the shutdown is causing the site to go offline.\nHere is an article explaining more details about what is and is not available during the shutdown.\nAnd if you are interested in seeing what APIs are available for when the website comes back online, try the Internet Archive for a list.\n", "usa - How to convert risk adjustment scores into dollar amounts from the Medical Expenditure Panel Survey?": "\nThis is a complex calculation. Several factors are considered before the risk score is created, including age, original reason for entitlement, Medicaid status, and primary payer information.  The data behind these include the Minimum Data Set Long Term Institutional File and the Common Medicare Environment from the Beneficiary Demographic Input File. The Medical Expenditure Panel has a query interface.\n\"Diagnostic Data is used in risk score calculations and is obtained from both plans and FFS providers.\n\nThe Risk Adjustment Processing System (RAPS) Database contains the diagnostic data submitted by Medicare Advantage plans, PACE organizations, and cost plans.\nThe National Medicare Utilization Database (NMUD) contains the diagnostic data submitted by fee-for-service providers.\n\nThe Risk Adjustment System (RAS) calculates risk scores for all Medicare beneficiaries.\" (Medicare Managed Care Manual)\n", "geospatial - Is there anywhere to get US gps pipeline route data?": "\nYou can find geospatial data about this on Data.gov (this links to gas pipelines, but you can look for other types of pipelines). To find the geographic region you are looking for, you can draw a boundary box over the map in the upper left.\n(Disclaimer: I am the Evangelist for Data.gov)\n", "medical - Data for \"Driving is Why You're Fat\" graph": "\nLet's try to follow the sources one by one:\n\nTrust for America's Health has obesity ranks (sense probably inverted compared to the graphic) and the proportion (%) of obese people per state, directly linked from their homepage.\nU.S. Census American Community Survey should have this data. They have per-state transport mode usage statistics.\nStreetsblog is both a blog about (public) transportation and a network of grass root organisations promoting alternate (as in not passenger car) transport modes. I could not find any prominent study, statistic, survey or data source on their pages.\n\nSorry for the poor return, but I tried!\n", "best practice - A Python guide for open data file formats": "\nHere is a guide for each file format from the Open data handbook and a suggestion with a Python library to use.\nJSON is a simple file format that is very easy for any programming language to read. Its simplicity means that it is generally easier for computers to process than others, such as XML. Working with JSON in Python is almost the same such as working with a Python dictionary. You will need the JSON library, but it is preinstalled to every Python 2.6 and after.\nimport json\njson_data = open(\"file root\")\ndata = json.load(json_data)\n\nThen data[\"key\"] prints the data for the JSON.\nXML is a widely used format for data exchange, because it gives good opportunities to keep the structure in the data and the way files are built on and allows developers to write parts of the documentation in with the data without interfering with the reading of them. This is pretty easy in Python as well. You will need the MiniDom library. It is also preinstalled.\nfrom xml.dom import minidom\nxmldoc = minidom.parse(\"file root\")\nitemlist = xmldoc.getElementsByTagName(\"name\")\n\nThis prints the data for the \"name\" tag.\nRDF is a W3C-recommended format and makes it possible to represent data in a form that makes it easier to combine data from multiple sources. RDF data can be stored in XML and JSON, among other serializations. RDF encourages the use of URLs as identifiers, which provides a convenient way to directly interconnect existing open data initiatives on the Web. RDF is still not widespread, but it has been a trend among Open Government initiatives, including the British and Spanish Government Linked Open Data projects. The inventor of the Web, Tim Berners-Lee, has recently proposed a five-star scheme that includes linked RDF data as a goal to be sought for open data initiatives I use rdflib for this file format. Here is an example.\nfrom rdflib.graph import Graph\ng = Graph()\ng.parse(\"<file root>\", format=\"<format>\")\nfor stmt in g:\n   print(stmt)\n\nIn RDF you can run queries too and return only the data you want. But this isn't easy as parsing it. You can find a tutorial here.\nSpreadsheets. Many authorities have information left spreadsheet documents, for example Microsoft Excel. This data can often be used immediately with the correct descriptions of what the different columns mean. However, in some cases there can be macros and formulas in spreadsheets, which may be somewhat more cumbersome to handle. It is therefore advisable to document such calculations next to the spreadsheet, since it is generally more accessible for users to read. I prefer to use a tool like xls2csv and then use the output file as a CSV file. But if you want for any reason to work with an .xls file, www.python-excel.org is the best source I had. The most populars are xlrd and xlwt. There is also another library, openpyxl, where you can work with .xlsx files.\nComma Separated Files (CSV) files can be a very useful format, because it is compact and thus suitable to transfer large sets of data with the same structure. However, the format is so spartan that data are often useless without documentation since it can be almost impossible to guess the significance of the different columns. It is therefore particularly important for the comma-separated formats that documentation of the individual fields are accurate. Furthermore, it is essential that the structure of the file is respected, as a single omission of a field may disturb the reading of all remaining data in the file without any real opportunity to rectify it, because it cannot be determined how the remaining data should be interpreted. You can use the CSV Python library. Here is an example:\nimport csv\nwith open('eggs.csv', 'rb') as csvfile:\n    file = csv.reader(<file root>, delimiter=' ', quotechar='|')\n    for row in file:\n        print ', '.join(row)</pre>\n\nPlain Text (txt) are very easy for computers to read. They generally exclude structural metadata from inside the document however, meaning that developers will need to create a parser that can interpret each document as it appears. Some problems can be caused by switching plain text files between operating systems. MS Windows, Mac OS X and other Unix variants have their own way of telling the computer that they have reached the end of the line. You can load the txt file, but how you will use it after that depends on the data format.\ntext_file = open(\"<file root>\", \"r\")\nlines = text_file.read()</pre>\n\nThis example will return the whole txt.\nPDF Here is the biggest problem in open data file formats. Many datasets have their data in PDF, and unfortunately it isn't easy to read and then edit them. PDF is really presentation oriented and not content oriented. But you can use PDFMiner to work with it. I won't include any example here since it isn't a trivial one, but you can find anything you want in their documentation.\nHTML. Nowadays much data is available in HTML format on various sites. This may well be sufficient if the data is very stable and limited in scope. In some cases, it could be preferable to have data in a form easier to download and manipulate, but as it is cheap and easy to refer to a page on a website, it might be a good starting point in the display of data. Typically, it would be most appropriate to use tables in HTML documents to hold data, and then it is important that the various data fields are displayed and are given IDs which make it easy to find and manipulate data. Yahoo has developed a tool, YQL that can extract structured information from a website, and such tools can do much more with the data if it is carefully tagged. I have used a Python library many times called Beautiful Soup for my projects.\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(html_file)\nsoup.title\nsoup.title.name\nsoup.title.string\nsoup.title.parent.name\nsoup.p\nsoup.p['class']\nsoup.a\nsoup.find_all('a')\nsoup.find(id=\"link3\")\n\nThose are only a few of what you can do with this library. By calling the tag, it will return the content. You can find more in their documentation.\nScanned image. Yes. It is true. Probably the least suitable form for most data, but both TIFF and JPEG-2000 can at least mark them with documentation of what is in the picture - right up to mark up an image of a document with full text content of the document. If images are clean, containing only text and without any noise, you can use a library called pytesser. You will need the Python Imaging Library (PIL) library to use it. Here is an example:\nfrom pytesser import *\nimage = Image.open('fnord.tif')  # Open image object using PIL\nprint image_to_string(image)</pre>\n\nProprietary formats. Last but not least, some dedicated systems, etc. have their own data formats that they can save or export data in. It can sometimes be enough to expose data in such a format - especially if it is expected that further use would be in a similar system as they came from. Where further information on these proprietary formats can be found should always be indicated, for example by providing a link to the supplier\u2019s website. Generally it is recommended to display data in non-proprietary formats where feasible.. I suggest to google if there is any library specific for this dataset.\nTab Separated Values (TSV). A tab-separated values file is a simple text format for storing data in a tabular structure (for example, database or spreadsheet data). Each record in the table is one line of the text file. Each field value of a record is separated from the next by a tab stop character \u2013 it is a form of the more general delimiter-separated values format. Unfortunately, I haven't found any good working Python library only for TSV. Until now, I have worked with CSV library like the following example:\nimport csv\nwith open(\"tab-separated-values\") as tsv:\n    for line in csv.reader(tsv, dialect=\"excel-tab\"): #You can also use delimiter=\"\\t\"\n\nShapefiles are files used to represent spatial data such as polygons that define a city, a neighborhood etc. You can use the libraries  fiona and shapely (pip install fiona shapely) to help with this job. For example, if you want to load a shapefile, simplify its polygons (to reduce size) and then export to GeoJSON (so you can plot in your Web browser using JavaScript libraries such as LeafLet), you can use this code:\nimport json\n\nimport fiona\nimport shapely.geometry\n\n\nshapefile = fiona.open('my_shapefile.shp')\nshapes = shapely.geometry.shape(shapefile['geometry'])\nsimplified_shapes = shapes.simplify(0.01) # 0.01 is the simplification factor\ngeodict = {'type': 'FeatureCollection',\n           'features': shapely.geometry.mapping(simplified_shapes)}\nwith open('my_geojson.json') as fobj:\n    fobj.write(json.dumps(geodict))\n\nVOTables is a mix of HTML and XML used most of the time in astronomy. This kind of data contains metadata that it is vital for you. It is pretty simple to extract them with Python using the following library.\nfrom astropy.io.votable import parse\nvotable = parse(\"votable.xml\")\n\nAdditional Information. Maybe you will find the Pandas library useful, whose I/O capabilities integrate and unify access from/to most of the formats: CSV, Excel, HDF, SQL, JSON, HTML, and Pickle.\n", "language - Data for word hierarchies": "\n\nConceptNet is a semantic network containing lots of things computers should know about the world, especially when understanding text written by people. \n\nTrying to reproduce your relation-sequences yields:\n\nfinger PartOf hand IsA body part, which looks surprisingly \"dead end\".\nchair IsA seat RelatedTo furniture MadeOf wood ...\n\n", "data request - Where can I get Tectonic Plate Shape Files": "\nI solved this problem using QGis with the following steps:\n\nVector -> Geometry Tools -> Check Geometry Validity\nVector -> Geometry Tools -> Simplify Geometry\nSaved as a new Shape File\n\nIt loaded just fine after this\n", "data request - Dictionary of misspelled words": "\nThere is actually a Wikipedia page about this very topic, and they maintain a machine-readable list that you can use as a starting point if you're writing a tool.\n", "data request - Birthday and Marriage Information": "\nSomething like this? \nYou can find the spreadsheet on the bottom of the page.\n", "transportation - Where can I find data on aircraft?": "\nNASA does quite a bit of aircraft testing.  (It's the National Aeronautics and Space Administration, after all).\nI'm not familiar with repositories of that sort of data, but you can find technical papers, including those on Blended Wing Bodies in the NASA Technical Report Server, and that would give you contacts to request the data if it wasn't specifically deposited separately.\n(disclaimer: I work at a NASA center)\n", "Data standards for election results": "\nThis isn't widely used, because we just started working on it this year, but our Knight Foundation project, OpenElections, is developing specs for election results in the United States. You can see the latest specification on our Github wiki. We also have a format for election metadata. Project details at openelections.net.\n", "usa - What does 'subregion' mean in Illinois State Report Card data?": "\nI think these subregions are CPS 'networks': \nFrom http://www.cps.edu/fy13budget/pages/Schoolsandnetworks.aspx\n\nNetworks/Collaboratives District-run schools in CPS are organized into\n  five geographic collaboratives \u2013 North/Northwest, West, Southwest,\n  South, and Far South \u2013 and then further divided into 19 Networks,\n  which provide administrative support, strategic direction and\n  leadership development to the schools within each Network. There are\n  13 elementary Networks, four high school Networks, one K-12 Network\n  that serves both elementary and high schools, and one alternative\n  Network for alternative schools.\n\nNetwork                                   No. of Schools \nAustin-North Lawndale Elementary Network  32\nBurnham Park Elementary Network           36\nEnglewood-Gresham Elementary Network      33\nFullerton Elementary Network              41\nFulton Elementary Network                 30\nGarfield-Humboldt Elementary Network      28\nMidway Elementary Network                 36\nO'Hare Elementary Network                 42\nPershing Elementary Network               31\nPilsen-Little Village Elementary Network  26\nRavenswood-Ridge Elementary Network       40\nRock Island Elementary Network            29\nSkyway Elementary Network                 40\nFar South Side K-12 Network               37\nNorth-Northwest Side High School Network  25\nSouth Side High School Network            19\nSouthwest Side High School Network        19\nWest Side High School Network             27\nAlternative Schools                        6\n\n", "usa - What to do when license statements conflict?": "\nThe Massachusetts Open Data Initiative seems to provide only a link to the data, not the data themselves. Their terms of use couldn't be really applied here.\nThe terms and conditions of the Massachusetts Archives website refer to the documents, not to their data collections.\nAs to historical vital records, this page states that \"[t]he Archives' collections are public records and are open to all for research.\" Besides, it states\n\nPublication \nThe Archives requires patrons who copy materials for\n  publication to complete a permission to publish form agreeing to use a\n  standard citation for archival materials and to give a copy of the\n  published work to the Archives. There are no charges for publication.\nCopyright \nRecords created by Massachusetts government are not\n  copyrighted and are available for public use. Copyright for materials\n  submitted to state agencies may be held by the person or organization\n  that created the document. Patrons are responsible for clearing\n  copyright on such materials.\n\nThere is also A Guide to Massachusetts Public Records Law on their website.\nI'd suggest contacting archives staff to clarify the issues with a permission to publish form and a proper citation.\nHope it helps.\n(edited in response to Rob's clarification)  \n\nSorry, I focused on this precise example and missed your more general question. Let's make clear some terms, shall we? For any data distribution process, there are the following participants:\n\ndata collector (a person or persons who actually collected the data); \nauthor/creator/owner/principal investigator - it might be either the collector herself or an organization, e.g. government agency, university, research institute, which the collector has collected the data for;\ndata publisher (keeps the data and provides them for someone's use);\ndata distributor (it might be the same organization as publisher, but also might be another organization, say, with broader opportunities to disseminate information)\n\nNow, the copyright always remains with the author/owner. And whatever other participants do, the data belong to the principal investigator.\nData provider/publisher shall comply with the owner's terms of use, especially regarding confidentiality.\nData distributor shall comply with the data publisher's terms of use, hence, automatically, with the owner's.\nWhen someone looks for data, one should apply the terms of use attached to the data, whether they are written by the owner, publisher, or distributor. If the terms of use are missing or conflicting, one should apply the terms of use of the participant, which is one step higher on that list - publisher's instead of distributor's, or owner's instead of publisher's.\nIn this case, the data owner should be, in my understanding, the Massachusetts Registry of Vital Records and Statistics, the data publisher should be the Massachusetts Archives, and the Massachusetts Open Data Initiative goes as a distributor.\nCan you download the data from the MODI? No. There is only a link, so forget about their terms of use. Can you get the data from the archives? Yes. OK, then you should comply with their terms of use. Specifically: not with the terms of use of their website but with the terms of use of their data collections under the Massachusetts law.\nAccording to my experience, a possible citation for these data might look something like this: Massachusetts Registry of Vital Records and Statistics, 2013, Vital Statistic Records (1841-1910). Boston, MA: Massachusetts Archives [distributor], though I'd hesitate to use it without contacting the archives.  \nWhat is the purpose of an Open Data directory if the collections it links to are not open data? Excellent question. Well, public records are open data, are they not? The fact that you should do some paperwork for the data publisher doesn't make these data closed or private. But besides this, I totally agree with you: the ultimate goal of any open data distributor should be to provide open data, not to complicate the whole process.  \nAs discussed above, 2 out of 3 terms of use are not applicable here (one refers to a link, another to a website, but only the third one mentions the actual data).\nHave to say, I really sympathize with you about what you are going through. (The archives didn't answer even to phone calls? What's that again, the government shutdown?)\n\n", "data request - I'm looking for military discharge rates, the more focused the better": "\nI'm not an expert in this area, but I was told about this site:\nhttp://www.va.gov/VETDATA/Veteran_Population.asp\nWhich has some data on veteran populations in the U.S. I'm not sure if it has discharge rates, but it might be a good place to start. You could try emailing some of the contact folks referenced in the datasets on that VA site, as they might have a better idea.\n", "Do you know any open/standard resume format?": "\nResumeRDF seems to be the most \"ontological\" approach to normalization of CV information. The W3C has another article about ResumeRDF with further links.\n", "What are the Units of Measure for the ZAREALAND & MAREALAND fields in US Census Data?": "\nSince the file is a relationship file between ZCTA and MSA, it represents the area in square meters that the whole ZCTA (ZAREALAND) covers, and the area that the whole MSA (MAREALAND) covers. If you notice a difference between ZAREA and ZAREALAND, or MAREA and MAREALAND, this is because there is a waterbody in that area and the *AREALAND counts only land area instead of all.\nThe Census Bureau uses scientific measurement units, hence meters.\n", "tool request - Suspected dirty data in Wikipedia articles indirectly belonging to a category": "\nAs far as I can see, there is no link from the category Japanese people to the article John Andru (at least based on dump from 1 October 2013), so I'm not sure why did the tool tell you otherwise.\nBut the category structure is pretty messed up. For example, John Andru is in the category 18th-century German writers, through chain of categories like this:\nCategory:18th-century German writers \u2192 Category:Immanuel Kant \u2192 Category:Kantianism \u2192 Category:A priori \u2192 Category:Analysis \u2192 \u2026 \u2192 Category:Statistics \u2192 \u2026 \u2192 Category:Anatomy \u2192 \u2026 \u2192 Category:Memory \u2192 \u2026 \u2192 Category:Design \u2192 \u2026 \u2192 Category:Justice \u2192 \u2026 \u2192 Category:Anti-corruption measures \u2192 \u2026 \u2192 Category:Students \u2192 \u2026 \u2192 Category:New Left \u2192 \u2026 Category:Free will \u2192 \u2026 \u2192 Category:Technology \u2192 \u2026 \u2192 Category:History \u2192 \u2026 \u2192 Category:Millennia \u2192 Category:2nd millennium \u2192 \u2026 \u2192 Category:1932 births \u2192 John Andru\n", "api - Social Networks other than facebook and twitter that allow access to their data to developers?": "\nInstagram\nFoursquare\nLivejournal\nFlikr\nPinterest\nLinkedIn\nBlogger\nWordpress\nHope it'd be enough to begin with.\n", "Data licensing question": "\nThe answer depends heavily on the context of you work and on potential users of your data.\nFirst, there is no \"must\" about sharing your dataset. If the users want to get out more data than your website/app would allow them to do, they could always go to the original sources as they've been doing previously.  \nSecondly, if you decide to share your data (be it the whole database or just a part of it referring to a specific \"car\" with its \"parts\"), you must do it under the same licenses as the original sources do (or you might try to find one license, which satisfies them all, but it's not the easiest thing in the world). And as far as I can tell without more details, there might be already a potential problem with combining GPLv2 and LGPLv3.  \nThirdly, it really depends on your area whether you should or should not distribute openly your work. In academia, for instance, it's highly recommended not only because we all stand for open data and global knowledge but because it will bring you citations, mentions, and scientific prestige. No fear that the others will use your work because you want them to use your work as widely as possible and to give you credit for it.\nIn private sector, however, it might be different, so it's up to you what to decide. \nFinally, I should note that any data might be \"scraped\" from a website or an app. If somebody is really interested in getting a lot of data, what would stop them from doing it? You can try to build a protection against data scraping, of course, but at the end of the day, it might be more costly than sharing the data. Thus, you might want to add a way to contact you directly for big amount of data (say, \"Here is my email, please send me an inquiry, describing your interest, your company/institution, and the intended purpose of data use\").\nHope it helps.\n", "data.gov - Data Standards for Campaign Finance": "\nPolitical Disclosure Standard Electronic Reporting Format (PDSERF) is one such example. It may very well constitute the first such format.\n\nQuestion and Answer on DBA.StackExchange\nGithub project with parser\n\nNote: I'm the author of the question, answer, project parser.\n", "research - PhD for open data in Europe": "\nThe Open University and the University of Southampton definitely have their hands in open data. For more concrete ideas you'd have to look at the appropriate department (e.g. Computer Science) and find a supervisor.\n", "tool request - How can I work with a 4GB csv file?": "\nThis is the kind of thing that the csvkit was built for:\ncsvgrep -c \"Healthcare Provider Taxonomy Code_1\" -r '^282N' npidata_20050523-20131110.csv > hospitals.csv\n\n\ncsvkit is a suite of utilities for converting to and working with CSV, the king of tabular file formats.\n\nA little more efficiently, you could do: \nzcat NPPES_Data_Dissemination_Nov_2013.zip | grep 282N | csvgrep -c 48 -r '^282N' > hospitals.csv\n\n", "Do customers ever access data from a company or is everything they access just information?": "\nSounds like the question distinguishes between \"information\" which the customer receives, and \"data\" which is how the company itself internally organizes and manages the \"information\" product it delivers. Might be better termed \"digital products\" and \"product management information/data.\"\nAn example would be Netflix, which delivers information in the form of streaming media to customers but customers never see the data that Netflix uses internally to manage and deliver the steaming media. \nRegardless of the type of digital product delivered, even if it something like raw demographics data, the company will always have data for internal use that the customers never see. From the IT perspective, there would be compartmentalized systems and likely functional units, one which concentrates on product delivery and one that concentrates on internal operations. \n", "data request - Text message corpus for American english": "\nI don't know a corpus, but I know a way to create one. If you know how to program you can use the Facebook API and download all the public facebook status from USA with their comments. Then you can use them as a corpus.\nInfo about Facebook API\n", "metadata - Is it a good idea to think of defining a DCAT vocabulary in other languages?": "\nI would not recommend translating standard schema terms to other languages. This would make datasets not portable across toolsets that incorporate the schema. Instead, I would suggest mapping the terms to other languages in the visualization (e.g., viewer) of the data.\nFor example, an spreadsheet view in English might show \"title\" while in Spanish how \"titulo\". But the field naming in the raw data would not change.\n", "best practice - What type of software is used by the Open Data community?": "\nYou cannot find \"THE\" best answer for this question. It always depends of what you want to do with the data.\nStatistic Analysis\nIf you want to make a statistic analysis, you can use python like the question you mentioned. Otherwise, weka, spreadsheet (like microsoft excel), R or Rapidminer are a few tools I have worked with them.\nGeodata\nIf you have geodata and the only thing you want to do is to visualize them in a map, you can use cartoDB.\nVisualization\nA pretty cool tool is the open spending. You can create awesome graphs most of the time for financial data.\nClean data\nMany times, you will find data that needs cleaning such as removing unwanted parts or fix mispelling labels etc. For this kind of work, Google refine or Open Refine as it is called now is perfect.\nI cannot remember something else now. If I refresh my memory with anything else, I will update the post.\n", "historical - Data about when cities and neighborhoods were established": "\n'Established' can mean different things (first settlers, formally founded & named, formally chartered or incorporated, etc.)  My town was incorporated in 1870, but founded in 1706, and settled sometime near 1695.  \n'Neighborhoods' are even trickier, as they're not as formal.  You still have issues of multiple dates (when did New York's 'SoHo' start being called that, vs. when was it settled?) but the date and boundaries are much more ambiguous.\n Camp Springs, Maryland has been fighting with the County to be recognized, as they're spread across multiple county council districts.  Subdivision are easier to deal with, as there'd have been some sort of planning process and establishment of homes (although there may be older homes in the subdivison).\nFor municipalities, I'm not aware of any one source, but I'd look to either state archives, or state municipal leagues.\n", "data request - Public source for financial company customer counts?": "\nI am not sure you will find the number of customer per company but you can asses their size using other more accessible metrics like their revenue or number of employee.\n", "data portal - Difference(s) Between Datahub.io and CKAN": "\nAs you say, http://datahub.io is a particular instance powered by CKAN, so you can't really compare both. In terms of what is particular about datahub.io, it is a community centred and powered site where everybody should be able to host their data, rather than the more default scenario of one or more organizations publishing data on the portal.\nIt is in the middle of a migration process to a newer CKAN version, so I suggest checking the blog or contacting the maintainers if you want to know more about its status.\n", "What's a good resource for learning OpenRefine?": "\nThe documentation page on OpenRefine website point out to the official wiki a list of tutorials and the discussion list for your project specific questions. \nLike @Joe pointed out, I am not sure what are your looking for exactly. Maybe you can explain what you are tying to learn and why the book or existing documentation doesn't address your challenge. \n(disclaimer I am part of the OpenRefine team)\n", "Catalog.data.gov using CKAN API with python requests package": "\nThe POST syntax is fine - you get the same results using the web front end:\nhttp://catalog.data.gov/dataset?q=sea_water_temperature&sort=score+desc%2C+name+asc&ext_location=&ext_bbox=&ext_prev_extent=-254.53125%2C-75.84516854027044%2C54.84375%2C83.19489563661588\nHowever the underscores are being treated as spaces, so effectively your query becomes: \"sea\" OR \"water\" OR \"temperature\". This is simply SOLR default syntax, and that has its pluses and minus. I think could successfully argue that the OR thing is not what you might expect, leading to all those results, but I'm not sure that the underscores as spaces is useful to users. (Can someone explain this aspect of SOLR?)\nTo get what you're after, try adding quote marks:\nhttp://catalog.data.gov/dataset?q=%22sea_water_temperature%22&sort=score+desc%2C+name+asc&ext_location=&ext_bbox=&ext_prev_extent=-254.53125%2C-79.43237075914709%2C54.84375%2C80.87282721505686\nThis gives 52 results, starting with ones with exactly this tag, which is what I think you're after.\nIf you're really just interested in this tag in particular, you can use just search this field using 'tags:' instead of 'q:' (web URL) or inside of the q value in the API.\nYou can save faff of using requests and JSON decoding by using ckanclient. Once you get the hang of it, it is dead easy:\n>>> import ckanclient\n>>> from pprint import pprint\n>>> ckan = ckanclient.CkanClient('http://catalog.data.gov/api')\n>>> pprint(ckan.action('package_search', q='tags:sea_water_temperature', rows=1))\n{u'count': 51,\n u'facets': {},\n u'results': [{u'author': None,\n               u'author_email': None,\n               u'extras': [{u'key': u'bbox-east-long',\n                            u'value': u'-94.867'},\n                           {u'key': u'resource-type', u'value': u'dataset'},\n                           {u'key': u'bbox-north-lat', u'value': u'8.085'},\n...\n\n", "Geospatial Open Data from Sri Lanka or India": "\nDid you try to look for it at GeoCommons.com?\n\nIndia states\nIndia state boundaries\nIndia states\n\n", "Where can I find a repository of open data stories?": "\nAlright, some links!\n\nThis was started years ago, but didn't seem to take off: http://opendatastories.org/\nBefore the Beyond Transparency book, CfA did Engagement Stories: http://commons.codeforamerica.org/engagement-commons (The Commons hasn't been getting a ton of love recently though)\nOKF has a (more recent) living doc exploring how open data improved data quality: http://bit.ly/opendata-betterdata\n\n", "data request - Estimate how many linux operated computers are currently online": "\nThere is the good old Linux counter project1 which estimates around 69 million linux users. Now this is not exactly the same as number of online linux machines, but it should at least give some reference for what the number might be.\n1 Actually this is a newer incarnation of the original counter.li.org.\n", "usa - Is it still possible to download Census 2000 data at the level of the Metropolitan Statistical Area?": "\nIt looks like the geographic categorization is different for MSA between 2000 and 2010. If you use this link, and select \"BACK TO ADVANCED SEARCH\", it will bring you back to the search bar with the geographic level I specified. Since this level only seems to apply to 2000, you'll be getting table listings for the 2000 Census.\nIn the future, if you open the Topics tab --> expand the Year drop-down list, select 2000 and then go into the geography tab, it will only list available geography levels in 2000, and MSA appears on the list.\n\n", "Geospatial, temporal and keyword search using CKAN API on data.gov using ckanclient": "\nckan.action will pass all the keywords parameters onto whatever API call you are using, so you can pass any parameter supported by package_search to it (I'll use PDF instead of WMS to get some results):\nimport ckanclient\nq = 'tags:sea_water_temperature'\nckan = ckanclient.CkanClient('http://catalog.data.gov/api/3')\nd = ckan.action('package_search', q=q, rows=10, fq='res_format:PDF')\n\nYou can change slightly your code to pass a whole dict of parameters, which makes easier to see what is going on:\nimport ckanclient                                                                                                       \nsearch_params = {                                                                                      \n    'q': 'tags:\"sea_water_temperature\"',       \n    'fq': 'res_format:PDF',                         \n    'rows': 10,                                                                                        \n}                                                                                                      \n\nckan = ckanclient.CkanClient('http://catalog.data.gov/api/3')                                          \nd = ckan.action('package_search', **search_params) \n\nUse these parameters for the other query types that you needed:\n# sea_water_temperature + PDF + modified since + bounding box\nsearch_params = {                                                                                      \n    'q': 'tags:\"sea_water_temperature\" AND metadata_modified:[2012-06-01T00:00:00.000Z TO NOW]',       \n    'fq': 'res_format:PDF',                                                                            \n    'extras': {\"ext_bbox\":\"-121,45,-120,46\"},                                                          \n    'rows': 10                                                                                        \n}\n\nHope this helps!\n", "usa - What are good examples of how open data is driving community development?": "\nI'm not sure about specific examples in the USA, but the OpenStreetMap had some outstanding success in various countries. There are bound to be disaster stories in the US as well.\nA famous example is the Haiti earthquake. It also has nice visuals.\nLess known is perhaps the community mapping in Dar es Salaam, Tanzania:\nhttp://www.opendta.org/Pages/Initiatives/initiative-mapping-dar-es-salaam.aspx\nhttp://blogs.worldbank.org/ic4d/node/537\nAn open data platform that is cited numerous times for its success is Ushahidi.\n", "data request - Legacy/obsolete UK postcodes": "\nThis site UK Data Service has historical postcodes from 1988. But I don't know if it is open data since I have to login. I hope this will help you :)\n", "extracting - Where can one find developers interested in a PDF data extraction hackathon?": "\nPeople from ScraperWiki and OpenKnowledge Foundation sure will like it! They develop and maintain a software called pdftables which extracts tabular data from PDFs.\nThere is also an article on ScraperWiki's blog about research in identifying tabular data in PDFs (since PDFs do not have information about data semantic, only positions, font etc.).\nTo contact people directly you see members of OKFN's GitHub organization, ScraperWiki's GitHub organization and also pdftables contributors.\n", "security - Aggregated data for port number and vulnerability": "\nOpenVAS have a really good repository for \"Network Vulnerability Tests\" that contain all the information needed to carry a research like this.\nDownloading the archive from the nvts repository you can parse the file in the database,and make your own statistic of port sorted according vulnerability.\nWith a few bash lines of code you can filter all the files that contain at least one \"CVE-XXXX-XXXX\" and one \"port\" directive, make some research inside the filtered files (warning: there can be more than one CVE and more than one port per file) and then sort and filter the results.\n", "network structure - Data mining social group's interests, demographics, and geography": "\nThere are also academic surveys. Check ICPSR for open-access survey results.\n", "data request - Speech audio databases with phonemes labelled": "\nI am not 100% sure if the following links are useful for you. Please let me know if you are looking for something else.\nhttp://www.iitg.ac.in/ece/emstlab/SRdatabase/introduction.php\nhttp://accent.gmu.edu/howto.php\nhttp://www.signalprocessingsociety.org/technical-committees/list/sl-tc/spl-nl/2012-05/the-rss2015-speech-corpus/\n", "usa - After FOIA is fulfilled where can it be found?": "\nShort answer: No. As other people have said, the recipient of the FOIA information is not obligated to share it with others. There are some FOIA allowances for the needs and benefits of providing information to media, primarily with regards to fee waivers, but the recipient can do whatever they like with the information once they have it... including keeping it all to themselves.\nThat said, there are various initiatives to try to make these requests more useful by sharing them with a larger audience once they have been made. On the Federal side the most prominent that I am aware of is FOIA Online, which serves both as a conduit for requests and a place where others can see the responses. Unfortunately the participation seems to be voluntary and not all agencies participate.\nOn the public side you have projects like Muckrock which has submitted requests and responses listed and possibly FOIA Machine, which was kickstartered into existence earlier this year by the Center for Investigative Reporting. I'm unsure of FM will make public responses to other people; I can imagine a case for journalists to not necessarily make other journos aware of what they're working on.\nIn voluntary participation people often put results into Scribd or DocumentCloud and that might be your best avenue here - contact the people who made the request and ask them to put the results into there and share them. Here in the DC area people at the City Paper put supporting documents in one of those and attach them to the bottom of their online story. Your best bet may be to convince those folks that doing so would enhance their reporting.\n", "Source of historic RFPs for the City of Chicago": "\nWith the assumption that an RFP converts to a contract, they have an Awarded Contract page. From there they have scanned documents for older content and a Vendor, Contract and Payment Search system which looks to have the Specification number for linking back to the RFP/RFQ/RFI\nGiven the edited questions request to find RFPs pre 2010, I have not been able to find a decent search capability but there is pre-2010 data in the system as evidenced by Contract #13221. It was awarded in 2007\n", "Data Listing Request": "\nIt sounds like you're just looking for any large structured dataset to play with. If you could be more specific of the type of data you're interested in, we could probably provide some more targeted recommendations.\nThis gist provides a good list of sample open data sets, including some curated collections by data scientists.\nI particularly like Hilary Mason's research-quality dataset list.\nMy advice is to pick a dataset related to a subject you're interested in. Good luck!\n", "tool request - Building maps using ESRI shapefile": "\nMy favorite is cartodb. It is free and you can create wonderful visualizations.\nAlso, you can read this tutorial about using ESRI shapefiles in cartodb.\n", "best practice - Optimal Data Format(s) for Open GIS Data Repositories?": "\nI would say that geojson is in the top of my personal list. JSON is really easy to use it with a programming language, especially with Python that I am familiar with. Also, it is easy for conversions. If someone has another system and he has to use the data in another format, he could convert it to that format from JSON pretty easy.\nAlso, shapefiles are very popular and I feel it would be my second choice.\n", "education - Has any MOOC (Coursera, edX, Udacity or others) publicly released some of their student data?": "\nFrom my knowledge, Coursera uses Backbone.js for their site, so you can find several JSON endpoints with a lot of data. Unfortunately, I am not familiar with Backbone and the only links I know are from Google\nList of all courses - https://www.coursera.org/maestro/api/topic/list?full=1\nAnother list of courses - https://www.coursera.org/maestro/api/topic/list2\nList of all universities - https://www.coursera.org/maestro/api/university/list\nInformation about a specific course - https://www.coursera.org/maestro/api/topic/information?topic-id=compdata\nMaybe if you are familiar with those technologies, you can find other links with more details and maybe with non-personal data you may find interesting.\nUPDATE\nAlso, this user seems to have a lot of data about MOOC (completion rates etc). Maybe you can contact with him and ask him to share them with you.\nhttp://moocmoocher.wordpress.com/2013/02/13/synthesising-mooc-completion-rates/\n", "standards - Is there an open data format for screen/play scripts?": "\nThe Text Encoding Initiative's XML language has a module for \"Performance Texts\" by which they mean plays, screenplays, and other sorts of scripts. If you're looking for very prescriptive rules, TEI will disappoint you (there's usually more than one way to do anything in TEI!), but if you want a hospitable markup language with a lot of existing scaffolding, TEI should work nicely.\nThe best way to get your head around TEI is TEI By Example, which has a module tailored to drama.\nGood luck!\n", "data request - Cellular Network database": "\nThis is a list of mobile phone network and satellite phone network operators measured by number of subscribers which is free in wikipedia with link\nFurthermore, there are a few more paid services in any case:\n1) researchandmarkets.com\n2) deveo\n", "data request - Shape Files (Free downloads) for districts of the world": "\nThere are 3 ways to do it\n1) Download shapefiles from here for cities boundaries. A single file  for each country.\n2) Use Twitter API to get the coordinates and polygons for each city.\n3) Use wikimapia for the same thing such as Twitter API\n", "calendar - Is there an open data api or service for dates of official government holidays?": "\nThe system for (US) federal holidays is rather simple. As described in 5 USC 6103, it's basically just a short list of specific dates and rules for weekends.\nThis could certainly be integrated into your calendar but you might want to find a more complicated data source to practice with an API. To keep on the same theme, how about religious holidays? (example)\n", "data request - German - English dictionary": "\nIf GPL licenced is acceptable to you, then you can use the German <-> English dictionary included in the Ding package:\nhttp://www-user.tu-chemnitz.de/~fri/ding/\n", "data request - Set of handwritten, labeled characters": "\nI created a dataset with of on-line data. It has 369 symbols (including a-z A-Z 0-9 \\alpha-\\omega), but it is online data. You will have to create the rendered versions yourself:\nhttp://write-math.com/data\nEach class has at least 50 recordings. (I don't have much data for letters, so it will probably not be more.)\nedit: I've redered it. See The HASYv2 dataset.\n", "data request - Where can I get private companies' financials?": "\nThere is no comprehensive source of open data as you describe, but there are public and restricted-access open data sets that can illuminate certain aspects of private company finances. Privately-held companies often disclose information to the public in accordance with federal, state, and local laws. Some of this information directly involves company finances (such as public bankruptcy records; public property records) and some of it may serve as an indirect indicator of a company's financial circumstances (e.g., the U.S. Department of Labor publishes disclosure forms with rich information on the retirement plans maintained by U.S. employers, including 401(k) plan assets).  \nCertain sectors of private companies are subject to more comprehensive financial reporting, such as banks (see, e.g., the extensive federal open data on U.S. banks published by the banking regulators). Data.gov's search feature can allow you to explore data available on particular sectors, such as finance.\nDepending on the purposes you have, you may also be able to take advantage of restricted-access data sets on individual firms. For example, the U.S. Census provides researchers with access to nonpublic data at a number of Census Research Data Centers (RDCs). Restricted use data includes economic data for business establishments and firms, including: \n\nEconomic Census\nLongitudinal Business Database (LBD) \nAnnual Survey of Manufactures (ASM)\n\nThe list of Census restricted use data sets is available on the Census Center for Economic Studies site.\n", "data request - Getting legal information": "\nThis is an example of a question that we won't be able to answer until the CourtListener archive is complete. Using the data that is available, one could run queries there for particular statutes by citation or name.\nFor example, if interested in the Convention Against Torture, do a query at CourtListener for that with surrounding quotes, or if interested in cases invoking \"42 U.S.C. \u00a7 1983\" query that.\nBut cases aren't always \"filed under\" a particular statute. When filing a federal lawsuit one is required to fill out a cover sheet and choose a \"Nature of Suit\" (NOS) code that categorizes the case somewhat, but many lawsuits involve more than one issue, so a case alleging both copyright and trademark infringement might only be listed under the copyright NOS code and so even if you could search across all the NOS codes you wouldn't get exactly what you want. A separate problem illustrated by this example is that Copyright law is vast. Cases dealing with 17 U.S.C. \u00a7 107 (fair use) are often totally different from cases dealing with 17 U.S.C. \u00a7 109 (the first sale doctrine). \nSo, in some ways your question is not exactly framed in an answerable form. What could be done (when CourtListener's archive is complete) is we could create a giant list of every opinion that mentions \"17 U.S.C. \u00a7 107\" and every opinion that mentions \"17 U.S.C. \u00a7 108\", \"17 U.S.C. \u00a7 109\", etc. through the entire United States code. That would be really interesting. What is the most litigated part of the code? What statute is cited the most? etc.\nThe best collection of federal district court opinions that you could download/scrape would be the RECAP collection from the good people at CITP, hosted by the Internet Archive here: https://archive.org/details/usfederalcourts\nHowever, that collection includes any kind of document that might appear on a federal docket, including complaints, briefs, declarations, etc. and so it would take some additional effort to sort out the opinions from the rest. One day CourtListener will ingest that collection and make this easier for people trying to tackle questions like yours.\n", "data request - Practice Hadoop?": "\nHere you are:\nA MOOC from coursera, but you have to wait until they will run it again or\nYou can start from dozens of courses here in big data university.\n", "data request - WHO's Defined Daily Dose guidelines": "\nThe ATC index DDD is copyright protected, so I do not believe one could legally make an open-data version.\nYou can find information on their licensing on this FAQ page:\nhttp://www.who.int/classifications/help/FAQOther/en/index.html\nYou can find their copyright statement at:\nhttp://www.who.int/about/copyright/en/\n", "usa - Raw data from US Census and BLS": "\nThe native raw datapoints comprising some of the more popular tables related to your search are accessible through sites at both agencies.  As examples:\n\nSummary reports on current employment statistics and detailed data behind the report from the Bureau of Labor Statistics\nSummary reports on firm size, employees, and payroll and detailed data behind the report from the U.S. Census\n\nA collection of all the datasets by organization can be found at the Data.gov catalog.  For the two you note:\n\nBureau of Labor Statistics\nU.S. Census\n\n(Disclaimer: I am the Evangelist for Data.gov.)\n", "usa - How can I convert from Census 2010 tracts to Census 2000 tracts?": "\nMissouri Census Data Center provides a (SAS-based) app that does this pretty smoothly.\n", "data request - Congressional Record in electronic form": "\nThe Sunlight Foundation makes a lot of Congressional Record data available through its Capitol Words API:\nhttp://capitolwords.org/api/1/\nIt's focused on speeches, and doesn't bring much order to chaos beyond that.\nBut you're also looking for votes, and bills, and bill text? Rather than plumb the depths of the Congressional Record, you can get that from this project:\nhttps://github.com/unitedstates/congress\nWhich has various scrapers that assemble this from around Congress. This is the data that ends up in GovTrack and in the Sunlight Foundation's Congress API.\n", "usa - Panel data for public companies' financials? Alternative to Capital IQ Compustat?": "\nThe SEC publishes standardized machine readable data in XBRL format on public companies and other entities, such as mutual funds. Public company financial statements are among the data sets available. These data sets are available on the interactive data home page of the SEC, www.xbrl.sec.gov. \nIn addition, the SEC maintains a market structure and data analysis site. (See announcement.) This site, www.sec.gov/marketstructure, contains data such as metrics for individual securities. The individual security data sets provide metrics for more than 4,800 securities. Columns include: Ticker, Date, Security, McapRank, TurnRank, VolatilityRank, PriceRank, Cancels, Trades, LitTrades, OddLots, Hidden, TradesForHidden, OrderVol, TradeVol, LitVol, OddLotVol, HiddenVol, TradeVolForHidden.\n", "File formats by scientific community?": "\nThe BioSharing Standards Database may be of some use to you.\nYou're going to have a hard time with this, though, because of the scatteredness of the information and because quite a few instrument-science file formats are proprietary.\n", "data request - What are the different types of Degrees, Diplomas and Certifications and Industry types?": "\nI'm not entirely sure what exactly you are after, but in any case, looking at the ESCO-standard would probably get you in the right direction.\nESCO is a EU-standard published in October, linking the NACE sector classification, the ISCO occupational standard and the EQF qualifications standard.\nFor instance the information about the European Hairdressing Certificate  contains standardized metadata about the awarding bodies, the corresponding qualifications, the related occupations, etc.\nThe complete standard is available as open data, and can be downloaded as RDF.\n", "best practice - Captcha in a website with open data": "\nIt might be consider illegal. Autofilling captcha is a blackhat toolbox and in most of the cases you should avoid it. The best thing you can do is read the \"term of uses\" of the site and if you cannot find anything there, then you can always contact with them and ask for a certain reason to send you the data directly.\n", "web crawling - Alexa top dataset": "\nAlexa is a division of Amazon.  They still have an API, but you need to access it as a paid service through AWS:\nAlexa Web Information Service\nhttp://aws.amazon.com/awis/\nand\nAlexa Top Sites\nhttp://aws.amazon.com/alexatopsites/\n", "data request - Amazon price history dataset": "\nwww.keepa.com is a similar website to camelcamelcamel. I cannot find a way to download the data but maybe you will.\nIn addition, Terapeak provide a free package with limited api calls (500/months) where you can find historical prices.\n", "data request - Indian Movie Database?": "\nHmm I don't know if you could find something for a commercial use. But here are a few suggestions.\n1) imdb if you do not want to play with api, you can download all the database. But not for commercial use.\n2) An alternative api for movie database rotten tomatoes\n3) An alternative api for movie database anditson\nSince you need a commercial use, read very well all the term of uses, just to be 100% sure.\n", "best practice - What are examples of people sharing personal data as open data?": "\nStephen Friend of Sage Biometrics gave a talk at the Symposium on Global Scientific Data Infrastructures  in which he mentioned that people were putting their medical data out there to try to help find cures for their ailments.  (and often paying of of pocket for their DNA to be sequenced so it could be shared)\nSage Biometrics operates Synapse to help distribute  and analyze that and other public biometrics data.\nupdate More info on the effort is at http://sagebase.org/bridge/ :\n\nToward this goal, Sage Bionetworks, with support from the Robert Wood Johnson Foundation\u2019s Pioneeer Portfolio is building BRIDGE a web-based, open-source platform that will allow patients to provide their data and insights as research partners on the health problems that matter most to them.\nOn BRIDGE, citizens, patients and researchers will be able to use online tools that connect people, their data and their stories to build communities focused on defining the research question that matters most to patients and their families. Participants will be able to use BRIDGE\u2019s consent tools and data portal to contribute their health data into open research projects such as those that Sage Bionetworks is enabling with its Synapse data repository and collaborative work space. The insights that come from the data will then be reported back on BRIDGE and also drive new rounds of research collaborations.\n\n", "Journals for Open Data research": "\nIt seems research is interdisciplinary. \n\nFor technical aspects I would look, for example, at journals that publish research on linked data. \nFor business models etc management journals.\nFor public sector information, journals that focus on pubic administration.\n\nMore research might be in Information Systems and others.\nWe at the ODI started a Zotero group for open data papers, but early stages.\n", "data.gov - Trying to locate US Census Shapefiles for all MSA's (Metropolitan & Micropolitan Statistical Areas)": "\nYou have been misled, CBSA files are in fact what you are looking for. Check the documentation detailing what each type of shapefile contains.\nWant more verification? Here is the description of the GEOID field for the CBSA shapefile: 2010 Census metropolitan statistical area/micropolitan statistical area code found in CBSAFP10. And here is the description for the CBSAFP10 field : 2010 Census metropolitan statistical area/micropolitan statistical area code. You may want to use the NAME10 field to reference each specific MSA.\nHere is another tip for you, all of the data released by the Census (as far as I know) comes labeled with a year in which it is measured. If your data is a 5-year average, the most recent year it covers is the shapefile year you should attach it to. If it is one year data, then the shapefile should come from the same year. I am going to assume you want population statistics and that the year of data you have is for 2012 or is for 2008-2012, therefore, I believe you want this.\n", "government - Involvement of central statistics bureau with national open data portal": "\nI have a CSV dataset you should find useful. It contains primary government website URLs for all countries in the world. The third field is to the country's statistical division.\nhttp://www.opengeocode.org/download.php#govweb\n", "data request - A dataset for Google Books": "\nGoogle Books publishes an API. https://developers.google.com/books/docs/v1/getting_started\nOf course one of the prohibitions in their ToS (https://developers.google.com/terms/) is \"you will not...Scrape, build databases or otherwise create permanent copies of such content\" so if anyone built an open database from this data, which I think is what you're asking for, it may be in violation of the ToS.\n", "government - Crowdsourced Open Data": "\nI aggregate datasets from different sources together frequently. I sometimes use a hybrid database/table representation, that follows some adaption of this basic approach:\n\nSplit the fields into two table schemas.\nA. one for things that are invariants (e.g., place name, ISO codes, etc)\nB. one for things that are variants (e.g., population, coordinates, etc).\nC. the 2nd table schema uses a foreign key to tie its records to the first table schema.\nI then make an instance of the 2nd table for each dataset source.\nWhen querying, I join the first table with an instance of the second table depending on which data source I want to use.\n\nI avoid blending data for the same field from different data sources if there is a potential to be different, and I hate the idea of adding N fields for the same value in one table to cover each dataset. This way, if I decide to add a new dataset source, I can simply create another instance of the table schema. Other benefits I have:\n\nDon't have to update existing table schemas.\nDon't have to re-optimize storage of in-production databases.\nDon't have to modify queries in my middle-ware.\n\n", "Is there any documentation or case study of scaling up an Open Data API?": "\nYou can have a look at the main data portal in the UK: data.gov.uk\nAn example of the published statistics is below.\nDataset                                         Views   Downloads\nEnglish Indices of Deprivation 2010             45173   15046\nBona Vacantia Unclaimed Estates and Adverts     38263   24112\nLower Layer Super Output Area (LSOA) boundaries 35547    8785\n\n", "data request - Real Newbie Question": "\nThere's Data.gov for U.S. federal government open data, and Datahub for international open data, as a start.\n", "api - Techniques for Pulling Prices for a Large Number of Amazon Items": "\nAs I can understand from this blog post they split the database in groups of interest and they update products in base of these groups.\nWithout \u0399 know more than this blog post, several companies provide the option to contact with them and arrange a different paid package for their API. I don't know if this is the case for Amazon, but why not?\n", "data request - Music partitions for piano": "\nI found this website (musopen.org) the other day and I bookmarked it. Looks interesting. From the about section of their website:\n\nWe provide recordings, sheet music, and textbooks to the public for free, without copyright restrictions. Put simply, our mission is to set music free.\n\n", "usa - Occupational Outlook Handbook dataset": "\nhttp://www.bls.gov/data/#api\nYou should be able to use the API to get the raw data.\nDouglas\n", "government - A file listing adjacent congressional districts?": "\nI just solved a problem exactly like this with the United States' 2012 Census Block Groups. I used ArcGIS to breakdown the borders to their individual line segments (in other words each line that consists of only two vertices), and captured the line length of each segment of each area. I then transformed the lines into midpoints and did an intersect analysis. This method I found was exponentially faster than near table analysis using the whole polygons. I then selected records that where only Block Group GEOID's from GEOID1 did not match the id in GEOID2.\nThis took ArcGIS too long so I had to do it in SAS. I then dissolved the records by GEOID1 and GEOID2 and summed the line length. This produces the total length of borders that are shared by GEOID1 and GEOID2. I then compared this length with the total length of the original Block Group. This helped me to solve the question: What percent of Block Group 1's border is shared by Block Group 2's border? And it was for all Block Groups. This also tells you what Block Groups are adjacent to each other.\nI realized based on some of the answers provided that I was not including adjacent districts where only one corner from each was touching, so I broke down each district by vertex instead of line midpoint and did a similar analysis. I then joined the first analysis based on line midpoints to the vertex analysis and give a proportion score of zero to any that were not caught in the first analysis (because if only corners touched they do not share a common border, but a corner).\nThis file includes my analysis. The first column is the State and District number of the reference district; the second column represents the districts adjacent to it; the third column represents what proportion of the reference district's borders are shared with the adjacent district. If the proportion is zero, then they are adjacent by touching corners.\nSource of data.\n", "data request - Results of past NCAA games": "\nI created a git repo with the data I have collected from the past few seasons for the NCAA mens basketball division I. \nCheck it out here: \nhttps://github.com/mgoldwasser/ncaa-basketball-historical-final-game-scores\n", "data request - Database of trademarked words": "\nCheck the following two resources (sorry, only US info):\n\nhttp://www.uspto.gov/ip/officechiefecon/tm_casefiles.jsp\nhttps://explore.data.gov/Business-Enterprise/Trademark-Daily-XML-Applications-Assignments-and-T/eqbw-esys\n\nHope this helps.\n", "government - How to acquire data about the speed and density of vehicle on freeways?": "\nI couldn't find one dataset for both of your request.\nHere is the ITOworld project which contains data about speed limits.\nAnd here is a dataset for UK about traffic. Maybe from this dataset you can find the density.\n", "CKAN vs. Socrata": "\nI'm also quite biased because I work on CKAN and DKAN, but four things to add:\n1) Both CKAN and Socrata are often integrated in various ways with a Content Management System such as Wordpress (data.gov) or Drupal (data.gov.uk), in order to more readily provide a more full featured web portal including use cases like telling stories around data, creating groups to collaborate around the data, etc. \n2) If this CMS use case is relevant for you, and especially if your organization already uses Drupal for its websites (and/or has a PHP-savvy than Python-savvy team), do look at \"DKAN\" as well (http://nucivic.com/dkan).  DKAN seeks to mimic the features and API of CKAN natively within Drupal, so there's just a single LAMP stack of software to deal with rather than an integration of two different open source platforms, as in the case of Wordpress+CKAN or Drupal+CKAN.  \n3) I think open-source / lack of \"vendor lock-in\" is a big deal; even if you're buying a turnkey SaaS solution, I would always go with a one that you could always change your mind and take over direct responsibility for if you ever need to.  For now, CKAN and DKAN are stronger in that regard.\n4) Both CKAN and DKAN are available in the cloud on IaaS platforms like AWS and Azure, as well as in SLA supported SaaS versions.  Open source + SaaS = OpenSaaS (http://opensource.com/government/14/1/opensaas-and-government-innovation).\n", "api - Tool to extract the main concepts/topics from web pages": "\nI ended up creating my own system combining a bunch of APIs. Here is what I did:\n\nI pulled the homepage text from each website and cleaned up all the html, javascript and styles\nI pushed this text out to various APIs to process and stored the results. I used alchemyapi.com, textrazor.com, opencalais.com. Those APIs have a lot of options but mainly I focused on keywords, entities and topics.\nI then manually scored a bunch of websites if they were on topic or not\nI exported the data to a CSV file and uploaded it to BigML.com a SAAS machine learning tool. Using their very slick user interface, I was able to easily create a dataset, create a model or ensemble of models, run a 80/20 evaluation of my model and finally predict results.\nI originally used Google Prediction, but it was very complex to setup, had very poor documentation and examples in their API for prediction. and was black box - meaning It worked but who knows why.\nI'm getting around 80% accuracy in my predictions for new URLs to see if they are on topic or not.\nOddly enough I wrote this all in PHP as a WordPress plugin mainly because that is what I know and the end result of this will be a multisite wordpress website. A better choice would have been Python as there are many good text processing tools for that language.\n\n", "computing - User interface data": "\nWe can make some available from Retail systems, where the logs are the checkout operator. These are logs of main messages sent via PreTranslateMessage to the application. The operator tends to use the same screen all day, but there are a number of popup and child windows that show.  Our logs only show the inputs such as keystrokes and clicks; they do not show any output such as error messages. http://www.fieldpine.com/docs/tech/datasets.htm\nI am not sure if these will contain enough information, depending on what your aim is. To fully understand and analyse you might also need to know some details about what button is positioned where. Maybe we can provide that too if needed but this is then getting quite specific. We have instrumentation all through the code and can track from user action, to processing flow, to final outcome. From your sale receipt we can track back to exact keystrokes if we want too.\nAt the moment I have only loaded a small sample so you can verify it has what you need, I will get some larger samples online next week.\n", "Where can I find a complete list of Census Bureau Summary Level (SUMLEV) definitions in table format?": "\nThe page Missouri Census Data Center - Census Geography and Summary Levels has a link to a listing of all the sumlev codes known to them as a text file.  \nAlthough the text file is a SAS source code file it would be trivial to edit out the code before and after the list, and edit the rest to csv.  \nIt's unclear what the license is for their text version of the list.\n", "geospatial - Obtaining longitude/latitude boundaries for Google Maps regions": "\nGoogle Maps data comes with many restrictions so isn't really suitable as a base for further use.\nAlthough it can be of varying quality, Open Street Map (OSM) increasingly includes boundaries as well as ways (roads) and points, and is Open Data (subject to their terms of use including attribution).\nThe OSM data is in XML format (download link at bottom of the left column in the example) that lists the relations, ways and nodes in the boundary that was selected.  In a nice simple case like that Acadia University boundary, utilities like osmtogeojson should be able to convert it into a mapping format like geojson, but it might struggle with a more complex nested boundary.\nBy comparing Google Maps with the OSM data you'll see there are some differences with the campus boundaries (especially north of Hwy 1 / Main St).  It's debatable which is more accurate, the official campus map doesn't show any boundaries but does include (for example) buildings west of Westwood that neither Google or OSM include within the campus.\nFor a more general solution, many countries now have open data boundary files online, such as GeoBase for Canada.  These are more likely just to be government admin areas (provinces, municipalities, ridings and so on), and can be a large amount of data to deal with.\n", "Download Wikipedia articles from a specific category": "\nThe Wikipedia Special Export feature does exactly this.\nMore details in this answer.\n", "Using the API from Healthcare.gov to access healthcare plan data": "\n(Caveat - I have not used this API, but have used the raw datasets).\nI read the technical documentation for this API. It can be found at: https://finder.healthcare.gov/services\nThis is not a \"REST API\". It is similar, but instead of using parameters on the URL request, you place your request in XML as party of the body in a POST request to the healthcare API url.\nHow you do this will depend on what programming language you are using? Curious, are you a HS or college student.\n", "real time - Telemetry data feeds": "\n(note that 'telemetry' can mean a few different things ... for satellites, it's typically information about the location and other data about the operating status of the spacecraft)\nNASA's STEREO mission makes their telemetry available.  See : \n\nhttp://www.srl.caltech.edu/STEREO/attorb.html\n\nMost science missions have such information available, as the spacecraft location and pointing is necessary for proper interpretation of the data.  They'll also have other 'housekeeping' information (voltages, temperatures, etc.) which may be necessary for propler calibration.\nYou can find ephemeris data from other space science missions in the VSPO ... use the restriction 'Measurement Type' of 'Ephemeris'.\nYou can also compute spacecraft locations using SPICE.\n", "data request - Twitter open datasets": "\nWhile you will find some exceptions, sharing archived tweets is against Twitter Developer Policy.\n\nTake all reasonable efforts to do the following, provided that when requested by Twitter, you must promptly take such actions:\n\nDelete Content that Twitter reports as deleted or expired;\netc\n\n\nIf a user deletes their own content, the archive should reflect that deletion (which is a massive effort to continuously check).\nFor that reason, Twitter data sets are often shared as simply two fields: user_id and tweet_id. Then to reconstruct the dataset, one would query the API with those two keys. Then Twitter can ensure that if the tweet was deleted after the initial grab, the content won't show up in the second.\n\nAs an exception...  the banning of Politwoops, a service from Sunlight Foundation that archive deleted tweets from politicians. It has since been reinstated.\n\nIt doesn\u2019t appear that Twitter is changing its terms for developers. Instead, the company has reached an agreement with the Sunlight Foundation and the Open State Foundation, another group involved in versions of Politwoops around the world, to offer the an exception.\n\n\n\nIf you run the live stream, you can collect your own (you'll get more tweets than you can deal with).\nedit: I can add that if you want to 'search' for all tweets, you can use 'lang:en' as your search parameter. See here. Then you can loop over desired langauges.\n", "tool request - How might I go about visualising historical temperature CSV data?": "\nData\nHere is my take on it: I use R and its IDE RStudio.\nThe hard part, cleaning the data, is luckily done. Sharing the CSV via a dropbox link is not bad. The file is well structured. To improve it you could add a licence and provide a bit more information about the source. For more information see our certificates.\nIf you want to publish in a more \"professional\" way, you can use a platform such as Socrata or datahub.\nAnalysis\nUse the power of visualisation to get a sense of the data. For example, three histograms across all years. \n\nThen I would calculate summary statistics for groups such as years or months. That should inform you about outliers.\nWe can also look at a calendar visualisation. \n\nThe winter in 2010 seems to be particularly cold. Winter is coming...\nNow that you have a sense of the data, you can explore ideas of how to create an algorithm that finds interesting patterns and outliers.\nCode\nI have run similar analyses before therefore I have some code that I was able to re-use.\nWe share all of our code on GitHub. The relevant repository is here:\n https://github.com/theodi/R-playground/tree/master/weather-centuries\nThis is the syntax file for the graphics.\n(Note that calendarHeat.R is from Paul Bleicher.)\n", "government - Demography vs. political preference data sources": "\nTwo resources you might consider are:\n\nThe European Social Survey\nThe Comparative Study of Electoral Systems\n\n", "government - Understanding City Budgets": "\nopen budget and open spending are what you are looking for:\nhttps://github.com/adstiles/openbudgetoakland\nhttps://openspending.org/\nopenspending csv format is basic, but still quite confusing, at least for me:\nyou only need three columns: date, amount, unique id. from what i can tell, there is no way to automate this, and you're going to have to literally get all up in the guts of budget documents, and rip out what you need. i created a google doc you can see/copy here:\nhttps://docs.google.com/spreadsheet/ccc?key=0Aq0hZevysGfkdDNUVjVwazJ0M0NSZm9MdVBpTkhJdEE&usp=drive_web#gid=0 \nthere's more to openspending than three columns, plus my explanation is quite vague, so loading data into openspending in-depth guide here:  https://docs.google.com/document/d/1YBXX6du4rOV6OutZncT7gyJeOA7zHml3cC1TtWJW65w/edit \nopenspending is where you want to submit actual spending, so final budgets, and openbudget is where you want to release budget submissions, proposals, etc.  \n", "data request - Population Within Radius": "\nI have not done this before, but I do not think you can get exactly what you want (population + GDP) within a radius. You can get population/GDP/demographics down to a census tract. Here is what I would suggest as a rough method (in pseudo code), assuming your radius stays within a US county.\n\nGet the census tract KML datasets from US Census.\nFor the point of interest (POI)\nA. Determine which county the POI is in.\nB. Determine which census tracts are in that county.\nFor each census tract determine if the census tract polygon is following within the circle defined by the POI and radius.\nA. If so, include the population and other data as part of the radius\nB. If not, determine if the polygon intersects the circle.\nC. If so, use a rough approx. of how much of the population data for the census \n     tract to include (e.g., 50%).\n\nThe US Census KML datasets can be found here:\nhttp://www2.census.gov/geo/tiger/KML/2010_Proto/\n", "data request - Numeric facts database?": "\nI think the highest quality database you can find is wikipedia. Now bear with me for a moment: for every article wikipedia usually provides a very vast set of numeric facts along with the textual facts. The only thing you have to do is to download a wikipedia dump and using the markup language that wikipedia uses, extract the numeric facts.\n", "data request - How can I get a list of currencies from Wikidata?": "\nYes, there is.\nThere are several ways to get this list:\n\nYou can, in Wikidata website, look at every entity that has a link to currency (Q8142): https://www.wikidata.org/wiki/Special:WhatLinksHere/Q8142. The problem of this is that you will have some entities that makes a link to currency other than instance of.\nThe best way is to use an external tool called AutoList, developed by a German guy called Magnus Manske. The link of his tool is here:\nhttps://tools.wmflabs.org/autolist/autolist1.html\n\nThis tool uses a query language WDQ. Its documentation is here:\nhttps://wdq.wmflabs.org/api_documentation.html\nHere is how it works:\nThe property \"instance of\" has the identifier P31.\nThe value \"currency\" has the identifier Q8142.\nIf you want every Wikidata entity with the condition (P31 == Q8142) equals true: \n\ncopy-paste claim[31:8142] in the form field called Query;\nclick on the Run button.\n\n", "usa - Open API for SEC data?": "\nYou should take a look at Rank and Filed, a relatively new (Feb 2014) site about EDGAR filings, with lots of data export options.\n", "data portal - Search CSW for opendata without using an anytext filter?": "\nThe following python 2.7 code...\n# -*- coding: utf-8 -*-\n\nimport requests\n\ndef main():\n\n    get_records = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<GetRecords\n    xmlns=\"http://www.opengis.net/cat/csw/2.0.2\"\n    xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\"\n    xmlns:ogc=\"http://www.opengis.net/ogc\"\n    xmlns:ows=\"http://www.opengis.net/ows\"\n    xmlns:dct=\"http://purl.org/dc/terms/\"\n    xmlns:gml=\"http://www.opengis.net/gml\"\n    xmlns:gmd=\"http://www.isotc211.org/2005/gmd\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    service=\"CSW\"\n    version=\"2.0.2\"\n    maxRecords=\"{max_records}\"\n    startPosition=\"1\"\n    resultType=\"results\"\n    outputFormat=\"application/xml\"\n    outputSchema=\"http://www.isotc211.org/2005/gmd\"\n    xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\">\n    <Query typeNames=\"gmd:MD_Metadata\">\n        <ElementSetName typeNames=\"csw:IsoRecord\">full</ElementSetName>\n        <csw:Constraint version=\"1.1.0\">\n            <ogc:Filter xmlns:ogc=\"http://www.opengis.net/ogc\" xmlns:gml=\"http://www.opengis.net/gml\">\n                <ogc:PropertyIsLike wildCard=\"%\" singleChar=\"_\" escapeChar=\"\\\">\n                    <ogc:PropertyName>apiso:Subject</ogc:PropertyName>\n                    <ogc:Literal>%opendata%</ogc:Literal>\n                </ogc:PropertyIsLike>\n            </ogc:Filter>\n        </csw:Constraint>\n    </Query>\n</GetRecords>\"\"\".format(max_records = 1) # minimal 1 !\n\n    r = requests.post('http://gdk.gdi-de.org/gdi-de/srv/eng/csw', data=get_records, headers={'content-type': 'application/xml'})\n\n    xml_string = r.content\n\n    with open('for_opendata_stackexchange_com___output.txt', 'w') as file_out:\n        file_out.write(xml_string)\n\nif __name__ == '__main__':\n    main()\n\n...will result in a text file starting with...\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<csw:GetRecordsResponse xmlns:csw=\"http://www.opengis.net/cat/csw/2.0.2\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.opengis.net/cat/csw/2.0.2 http://schemas.opengis.net/csw/2.0.2/CSW-discovery.xsd\">\n\n  <csw:SearchStatus timestamp=\"2014-02-23T21:03:24\" />\n\n  <csw:SearchResults numberOfRecordsMatched=\"1053\" numberOfRecordsReturned=\"1\" elementSet=\"full\" nextRecord=\"2\">\n...\n\nSo the German geo data catalogue has 1053 open data sets.\n", "tool request - Parsing Curriculum Vitaes": "\nI'm a python fan, so that is the path I normally take.\nIf the CVs are in PDF format, then I use pdftotext to convert them into .txt files (without formatting).\nOnce in .txt files, to find email addresses, I usually split the lines into individual strings, and then look for strings that contain the '@' character. Since that may include twitter names or other noise, I then include only the strings that have a '@' character between two alpha-numeric characters.\nTo get phone numbers I look for clumps of numbers separated by '.', '-', or including parentheses. To get addresses I look for integers and then alpha strings. Names are usually the first alpha string in the .txt file. I have some synonym list for Skills, Education, etc.\nThis process goes on and on, and gets more complicated for getting sections and tagging appropriate sections.\nI do the process iteratively, meaning that I scan 100 CVs in a training set. This tests my algorithm. If the CVs are from a particular industry (or academic), then my algorithm is tuned along the way to take into account different options.\nedit: It's a dirty process, but it works for me. I value volume over accuracy.\nIf the CVs are in HTML format, like from indeed.com, then I use BeautifulSoup to parse the fields from the text. This is usually easier because the fields like Education are wrapped in an HTML tag that includes the string 'Education'. The only difficulty is being aware of all fields that may exist, since not everyone has all the sections.\n", "tool request - Open Product Data sources and importing into SQL": "\nRegarding your first question, Open Product Data provides guidelines for data importing which suggests using software such as BigDump to handle importing the data.\nNow, to answer your second question:\nOutpan is my personal project focusing specifically on creating a free database of all barcoded products. The database currently includes more than 18 million products and an API to easily access the content programmatically. As I mentioned before in another answer our top priority is to keep this completely free (hopefully even open source) and accessible for everyone.\nNOTE: I am affiliated with Outpan.\n", "tool request - How do I share Open Data with others on this SE site?": "\nWe have recently created the OpenData StackExchange organization on datahub.io.\n\nWelcome to the Datahub, the free, powerful data management platform from the Open Knowledge Foundation, based on the CKAN data management system.\nThe Datahub provides free access to many of CKAN's core features, letting you search for data, register published datasets, create and manage groups of datasets, and get updates from datasets and groups you're interested in. You can use the web interface or, if you are a programmer needing to connect the Datahub with another app, the CKAN API.\n\nOpenData meta discussion\n\nUpdate 2017: I've learned that there is a 100MB limit per file (see answer), and there is an open question about how to host larger files.\n", "data request - Famous people dataset": "\nWikipedia has a list of lists of people, from that you can focus on the specific lists that are of interest to you.\nThey also have a very functional API for collecting data, see here for details. For your lists of interest you can collect whatever data you like for individual \"famous people.\"\n", "data request - Free database or API of all North American businesses": "\nThis question was previously answered for non-profits, and I think the same answer works here at least partially.  \nIn addition, you can find all U.S. companies at the Securities and Exchange Commission's EDGAR site. Technical documentation is also available.\nInformation about Canadian companies is accessible through their open data site, but seems to be segmented by industry.\nMexico does not seem to have corporate content available publicly, but an explanation of where they are is at the Open Data Index.\nThere's a nice overview on Programmable about ways to access this type of information.\n", "data request - Where can I find dataset around child labor in cocoa production?": "\nGreat question!\nI'll assume a dataset doesn't exist because it makes the creation of a data-story more interesting.\nHere are some steps that I would take to collect the data and start to analyze it. Remember, a question is needed before we can ask the data for answers. For that reason, I'll make up the question: \"Do cocoa producers have higher rates of child labors than other agricultural exporting countries?\"\n\nI would do some background reading. Wikipedia has a section about cocao and child labor. From this reading I learn that Ivory Coast has approx 200,000 child laborers in the cocao industry (wow). From this article and other I'll start by making a list of cocoa producing countries.\nI would then make a spreadsheet with rows for each cocao producing countries and columns for data like population, Gross Domestic Product (GDP) per capita, amount of cocao exported, amounts of total agricultural exports, etc. CIA World Factbook is one way to collect data, although it doesn't break down exports as percents of GDP, or of a total amount. I'd have to dig more for a more detailed data source.\nNow that I have columns of economic data, I would add columns for child labor statistics for those countries. There may exist a database but you can also manually collect data for the 10-15 countries that are cocoa producers. Here is the data for Ivory Coast. Wow, 35% child labor between ages 5 and 14 (definitions).\nNow comes the fun part - playing with the data. Scatter plots are easy and a great way to explore data (see the image blow). For this concept, I would start with the econonic indicators (best option is \"cocoa production as percentage of agricultural exports\") of each country on the x-axis and the child labor stats on the y-axis. In this case, maybe I'll find that countries that have more cocoa (large x) also have high percentages of child labor (large y). In that case, maybe the scatter plot shows a linear relationship (correlation). I don't expect my first guess to be the final product, but I start to get a feeling for the data. I have to understand the data, for example, when comparing countries, it's often the case that I have to normalize by population (per capita).\nNow that I'm familiar with the data, I should probably clean things up. This may mean including other agricultural countries that don't have cocoa production as a comparison. Now my scatter plots have two colors. Do these countries also have high child labor rates? Are there outliers (cocoa exporting countries without high rates of child labor)? Ask I ask these questions, it becomes essential to collect other data. And the process continues until I can pull the data together and tell a story.\n\n\n", "data request - Open Source alternative to IMDB": "\nSadly, Freebase has been made \"read only\" March 31st 2015.\nAlternatives are:\n\nhttps://www.themoviedb.org/\nActively maintained by a large community and used by a broad range of apps under a propriatary license that states free use:\n\nTMDb is committed to free and open access to our APIs for commercial and non-commercial purposes. However, providing the APIs does have real costs for TMDb. For uses of TMDb APIs over a certain rate or for certain types of commercial applications, TMDb reserves the right to charge fees for future use of or access to the TMDb APIs.\n\nhttp://www.omdbapi.com/\nRun by Brian Fritz under a CC-BY 4.0 licence.\n\nThe OMDb API is a free web service to obtain movie information, all content and images on the site are contributed and maintained by our users.\n\n\n", "data request - Free high quality geolocation database": "\ncheck out geonames, that's the only thing that comes to mind\nhttp://www.geonames.org/\n", "JSON file with jquery - Stack Overflow": "\nI think this is a question for Stackoverflow, but in the meantime I can point you to an intersting tutorial. It's pretty comprehensive.\n\nWe use jQuery\u2019s getJSON function, which, by definition, loads \u201cJSON-encoded data from the server using a GET HTTP request.\u201d\n\n", "spending - Estimate of total public expenditure from governments around world?": "\nThis is the type of question that wolframalpha.com is really handy for.\nEnter \"total government expenditure\" in the input box, and see the results. The results also provide the information sources used.\nEdit:\n\n", "data request - Translation dictionaries": "\nFor free Japanese to English datasets, I think this stackoverflow question may have some answers for you: https://stackoverflow.com/questions/2716792/freely-available-dictionary-data-for-chinese-japanese-cjk-characters\nThe referenced JMDict project may be what you are looking for.\n", "data request - Where can I find a dataset of songs labeled with their genre, BPM and key?": "\nI haven't used them before. I've just had them in my bookmarks for any case.\n1) http://echoprint.me/\nFree: 'Use our data for whatever you want (commercial or non, research, personal use)'\n2) http://the.echonest.com/\nFree for non-commercial: The Echo Nest APIs are free for non-commercial purposes. To use them commercially, contact us and we will go over licensing options.\nI think that both of them are services to identify a song like Shazam does. But they let you download their data or use their API.\n", "data request - Where can I find a dataset of albums labeled with their release date, band name, genre, and number of sold copies?": "\nI think you can find an API based on the answers to the question here.\nIt may be harder to find the number of copies sold, so you may need to collect from two sources and join them together. In particular, the API listing site ProgrammableWeb lists many music APIs.\n", "data request - Where can I find a dataset of academic conferences?": "\nI would suggest that you scrape the data on SSRN by looking by field/location/date.\n", "data request - Where can I find a dataset of drones attacks?": "\nI found at least one non-profit that has a website (updated link) that contains statistics based on specific country. I don't know how accurate they are because many attacks are probably still unknown. I don't know if you can download their data as a database, but I think with finite data points, you can reconstruct the data set yourself.\nFor the optional stuff:\n\nDemographics: are sort-of there (Militants killed, Civilians killed, Unknown killed, Target organization)\nReason for attack: you may have to dig up media stories for specific attacks and then join that info with the data set\nOrganization/country piloting the attack: this website has organization and is sorted by country.\n\nI would start by reaching out to this organization (or any other similar ones) and ask if you can take an export of their data set. They use a Creative Commons license, so they encourage non-commercial use with attribution.\n", "data request - Dataset with biggest events in the world/region, reporting number of attendees, geolocation and date": "\nHere's a slightly off-topic, but perhaps interesting pick from Wikipedia on the (estimated) Largest peaceful gatherings in history, mainly for religious occasions. Some examples:\n\nAn estimated 20 to 25 million people visited the shrine of Husayn ibn Ali in Karbala, Iraq during Arba'een in December 2013.\nAn estimated 4.2 million people attended a concert given by Rod Stewart in Rio de Janeiro, Brazil on 1996-12-31.\nAn estimated 1.25 million people attended a Papal mass given by Pope John Paul II in the Phoenix Park, Dublin, Ireland on 29 September 1979.\n\nSimilar lists by topic could get you started on multi-sport evens (like the Olympic Games), running events, the largest concerts ever...\n", "What can \"open data\" with a CC-BY-NC-ND (Creative Commons Attribution NonCommercial NoDerivatives) license be used for?": "\nI don't agree with your conclusion completely in one respect:\nI think there is a scenario where you can use CC BY-NC-ND (version 3 or 4) data and still stick to the license restrictions if you:\n\nUse it for a non-profit/ non-commercial use case such as a web offering by a public body or an NGO\nYou do not change any of the triples in this data set\nYou do not extend the data set by triples which use their ontology (classes, properties)\n\nI think the third restriction might be a little bit too cautious but I just want to make sure there is really no potential infringement to the license.\nIf this is assured, I think there is no problem in importing this data together with your data in a database or a triple store as this is not a derivation of the external data set but merely a physical representation of the data.\n", "transportation - Free database of vehicle data and images": "\nFor the United States, FuelEconomy.Gov provides a seemingly comprehensive database of vehicles. You can use the API to pull down select vehicles or all vehicles in the database. In is primarily to communicate fuel economy, but a number of other vehicle features are also reported (transmission, engine type, etc.).\nFull disclosure: I'm the developer of an R package aimed at extracting data from the API.\n", "telecom - Any CDR (call data record) dataset?": "\nTry with d4d challenge from orange Senegal:\nhttp://www.d4d.orange.com/en/home\n\"2.5B anonymized records of 5 million mobile phones\"\n", "finance - Banking open data from Credit Agricole?": "\nI think you are looking for the Cr\u00e9dit Agricole API, which has a page here. As far as I can tell, it's designed for app designers so each account would require individual authentication. Maybe\nThere are tons of news stories about it, check out examples here, here or here.\n", "data request - Vehicle Miles Traveled in New Jersey before 1981": "\nNational statistics for miles traveled on different roads and for different uses have been compiled for the United States by the Department of Transportation.\n\nHistorical listings from 1936-1995\nDetailed listings from 1995-present\nNational Household Travel Survey (online data extraction tool and tables)\nSpecific to New Jersey (after 1995)\n\nEven at the New Jersey state level, the statistics only go back to 1995, but they are complete for miles traveled.\n", "best practice - A web API user's guide for free and open data": "\nAPIs are often offered by websites so that developers can use the web-based data for apps, without having the uncertainty and difficulty of scraping the HTML. But it's not necessary to use the data to build apps, and this means that APIs can be a great source of data for research and analysis. Just to name a few types of API data: weather forecasts, historical stock prices, the twitter live-stream, wikipedia page views, ... and the list goes on and on.\nFull list of APIs: ProgrammableWeb has a comprehensive list of APIs (more than 10,000). The list is so comprehensive that it's hard to know where to start. In the answers below you'll find details about popular APIs as well as some instructions for how to connect to data.\nAuthentication: Some APIs are free and open and don't require authentication, although they will often block your IP address if you make too many requests or request too fast. Other APIs require authentication but are free for basic usage. Each API has its own terms of service, and can restrict unlimited access by either capping the total calls per time period (i.e. 1000 per day), or by using a rate-limiting quota (i.e. 1 call per second).\nData formats: Data from APIs usually flows via a RESTful interface, and common formats are JSON, XML, and sometimes CSV (comma separated values).\nSoftware and code: With a correct URL/URI, anyone can access the raw data from the browser (for example, JSON, XML, or even a CSV file). Here is a handy overview for non-programmers. Programming skills are usually needed to repeat or automate a process. There are some open source tools such as KNIME that allow non-programmers to connect to a RESTful interface and then collect and analyze data. For Python, see the Open Data Python Guide for examples of how to read and use the data files described above.\n\nHere are just a handful of examples of the diverse and useful web-based API data\nWikipedia page views:\n\nstats.grok.se\n\nWikipedia page views are useful for many things, including predicting changes in a stock price (paper)\n\nA flood of views to a company's Wikipedia page may be a sign that their stock price is about to plummet. (news article)\n\nYou can access the raw data without any authentication, either by download or via an API. \nFor the API, you construct a URL like this: http://stats.grok.se/json/en/201306/Data , which would give you the page views in JSON format for the article 'Data' from June, 2013.\nIt helps very much to first verify that you page and title exists. It does.\nSample python code to access page views for two articles ('Advisor' and 'Adviser') for all months between 2008 and 2014 (my reason). Note that days that don't exist (i.e. June 31st) will return 0 (zero) page views.\n\n\n\nWeather data (historical):\n\nWunderground.com \n\n500 free API calls per day (with authentication), and offers historical detailed daily weather data going back to 1997 for my location (details).\nInternational weather, so it is not only restricted to the US.\nData comes in JSON format (example data, click \"show response\").\nSample python code for current and historical weather.\n\n\n\nStock quotes\n\nMarkit On Demand\n\nFree and authentication-less API for obtaining stock quotes and historical data\nDocumention\nexample GET request with JSON output\n\n\n\nThe Internet Archive\n\nWayback Machine\n\nArchives HTML from old websites - example: NY Times from Feb 24, 2001\nAPI Documentation - No authentication needed (!)\nExample python code that finds historical New York Times front pages that include a certain string, and prints a link to those archived pages.\n\n\n\nGoogle Maps Geolocation API\n\nPass search string and receive latitude, longitude and other data.\n100 free requests per 24 hours (no authentication required)\nExample lookup\nPython code sample\n\n\n", "data.gov - Federal Holiday Calendar Service": "\nHere are a few things that might be helpful.\n\nFederal holidays through 2020\nCurrent status with XML and RSS feeds\n\nAlerts and hurricane warnings are also listed.\n", "data request - Drink Recipe Database": "\nThe Wikimedia project Wikibooks has a book on bartending, including a full chapter of cocktail recipes. Not a database, but pretty comprehensive and possibly a good start for a proper database.\n", "computing - A resource for wearable data?": "\nKaggle had a competition that involved \"biometric data,\" which came from sensors in people's cell phones.  The data is available at http://www.kaggle.com/c/accelerometer-biometric-competition/data\n", "data request - Where can I find a dataset that lists all the businesses and their addresses operating in a given city?": "\nI would look into Google Places API for this. You have to register for an API key and the quota is limited to 100,000 requests a day (plus some multipliers on some requests), but it's a rather verbose database.\nExample:\nHere is a request URL to get all \"food\" places within 500m of the specified latitude-longitude that has the word \"harbour\" in it:\nhttps://maps.googleapis.com/maps/api/place/nearbysearch/json?location=-33.8670522,151.1957362&radius=500&types=food&name=harbour&sensor=false&key=AddYourOwnKeyHere\n\nThe response is JSON so you will need to do a little parsing unless you use their JavaScript API or use some other wrapper.\nIf you're using Java, I actually developed my own Java wrapper around this API which I believe successfully maps the API 1 to 1 if it fits your purposes: https://github.com/windy1/google-places-api-java\nWith this library you can just do something like:\nGooglePlaces client = new GooglePlaces(\"apiKey\");\nList<Place> places = client.getNearbyPlaces(lat, lng, radius, GooglePlaces.MAXIMUM_RESULTS);\nfor (Place place : places) {\n    System.out.println(place.getName());\n    System.out.println(place.getAddress());\n}\n\nHope this helps!\n", "web crawling - Is it legal to crawl research papers from ACM/IEEE?": "\nFrom the ACM terms of usage page\n\nTo copy otherwise, to republish, to post on servers, or to\n  redistribute to lists, requires prior specific permission and/or a\n  fee. Send written requests for republication to ACM Publications,\n  Copyright & Permissions at the address above or fax +1 (212) 869-0481\n  or email permissions@acm.org.\n\nThus, I believe that you shall contact with them before doing anything else. Also, without looking for the copyrights on IEEE, something similar probably would be the case.\n", "data request - Product database for firearms, guns, ammo, weapons, munitions?": "\nSIPRI (Stockholm International Peace Research Institute) has some databases available on weapons that were traded between countries or political groups. I think the weapons are going to be military-grade, so I'm not sure how useful for your question. Also, the data is there but quiet buried under manual extraction process and a format that is not easily machine readable.\n\nSIPRI Arms Transfers Database\n\n\nShows all international transfers in seven categories of major conventional arms since 1950, the most comprehensive publicly available source of information on international arms transfers.\n\nThe data at this level is aggregated, so for particular weapons, we have to go one level deeper. \n\nTrade registers\n\n\nProvide information on each deal included in the database. Information provided includes, inter alia, the suppliers and recipients, the type and number of weapon systems ordered and delivered, the years of deliveries, and the financial value of the deal.\n\nThe screen should look like this:\n\nThe exhausting effort will be to create all the combinations of countries, but you can narrow the search to countries that manufacture weapons as the supplier.\nThen, when you've gone through all \u221e combinations, the data turns out to be in RTF files (which you can convert to text and then parse and structure).\nHere's a screenshot of the \"raw\" data.\n\n\nThis is a good task for a web scraper like Python's mechanize, scrapy, etc\n", "data request - Where can I find a database of hotel property locations?": "\nThere is a long discussion from 2009 on StackOverflow which you may find helpful.\nIn particular, I'd check out the Expedia Affiliate Network or try to get TripAdvisor access. \n... And maybe api.hotelbase.org will come back some day.\n", "data request - Searching for a database for spaceships, satellites, drones, etc.?": "\nI would guess that scraping lists an timelines linked on the summary page Spaceflight lists and timeline (Wikipedia) will cover most satellites, probes and drones operational today and in the past.\n", "computing - Data sets from motion capture?": "\nHere is a source of many varied subjects, motions, and motion categories: CMU Graphics Lab Motion Capture Database\n", "medical - Diabetes patient record data sets?": "\nYour question is so general it's hard to be sure I'm answering it correctly. More detail would be quite helpful. Here's a stab, though:\nIf you're looking for datasets that are already open and are fairly standard social science data, I would look at what ICPSR holds. They have several datasets, in particular this one, contributed by Jens Ludwig at UChicago.\nHowever, none of the files they have probably match what it sounds like you're looking for: in-depth logs from monitoring devices. On that I don't know of any data that's available, at all; to my knowledge, monitoring devices (at least in the US) don't tend to be wired at all.\n", "images - Is it legal to make an open face recognition database of public people like celebrities?": "\nWhy wouldn't it be?\nYou can take pictures of people in public and post them online legally in the U.S..  \nA better question around this topic is:\n\"What are you planning to do with an open face recognition database of public people/celebrities?\"  \nAnd an even better question for this topic:\n\"Are your plans involving creating an open face recognition database of public people/celebrities ethical and moral?\" \n", "education - Where can I find data on university expenditures?": "\nEvery year the US Department of Education requires all accredited post-secondary education institutions to complete the IPEDS survey. It's huge amount of data. You can get canned and customizable downloaded datasets here. I've used this site a lot. \nhttp://nces.ed.gov/ipeds/datacenter/\nThe datasets contain information on past, current and projected costs of education per institution, including breaking it down by in-state, out-of-state, foreign, tuition, books, fees, on and off campus housing, etc.\n", "uk - When is open data not open?": "\nAll open data are good, but some are better.\nIn a colloquial way: open means that everyone can use it, for any purpose. The idea is that you have as few technical, financial and legal barriers as possible.\nQuite a popular definition is the one from the OKF:\n\n\u201cA piece of data or content is open if anyone is free to use, reuse, and redistribute it \u2014 subject only, at most, to the requirement to attribute and/or share-alike.\u201d\n\nI believe this becomes more clear when we enter the \"grey areas\":\n\nOpen usually means free. However, sometimes it may include a nominal fee when the publication of data is costly.\nOpen usually avoids a non-commercial licence. Excluding commercial activities brings the uncertainty of the exact definition of \"non-commercial\" and its potential legal ramifications.\nTwitter is also somewhere in between. Some people argue it's \"open\" (e.g J. Gurin in Open Data Now) perhaps because of its public nature. However, there are serious restrictions of who and how you can use Twitter data.\n\nGlasgow\nOn their site they write\n\nGlasgow City Council data is now open by default, freely and easily\n  accessible to all.\n\nUnfortunately, the link to \"open by default\" goes nowhere at the moment. The licence would explain what you can and cannot do with it. I would assume (and I may be wrong) that the Open Government Licence applies. It is relatively generous and easy to read.\n", "tool request - Collect a list of open data systems": "\nI've compiled a fairly extensive list of open government portals both in US and around the world in the last few days. I am working on code to support crowdsourcing the catalog. I also have a CSV version for download. There is about 700 sites listed so far:\nONLINE CATALOG: http://www.opengeocode.org/opendata/\nDOWNLOAD: http://www.opengeocode.org/opendata/opendata.csv\nI've setup the catalog for crowdsourcing, so feel free to send us your suggested portals to add to the catalog.\n", "data request - Popular News Database": "\nThe New York Times has a web-based archive of news stories going back to 1851. Headlines are available without being a paid member. I'm sure other individual media companies will have a similar archive, but I'm only familiar with the NYTimes one.\nAfter searching a time range, we notice that the URL of the results page can be used to create a scraping alogirthm.\nhttp://query.nytimes.com/search/sitesearch/#/*/from19140101to19140131/\n\ncontains results from Jan. 1, 1914 to Jan. 31, 1914. You can even a summary of each article and download the full PDF!\nUnfortunately, it's not a neat and clean database... Here is a sample scraping algorithm I wrote in order to get the Google results count from the HTML page (link). The important part is here (download and parse)\ntext = requests.get(url).text # get google html\nm2 = re.search('About ([0-9,]+) results', text) # search for results\n\nYou may find that the number of searches is limited (so be gentle with the scraping).\n", "data request - Datasets with Practical Applications": "\nMy opinion: Most of machine learning isn't wasting cycles but asking a question and then designing a path to an answer.\nIf you want to 'waste' cpu cycles, consider running one of the distributed computing projects. Folding@Home is one of the best known examples.\nIf you want to find something new, then find a topic that interests you and then ask a question that can be answered by collecting data. There are seemingly infinite projects and data for those projects exist(s) all around us. Sometimes it's as simple as running a code or collecting web-based data, like N-grams or Search Trends. Other times it means downloading a 'traditional' data set and creating a unique analysis or new ways to visualize the data. And other times it requires being a 'data journalist' who gets dirty digging through data to uncover a story.\nIf you want to see what people are doing outside of Kaggle, check out datatau.com, /r/machinelearning, etc. Or get involved with the Open Knowledge Foundation. Or find local people and start to see where there is local need.\n", "tool request - Do you know a technique for text mining judicial decisions?": "\nIf you are interested to just scrape some data from web pages, then like @magdmartin wrote, you just need to write some code to download the HTML (python requests package) and then to parse the HTML (BeautifulSoup in this case).\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nr = requests.get('https://www.canlii.org/en/ca/scc/doc/2007/2007scc4/2007scc4.html')\nsoup = BeautifulSoup(r.text)\nprint soup.find(text=re.compile('Present:')).encode('utf-8')\n\nThis python 2.7 code finds the names of the judges based on finding the string 'Present' in the HTML-encoded text.\n\nThe way I approach a problem like this is:\n\nFind one website that has as many court cases as possible.\nGet familiar with the HTML. In Firefox, control+U is how to view the page source.\nDevelop a scraping algorithm that starts small (like the code above that finds the judges' names), and then gets more ambititious as the code develops.\nDeploy your code to as many pages (court cases) as possible (not manually, but with a crawling algorithm).\n\nAlso, always look for APIs that would allow you to automatically download the data you want without the messiness of HTML scraping.\n\nIf you start to do full-blown text-mining (analysis on the text), you will need a legal ontology. One example is here.\n", "usa - List of data sources at the state, county and zip / zcta level": "\n\nHealth\n\nBehavioral Risk-Factor Surveillance System (BRFSS) - A health-related survey that asks respondents about health and disease risk factors.\n\nUnit of Analysis: County\n\nArea Health Resource File (AHRF) -  A compilation of Census Bureau demographic information, along with information about hospital utilization, health professionals, and natality/mortality.\n\nUnit of Analysis: County\n\nHealth Professional Shortage Areas (HPSA) - An estimation of areas where the population may be underserved by healthcare systems.\n\nUnit of Analysis: Varies from Census Tract to County\n\nSNAP Participation -  Enrollment Data for the Supplemental Nutritional Assistance Program by Year.\n\nUnit of Analysis: County\n\nFood Deserts - An analysis of populations that are far from stable food sources, and are thus reasoned to have low access to supply.\n\nUnit of Analysis: 2000 & 2010 Census Tract\n\nFood Security - Indicators of inequality of access to food sources.\n\nUnit of Analysis: County\n\n\nCrime\n\nNational Incident Based Reporting System NIBRS - \"The National Incident Based Reporting System (NIBRS) is an incident-based reporting system for crimes known to the police. For each crime incident coming to the attention of law enforcement, a variety of data are collected about the incident. These data include the nature and types of specific offenses in the incident, characteristics of the victim(s) and offender(s), types and value of property stolen and recovered, and characteristics of persons arrested in connection with a crime incident.\"\n\nUnit of Analysis: County\n\n\nEducation\n\nIntegrated Postsecondary Education Data System (IPEDS) - the primary source for data on colleges, universities, and technical and vocational postsecondary institutions in the United States.\nCommon Core of Data (CCD) - \"The Common Core of Data (CCD) is the Department of Education's primary database on public elementary and secondary education in the United States. CCD is a comprehensive, annual, national statistical database of all public elementary and secondary schools and school districts, which contains data that are designed to be comparable across all states.\"\n\nUnit of Analysis: Individual Institutions\n\nPrivate School Survey (PSS) - \"The target population for the survey consists of all private schools in the U.S. that meet the NCES definition (i.e., a private school is not supported primarily by public funds, provides classroom instruction for one or more of grades K-12 or comparable ungraded levels, and has one or more teachers. Organizations or institutions that provide support for home schooling without offering classroom instruction for students are not included.).\"\n\nUnit of Analysis: Individual Institutions\n\n\nEconomic\n\nLongitudinal Employer-Household Dynamics (LEHD) - This is an extremely useful dataset. Based on data from the Quarterly Census of Employment and Wages, the information published comes in three forms: The LEHD Origin-Destination Dataset (LODES), the Workforce Area Characteristics Dataset (WAC), and the Residence Area Characteristics Dataset (RAC). The WAC posts demographic information on jobholders by the area that their job is located; you will be hard-pressed to find this level of detail ANYWHERE else. The RAC posts demographic information on jobholders by the area that they live in. The LODES indicates the combination of the residence and worksite locations by each jobholder's job.\n\nUnit of Analysis: 2000 & 2010 Census Block\n\nCounty and Zip Code Business Patterns (CBP / ZBP) - \"County Business Patterns (CBP) is an annual series that provides subnational economic data by industry. This series includes the number of establishments, employment during the week of March 12, first quarter payroll, and annual payroll. This data is useful for studying the economic activity of small areas; analyzing economic changes over time; and as a benchmark for other statistical series, surveys, and databases between economic censuses. Businesses use the data for analyzing market potential, measuring the effectiveness of sales and advertising programs, setting sales quotas, and developing budgets. Government agencies use the data for administration and planning.\"\n\nUnit of Analysis: Zip Code, County, MSA, State\n\nQuarterly Workforce Indicators (QWI 1,2) - \"The Quarterly Workforce Indicators (QWI) are a set of economic indicators including employment, job creation, earnings, and other measures of employment flows. The QWI are reported using detailed firm characteristics (geography, industry, age, size) and worker demographics information (sex, age, education, race, ethnicity).\"\n\nUnit of Analysis: County, MSA, State\n\nLocal Area Unemployment Statistics (LAUS) - A predictive model of county-level unemployment by year. It is based on an estimation using the Current Population Survey, state payroll, and state unemployment insurance sources.\n\nUnit of Analysis: County\n\n\nMigration\n\nSOI Tax Stats - County-to-County Migration Data Files - This source of information is unique from Census Bureau Migration data because it estimates emigration from areas as well.\n\nUnit of Analysis: County\n\n\n\n", "data request - Where can I find the dataset that has all open research publications (just titles and abstracts)?": "\nRestricting this question to \"Computer Science\" (as suggested in the comments) and reducing \"all possible\" to the ones which were actually published, one resource is The DBLP Computer Science Bibliography. Note that this source follows ODC-BY 1.0.\nThe DBLP has a search interface with an attempt to identify coauthor communities. They also offer bulk downloads in XML format.\nSome other research subjects have good sources, such as PubMed for biology, medicine, etc.\n", "best practice - What are good examples of open data dashboards?": "\nYou already listed a bunch. There's also\n\nPhiladelphia's Open Data Pipeline\nWhitehouse Scorecard\n\n", "biology - Cannabis Data Set": "\nTry the sites linked at the bottom of http://www.theplantlist.org/tpl1.1/record/kew-2696480, particularly NCBI one.\nJust about the seeds: http://data.kew.org/sid/SidServlet?ID=4439&Num=5E7\n", "data request - Database of self-inflicted injuries?": "\nSounds like very interesting research.  \nThere is a database of self-inflicted injuries and suicide from the U.S. Centers for Disease Control and Prevention.  Faststats has both summary and detailed information about this.  There is a related set of data on unintentional injuries that may also be helpful.\n", "usa - Automobile accident data in the US": "\nCalifornia Polytechnic State University has a downloaded dataset on fatal accidents on the national highway system for the year 2007:\nhttps://wiki.csc.calpoly.edu/datasets/wiki/HighwayAccidents\nData.Gov has a number of datasets on accident data by state:\nhttps://explore.data.gov/catalog/raw?tags=crash\nThe NHTSA has summary statistics for 2012 on a state by state basis here:\nhttp://www-nrd.nhtsa.dot.gov/departments/nrd-30/ncsa/STSI/USA%20WEB%20REPORT.HTM\nThe Insurance Institute for Highway Safety has accident/safety ratings per make/model/year, but you will have to scrap it off their site:\nhttp://www.iihs.org/iihs/ratings\nSome of the city level open data portals also publish traffic accident datasets:\nDenver: http://data.opencolorado.org/dataset/city-and-county-of-denver-traffic-accidents\nSeattle: https://data.seattle.gov/Public-Safety/Traffic-Accidents/7ayk-pspk\nThe UK data.gov publishes accident/road safety datasets from 1979 to present here:\nhttp://data.gov.uk/dataset/road-accidents-safety-data\nI put together an online catalog of sites covering traffic surveys and traffic accidents: http://www.opengeocode.org/opendata/traffic.php\nA list of links from albert,\nNCSA Publications & Data Requests http://www-nrd.nhtsa.dot.gov/Cats/index.aspx\n you probably want this guy:\nhttp://www-nrd.nhtsa.dot.gov/Cats/listpublications.aspx?Id=F&ShowBy=DocType \nsuper in-depth stats\nhttp://www-fars.nhtsa.dot.gov/Main/index.aspx \ncrash stats archive\nhttp://ai.fmcsa.dot.gov/CarrierResearchResults/Archives.asp?p=23 \nstats and facts\nhttp://www.fmcsa.dot.gov/facts-research/art-stats-facts.htm \nnhtsa data\nhttp://www.nhtsa.gov/NCSA \nfederal trans safety somtehing\nhttp://www.fmcsa.dot.gov/ \nnational highway traffic safety administration\nhttp://www.nhtsa.gov/NCSA \nnhsta crash stats\nhttp://www-nrd.nhtsa.dot.gov/cats/listpublications.aspx?Id=F&ShowBy=DocType \nmore crash stats\nhttp://ai.fmcsa.dot.gov/CrashProfile/CrashProfileMainNew.asp \nhttp://www.fmcsa.dot.gov/facts-research/art-stats-facts.htm \nhttp://www-nrd.nhtsa.dot.gov/Cats/listpublications.aspx?Id=F&ShowBy=DocType \nhttp://ai.fmcsa.dot.gov/CarrierResearchResults/CarrierResearchContent.asp \n", "data request - Looking for a list of major cities of the world": "\nGeonames has a tsv with  data of the cities with more than 15k inhabitants. You only need to filter the ones you are interested in. The tsv has a column with the population (the 15th one) so it should be quite easy to get them.\nYou can download it here\n", "Data of vehicle traffic": "\nYou can find plenty of summary data, but I have not seen any publicly available raw counter data. Here's some summaries:\nFederal Highway Administration\nTraffic Volume Trends is a monthly report based on hourly traffic count data reported by the States. These data are collected at approximately 4,000 continuous traffic counting locations nationwide and are used to estimate the percent change in traffic for the current month compared with the same month in the previous year.\nhttp://www.fhwa.dot.gov/policyinformation/travel_monitoring/tvt.cfm\nArizona Dept of Transportation\nThe annualized average 24-hour volume of vehicles at a given point or section of highway is called a traffic count. It is normally calculated by determining the volume of vehicles during a given period and dividing that number by the number of days in that period. \nhttp://www.azdot.gov/planning/DataandAnalysis\nCalifornia Dept. of Transportation\nThe Traffic Data Branch is responsible for the collection and dissemination of historical volumes (counts). We also produce the Mobility Performance Reports.\nTRAFFIC COUNTS, also called Traffic Volumes, are available in various formats, and are only for the State Highway System. Highways are signed as Interstate, California State Route, or United States Route.\nhttp://traffic-counts.dot.ca.gov/\nColorado Dept. of Transportation\nThis is the access point to information frequently used for transportation planning and project development. Information is provided on current and projected traffic volumes, state highway attributes, summary roadway statistics, demographics and geographic data.\nhttp://dtdapps.coloradodot.info/otis\nFlorida Dept of Transportation\nWelcome to the Florida Department of Transportation's Traffic Information site. This site provides statistical traffic information for Florida's State Highway System.\nhttp://www.dot.state.fl.us/planning/statistics/trafficdata/\nIndiana Dept of Transportation\nINDOT Interactive Traffic Count Map showing the annual average daily traffic (AADT). \nhttp://dotmaps.indot.in.gov/apps/trafficcounts/\nMaine Dept of Transportation\nTraffic Monitoring is responsible for the collection of all types of traffic data including traffic volumes, vehicle classification, turning movements and special studies as requested by the Department.  The reporting of traffic volumes is accomplished through two distinct methods involving the Continuous Count and Coverage (i.e. short term) Count programs.\nhttp://www.maine.gov/mdot/traffic/tc.htm\nMassachusetts Dept of Transportation\nThe Massachusetts Highway Department conducts an annual traffic data collection program. This data is available online by autoroute and city/town list or as an interactive map. You can view data for a specific town from the Town Index, for numbered routes from the Numbered Route Index, or you can use the interactive map to browse data by area. You can also download the complete spreadsheets \nhttp://www.mhd.state.ma.us/default.asp?pgid=content/traffic01&sid=about\nMichigan Dept of Transportation\n2012 Average Daily Traffic (ADT) Maps\nhttps://www.michigan.gov/mdot/0,1607,7-151-9622_11033_11149---,00.html\nMinnesota Dept of Transportation\nThousands of traffic counts are collected on Minnesota roadways each year. This information is used to produce volume, classification, speed and weight data as well as traffic forecasts, reports, maps and analysis. \nhttp://www.dot.state.mn.us/traffic/data/\nNew York State Dept of Transportation\nThe Highway Data Services Bureau is responsible for the collection and dissemination of information on the extent, use and condition of the public roadway system in the State of New York. The Bureau consists of three sections: Highway Data, Traffic Monitoring, and Pavement Data.\nhttps://www.dot.ny.gov/highway-data-services\nNorth Carolina Dept of Transportation\nAnnual Average Daily Traffic (AADT) Traffic Volume Map presents the traffic average for the year at specific points on North Carolina highways. Data is collected at more than 40,000 locations throughout North Carolina using Portable Traffic Count Stations. AADT map is typically published at the later part of the summer.\nhttp://www.ncdot.gov/projects/trafficsurvey/\nOhio Dept of Transportation\nTraffic Count Information & Maps includes Traffic Survey Reports and Traffic Survey Flow Maps. The Traffic Survey Reports list an estimate of Annual Average Daily Traffic (AADT) volumes separated by cars (Pass &A Com\u2019l) and trucks (B&C Com\u2019l) for all Ohio Interstate, US and State highway system routes.\nhttp://www.dot.state.oh.us/Divisions/Planning/TechServ/traffic/Pages/Traffic-Count-Reports-and-Maps.aspx\nOregon Dept of Transportation\nThe Transportation Systems Monitoring (TSM) Unit has the mission to formulate a system to collect and process traffic related data on Oregon\u00b4s Highways. TSM provides traffic volumes, flow maps, trends, manual counts and vehicle class on state highways to Federal, State, Local, private and public constituents.\nhttp://www.oregon.gov/ODOT/td/tdata/Pages/tsm/tvt.aspx\nSouth Caroline Dept. of TransportationThe Traffic Polling and Analysis System provides traffic data and graphs from traffic counting devices on South Carolina's Highways. This system is intended for the general public to view current and historical traffic information during hurricane evacuation events\n**\nhttp://www.scdot.org/getting/trafficcounts.aspx\nTennessee Dept of Transportation\nAn AADT traffic volume is used throughout the project planning process to provide projected volumes of traffic.  It is based on a 24 hour, two directional count at a given location.  This raw traffic volume is then mathematically adjusted for vehicle type, determined by an axle correction factor.  Then this volume is statistically corrected by a seasonal variation factor that considers time of the year and day of the week. \nhttp://www.tdot.state.tn.us/projectplanning/adt.asp\nWashington State Dept of Transportation\nThe Annual Traffic Report (ATR) summarizes traffic data maintained by the Washington State Department of Transportation for the State Highway System. The report includes Annual Average Daily Traffic (AADT) figures and truck percentages, when available, for locations where data collection has occurred within the past four years.\nhttp://www.wsdot.wa.gov/mapsdata/travel/annualtrafficreport.htm\nWisconsin Dept of Transportation\nWisconsin Department of Transportation (WisDOT) traffic counts are now part of an interactive map that allows you to view counts anywhere in the state.\nhttp://www.dot.wisconsin.gov/travel/counts/\nChicago, IL TransportationAverage Daily Traffic (ADT) counts are analogous to a census count of vehicles on city streets.\n** \nhttps://data.cityofchicago.org/Transportation/Average-Daily-Traffic-Counts/pfsx-4n4m\nArlington, VA TransportationArlington County Regular Traffic Count Data is available in Portable Document Format (PDF).\n** \nhttp://www.arlingtonva.us/Departments/EnvironmentalServices/dot/traffic/counts/EnvironmentalServicesCounts.aspx\nDelaware Valley, PA Delaware Valley Regional Planning Commission\nhttp://www.dvrpc.org/webmaps/trafficcounts/\nBritish Columbia, Canada Ministry of Transportation\nThe Ministry of Transportation and Infrastructure's Traffic Data Program monitors traffic volumes at several locations throughout the province. This information is used by ministry staff to help support planning, design, construction, and operation of the Ministry road network. \nhttp://www.th.gov.bc.ca/trafficData/\nUnited Kingdom (UK) Dept of Transportation\nTraffic counts provides street-level traffic data for every junction-to-junction link on the 'A' road and motorway network in Great Britain.\nhttp://www.dft.gov.uk/traffic-counts/\nNew South Wales, Australia Roads and Maritime\nTraffic Volume Data books, sorted by Roads and Maritime Services regions, containing the Annual Average Daily Traffic (AADT) volumes for various roads. \nhttp://www.rms.nsw.gov.au/trafficinformation/downloads/aadtdata_dl1.html\n", "internet - What is the amount of open data that is currently available over the web?": "\nSome projects such as the Open Data Monitor aim to answer this question. For convenience, I split the answer into four sections.\nGovernments\nThe amount of open data released by governments is reasonably documented. Perhaps not in the sense of the exact amount of Gb.\nExamples are:\n\nThe Open Data Index by the Open Knowledge Foundation\nThe Open Data Barometer by the The Web Foundation & The Open Data Institute.\nCatalogues of datasets such as datacatalogs.org or the World Bank.\n\nBusinesses\nThis is much harder to quantify. Indicators are:\n\nThe GovLab are running a survey, the Open Data 500, analysing companies.\nData about businesses is published with a dual-licence by OpenCorporates. At the moment the database contains over 60 million entities.\nMany businesses publish data, often via an API. However, in most cases not as open data. An example is Twitter, where the licence and availability of the \"hose\" is substantially limited.\n\nInstitutions\nInstitutions are publishers of some of the largest datasets on the web. For example:\n\nThe 1,000 Genome project is one of the largest open datasets with an extreme size of 260Tb.\nThe Tiny Images dataset consists of almost 80 million images stored in the form of large binary files. They are used in an academic paper on scene recognition from 2008.\nResearchers at the Measurement Lab (M-Lab), who publish over 750Tb of data under a CCZero licence.\n\nIndividuals\nThe amount of personal data that individuals make available on the web is, to the best of my knowledge, negligible.\n", "data request - Full list of LinkedIn endorsements": "\nThe only thing that is available right now on Linkedin is the skills from a certain user and not the endorsements. And in order to download the skills, users have to log in with auth. Unfortunately, you will not find any complete list of skills from Linkedin.\nIf you want to download the skills from a certain user, here is the code for python using this library.\nfrom linkedin import linkedin\n\nAPI_KEY = '.............'     # This is api_key\nAPI_SECRET = '...........'   # This is secret_key\n\nRETURN_URL = '.............'\n\nauthentication = linkedin.LinkedInAuthentication(API_KEY, API_SECRET, RETURN_URL, linkedin.PERMISSIONS.enums.values())\n\nauthentication.authorization_code = code\n\nauthentication.get_access_token()\n\napplication = linkedin.LinkedInApplication(authentication)\n\nskills = application.get_profile(selectors=['skills'])\n\n", "data request - Intelligence, Internet usage and Ip addresses": "\nGoing by country is pretty coarse. But as indicated in my earlier comment, I would start with the (coarse) database showing the distribution of IP addresses to countries/cities: http://dev.maxmind.com/geoip/legacy/geolite.\nI would then use the World Bank datasets on economic related data per country, such as:\n\nGDP\nEconomic Indicators\nGender Statistics\nEducation Statistics\nGNI Gross National Income per capita\nHealth and Nutrition Indicators\n\nhttp://datacatalog.worldbank.org/ \n", "data request - \"Official\" list of business branches": "\nYou are looking for SIC codes (e.g., Used by Bureau of Labor) and NAICS codes (e.g., Used by US Census). The NAICS is gradually replacing the older SIC system. It is updated every 5 years and is developed/maintenance in conjunction with Canada and Mexico (hence the name North American Industry Classification Standard). One good place to start is at this US Census link:\nhttps://www.census.gov/eos/www/naics/ \nThis link has some downloadable definitions:\nhttps://www.census.gov/cgi-bin/sssd/naics/naicsrch?chart=2012\nThis is the BLS link describing how they have moved away from the SIC system to NAICS:\nhttp://www.bls.gov/bls/naics.htm\nDutch SBI System (thanks @Martjin):\nhttp://unstats.un.org/unsd/cr/ctryreg/ctrydetail.asp?id=1182\nEuropean NACE System (thanks @Martjin):\nhttp://epp.eurostat.ec.europa.eu/statistics_explained/index.php/Glossary:Statistical_classification_of_economic_activities_in_the_European_Community_(NACE)\nUnited Nations ISIC (thanks @Martjin):\nhttps://unstats.un.org/unsd/cr/registry/regcst.asp?Cl=27\n", "data request - publicly available spam dataset of social networks": "\nFrom the Machine Learning Repository at UCI there are some datasets that may get you started. \n\nemails\nSMS\n\nFor emails you may also find this StackOverflow discussion helpful, or this 300+MB dataset.\nFor blogs marked as spam, check out this site.\nTwitter data are not allowed to be publically shared. You can share the analysis or the visualization, but not the raw data of tweets. I imagine Facebook has a similar policy.\nBut, you can collect more than you ever dreamed of by using the public live stream. Here is an example python code to collect many, many tweets as they are sent. There should be plenty of spam if you let it run for a couple minutes.\n", "data request - Clickstream sample dataset": "\nDataset 1: \nWikipedia Releases Clickstream Data\nWikipedia has released a data set of clickstream data for January 2015. A clickstream is the path a user requests to get to a desired web page or article by using a referer\u2014clicking on a link or performing a search. The dataset contains 22 million referer-article pairs from the English language, desktop version of Wikipedia\u2014just a sample of the 4 billion total requests made in January. Clickstream data is a valuable analytical tool as it can determine things like the most popular links in a web page and how users navigate through a website.\nDownload data from: https://figshare.com/articles/Wikipedia_Clickstream/1305770\nDataset 2:\nVisualize Website Clickstream Data\nThese website log files contain data elements such as a date and time stamp, the visitor\u2019s IP address, the destination URLs of the pages visited, and a user ID that uniquely identifies the website visitor.\nDownload data from : https://s3.amazonaws.com/hw-sandbox/tutorial8/RefineDemoData.zip\nDataset 3: \nSample Clickstream Data\nDownload data from : https://gist.github.com/matthayes/4614332\n", "geospatial - APIs for oil and gas well location/production data?": "\nSome of the States provides datasets for oil and gas well locations:\nIllinois (ArcGIS, Shapefiles)\nhttp://certmapper.cr.usgs.gov/data/noga00/natl/spatial/doc/ilcells06g.htm\nLouisiana (XML)\nhttp://lagic.lsu.edu/data/losco/oil_gas_wells_ldnr_2007_faq.html\nNew York (CSV download)\nhttp://www.dec.ny.gov/energy/1603.html\nNevada (University of Nevada, Reno - 2011)\nhttp://www.nbmg.unr.edu/Oil&Gas/NVWellInfo.html\nTexas Railroad Commission (charge ~$20/dataset)\nhttp://www.rrc.state.tx.us/data/datasets/WellData.php\nUtah\nhttp://gis.utah.gov/data/energy/oil-gas/\nProduction Summaries by State:\nDistribution and Production of Oil and Gas Wells by State\nhttps://explore.data.gov/download/dwne-h7vw/XLS\nArcGIS free dataset (CC-BY) for Alaska 2008\nhttp://www.arcgis.com/home/item.html?id=ccfcd4a89a234275a18408b9d7fe9fe0\nUK on/off shore oil wells/rigs (MS-Excel)\nhttps://www.gov.uk/oil-and-gas-digital-data-exchange-format\nThere are also commercial providers (paid), for example:\nPetroView http://www.psg.deloitte.com/ProductsPetroviewData.asp\n", "api - Climate data from stations in Canada": "\nThe CKAN API is documented here: http://docs.ckan.org/en/latest/api/index.html.\n", "Any Open Data Sets for the (Football) World Cup (in Brazil 2014)?": "\nEdit: A new initiative on the web open_data_world_cup\nEdit 2: Gerald also started collecting a list of other football projects on GitHub.\nEdit 3: Here is a list of the football players ages in a spreadsheet format. \nYou can create data by putting the information from the wikipedia page into a more structured format.\nThere are plenty of tools available that make scraping easier. A starting point might be ScraperWiki, but it depends a bit on your interest and skills.\nHere's a quick proof-of-concept with GoogleDocs' ImportHTML function.\n\n", "api - Using Open Graph to see what comments are being made": "\nAs I said before, I made a small research about your question. Here is what I've found.\nYour url give me this response:\nURL: http://graph.facebook.com/?id=http://thanetstar.com/article/what-s-great-about-thanet-2\nResponse: {\n   \"id\": \"http://thanetstar.com/article/what-s-great-about-thanet-2\",\n   \"shares\": 19\n}\n\nBut if I use the same call for another website, I get this response\nURL: LINK\n{\n   \"http://www.hollywoodreporter.com/live-feed/scandal-shonda-rhimes-kerry-washington-olivia-pope-308845\": {\n      \"id\": \"http://www.hollywoodreporter.com/live-feed/scandal-shonda-rhimes-kerry-washington-olivia-pope-308845\",\n      \"shares\": 67,\n      \"comments\": 2\n   }\n}\n\nYou can see that in your link, even if you have comments in the site, response doesn't include them. I don't know why. Maybe you haven't used the OG meta tags with the right way.\nAlso, I found that this link will return the comments from a OG object:\nURL: https://graph.facebook.com/comments/?ids=http://thanetstar.com/article/what-s-great-about-thanet-2\nMy source: Quora\n", "computing - Is there any public domain Controller Area Network trace data from a real vehicle?": "\nI found this research paper and in the abstract, the authors write that they use trace data from CAN. You should contact them and ask them to share the data with you.\nHere is the research paper.\n", "Database with monthly climate/weather data by country": "\nThe UNDP gathers and reports data along the line of what you are looking for. I think its a matter of finding the right tables. They collect historic data as well as predicating climate changes to countries.\nThe following link is a repository of some reports and raw data for 61 countries, mostly africa, middle east and south america. Each country has a zip file (AllData). Search through the subfolders looking for time series.\nhttp://www.geog.ox.ac.uk/research/climate/projects/undp-cp/\nNote, this copy of the data is hosted by University of Oxford, Department of Geography and the environment. The time series are on a year basis (no monthly) and range from 1960 to 2006.\n", "data request - Large French dataset for NLP (not formal, rather discussions/reviews)": "\nI've been thinking about this question a lot and I have another solution. You correctly wrote that Wikipedia articles have too much quality due to the editing.\nBut, the discussion (talk) pages are full of raw text data and are not prone to being edited, or even correct.\nAn example for the French language Open Data page (source): \nY a-t-il une raison pour laquelle le terme Open Data est utilis\u00e9 partout dans l'article alors qu'il a pour titre Donn\u00e9es ouvertes ? Je propose d'\u00eatre uniforme dans tout l'article et d'opter pour la langue de ce wiki, c'est-\u00e0-dire le fran\u00e7ais. \u00c0 moins d'une opposition claire, je vais proc\u00e9der d'ici quelques jours. Dirac (d) 18 f\u00e9vrier 2013 \u00e0 19:48 (CET)\n\nIt's straightforward to download all the Discussion/Talk pages not only for articles, but also for Users, Files, Help, etc.\nDownload:\n\nWikipedia has a (massive) data dump at dumps.wikimedia.org (details). Be careful, the files are huge and won't open with most text viewers. It's best to have a programming approach for parsing the data.\n\nWhat's available?\nEnglish latest, French latest.\n\nThe specific files you want are labeleled with something like enwiki-latest-pages-meta-current1.xml-p000000010p000010000.bz2. The key part is pages-meta and then some sequence for the individual files of the split data.\n\nParse:\n\nThe header of these XML files contains these (selected) keys:\n<namespace key=\"1\" case=\"first-letter\">Talk</namespace>\n<namespace key=\"3\" case=\"first-letter\">User talk</namespace>\n<namespace key=\"5\" case=\"first-letter\">Wikipedia talk</namespace>\n<namespace key=\"7\" case=\"first-letter\">File talk</namespace>\n<namespace key=\"9\" case=\"first-letter\">MediaWiki talk</namespace>\n<namespace key=\"11\" case=\"first-letter\">Template talk</namespace>\n<namespace key=\"13\" case=\"first-letter\">Help talk</namespace>\n<namespace key=\"15\" case=\"first-letter\">Category talk</namespace>\n<namespace key=\"101\" case=\"first-letter\">Portal talk</namespace>\n<namespace key=\"109\" case=\"first-letter\">Book talk</namespace>\n<namespace key=\"119\" case=\"first-letter\">Draft talk</namespace>\n<namespace key=\"447\" case=\"first-letter\">Education Program talk</namespace>\n<namespace key=\"711\" case=\"first-letter\">TimedText talk</namespace>\n<namespace key=\"829\" case=\"first-letter\">Module talk</namespace>\n\nWikimedia's guide to parsing the XML files, a guide to SQL import, and tutorial about how to parse with python: link and source.\n\n", "data request - Cynthia Wessell's Dictionary of Affect": "\nWell, you picked a very commercial word list so even 'finding' a copy of the dataset could lead to legal problems or results you can't publish.\nBut, to get a taste, there is a web-app that I found after chasing down some broken links. It's apparently endorsed by Cynthia herself. Check it out here.\nNow, interestingly, the creator of that site posts a link to more info. \nAnd, in that more info section is a URL that may help you greatly:\ndead link http://compling.org/cgi-bin/DAL_sentence_xml.cgi?sentence=This+is+a+great+web+app\nreplacement? https://sail.usc.edu/dal_app.php\nSo, there you have it, a web interface to the DAL without having the DAL itself. For a sentence in the URL with words separated by a '+' sign, you get in return XML:\n<sentence>\n <word>\n  <token>This</token>\n   <emotion>\n    <measure type=\"DAL\" valence=\"1.7500\" activation=\"1.3333\" imagery=\"1.0\"/>\n   </emotion>\n </word>\n <word>\n  <token>is</token>\n   <emotion>\n    <measure type=\"DAL\" valence=\"1.8889\" activation=\"1.1818\" imagery=\"1.0\"/>   \n   </emotion>\n </word>\n <word>\n  <token>a</token>\n   <emotion>\n    <measure type=\"DAL\" valence=\"2.0000\" activation=\"1.3846\" imagery=\"1.0\"/>\n   </emotion>\n </word>\n <word>\n  <token>great</token>\n   <emotion><measure type=\"DAL\" valence=\"2.6250\" activation=\"2.1250\" imagery=\"1.0\"/>\n   </emotion>\n </word>\n <word>\n  <token>web</token>\n   <emotion>\n    <measure type=\"DAL\" valence=\"1.7778\" activation=\"1.8750\" imagery=\"2.8\"/>\n   </emotion>\n  </word>\n  <word>\n   <token>app</token>\n    <emotion>\n     <measure type=\"DAL\" valence=\"\" activation=\"\" imagery=\"\"/>\n    </emotion>\n   </word>\n  </sentence>\n\n", "data request - Police car chase dataset": "\nSuffolk Police Department's annual reports document most of this data, but only for a couple of years. pretty sure they have it for 2006-2008 or 2009. you'll have to check, but the data is there:\nhttp://www.suffolkva.us/spd/inside/annualreports/\n", "best practice - Which namespace should be used for an open vocabulary published by a private organization?": "\nI would use purl.org yes. That's weird that they're not answering. They answered pretty quickly when I tried myself. There's also https://w3id.org/ if you're looking for stable and persistent identifiers. \nThe best however could also simply be to publish the vocabulary yourself. One of the issues with using URIs for the vocabulary of your data is that there's so many to choose from already: http://vocab.cc/\nInstead of creating yet another vocabulary make sure the concepts you're trying to identify don't already exist. Also http://lov.okfn.org/dataset/lov/ is a good place to search for existing vocabularies.\nIf you feel you need to publish new URIs, then try to go with something persistent unless it's so specific for your use-case, then you should perhaps use your company's domain name.\n", "data request - Analyst Estimates for Earnings on the S&P 500": "\nI am not familiar with this crowd-sourced project, but they claim to be crowd sourcing estimated earnings:\nhttps://www.estimize.com/\nTheir tagline is: \nThe Most Comprehensive Earnings Forecasts\nCrowdsourced estimates from over 4,500 hedge fund, brokerage, and independent analysts\nAccording to Crunchbase, they are Venture funded.\nFunding Received $2.6 Million in 3 Rounds from 8 InvestorsMost\nRecent Funding$1.2 Million Series A on March 26, 2014\nHeadquarters:New York, NY\nDescription:Estimize is an open financial estimates platform which facilitates the aggregation of fundamental estimates from independent analysts.\nFounders:Matthew Jording, Leigh Drogen\nCategories:Crowdsourcing, \n", "historical - Open Data: Dataset containing records of events throughout history?": "\nAn application that uses a dataset like this one is Day Like Today from the OKFN Greek chapter. It makes use of DBpedia and retrieves information from Wikipedia infoboxes. For example dates of wars, deaths of popular persons etc.\nThus, I do not know a database that you can download and use, but you can use DBpedia to retrieve historical events from Wikipedia.\n", "usa - Seattle City Buildings Data Set": "\nIn addition to the building permit database, Seattle.gov provides a dataset for historic buildings at:\nhttps://data.seattle.gov/browse?tags=historic+register\nAny building in Seattle that is a public housing or multi-family housing inspected for public housing assistance program will have info in this HUD dataset:\nhttp://www.huduser.org/portal/datasets/pis.html\nThe footprints and building heights are commercially available ($1571) at:\nhttp://market.weogeo.com/datasets/nokia-here-buildings-seattle-wa-metro-region.html\nOther than that, the best way to get this information free (or near FREE) is to go to the property tax office for King County and see if they provide a electronic copy (e.g., like on a CD) for a fee.\nUPDATE: I found some more links on building lot outline in major cities open data. This comes from stackexchange question: https://gis.stackexchange.com/questions/2046/where-can-i-find-building-footprint-data/2050#2050\nChicago: http://data.cityofchicago.org/Government/Boundaries-Buildings/w2v3-isjw\nSeattle: http://data.seattle.gov/dataset/2009-Building-Outlines/y7u8-vad7\nBellingham: http://www.cob.org/services/maps/gis/index.aspx\nBloomington: http://bloomington.in.gov/documents/viewDocument.php?document_id=1870\nSpokane: http://www.spokanecity.org/services/gis/data/\nKitsap County: http://www.kitsapgov.com/gis/metadata/\nThere is also some additional data for several cities across the U.S. and federal data on building permits from Data.gov.\n", "data.gov - Looking for prior work parsing NCDC's Integrated Surface Data": "\nNeal Lott of NOAA/NCDC  is familiar with Integrated Surface Data.   He can be reached at Neal.Lott@noaa.gov \nPeter Grimm\nNOAA/NESDIS\nSilver Spring, MD\nPeter.L.Grimm@noaa.gov \n", "usa - Census data at every block level in US cities": "\nBlock level data is only available from the Decennial Census, which means that you can only get fundamental demographic and housing data. This amounts to age, sex, race, household/family structure, home ownership and home vacancy rates. Some Decennial Census tables are only tabulated at the census tract level and higher, because of restrictions based on releasing data which can personally identify respondents\nQuestions about poverty, income, education, etc are now collected in the American Community Survey, which only tabulates data down to the block group level. Furthermore, because it's a survey and not a complete count, for many estimates at the block group level there is a high margin of error.\n", "data request - A database of drugs and their targets": "\nTry the following:\nDrugBank: http://www.drugbank.ca/ \nopenFDA API to get some data online:   http://open.fda.gov/\n", "data request - VAT Rates of EU or better all Countries": "\nYou can find this information in the OECD Tax Database. The rates are provided in an Excel sheet.\nUnfortunately the table is confined to 33 OECD member states, among which 21 are also member of the EU.\n", "data request - Language Similarity Heuristic": "\nCheck out the World Atlas of Language Structures.  http://wals.info/\n", "data request - Download a sample database of calls in telecommunication": "\nHere's a CDR sample:\nMSIDN:IMSI:IMEI:PLAN:CALL_TYPE:CORRESP_TYPE:CORRESP_ISDN:DURATION:TIME:DATE \n068373748102;208100167682477;351905149071;PLAN1;MOC;CUST1;0612287077;247;12:07:12;01/01/2012 \n068373748102;208100167682477;351905149071;PLAN1;MTC;CUST2;0600000001;300;12:15:09;01/01/2012 \n068373748102;208100167682477;351905149071;PLAN1;SMS-MO;CUST1;0613637193;0;12:18:18;01/01/2012 \n068373748102;208100167682477;351905149071;PLAN1;SMS-MT;CUST1;0612899062;0;12:21:07;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;MOC;CUST1;0612283725;90;12:00:00;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;MOC;CUST1;0613069656;82;12:02:27;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;MOC;CUST1;0613481951;78;12:04:41;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;MTC;CUST2;0600000001;92;12:07:13;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;MTC;CUST2;0600000002;94;12:09:40;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;MTC;CUST1;0612063352;114;12:12:40;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;SMS-MO;CUST1;0613103364;0;12:13:42;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;SMS-MO;CUST1;0613751973;0;12:14:44;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;SMS-MO;CUST1;0613672843;0;12:15:44;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;SMS-MT;CUST1;0612769488;0;12:16:42;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;SMS-MT;CUST1;0613164676;0;12:17:39;01/01/2012 \n065978198280;208100310191699;356008289837;PLAN3;SMS-MT;CUST1;0613399901;0;12:18:39;01/01/2012 \n067599860569;208120276653317;353297808290;PLAN2;MOC;CUST1;0612089847;116;12:00:00;01/01/2012 \n\n", "data request - Where can I find a large list of English books published in the last 50 years?": "\nThere are lots of options, though Project Gutenberg is probably not one of them because of the time range. You could try using LibraryThing, Google Books (orderBy=newest), Goodreads or Amazon APIs to run search queries. Or try to get access to OCLC WorldCat or Ingram OASIS - talk to your friendly local librarian.\n", "geospatial - Looking for geo localization data of golf courses": "\nYou could start with the 25000+ objects tagged with leisure=golf_course on OpenStreetMap. To do this, you could extract the geolocations with GIS tools, or create a custom map like they did with bicycle tags. OSM data is ODbL licensed.\n", "data request - Searching a proxy for monthly GDP": "\nThe UNSD's Joint Oil Data Initiative (JODI) contains oil production, export/import and consumption on a monthly basis for all the countries in the world since 2002. I have a tutorial here on using the dataset:\nhttp://opengeocode.org/tutorials/UNSD.php\nThe UNSD also publishes annually a very comprehensive energy statistics yearbook for 215 countries (currently). \n'The 2010 Energy Statistics Yearbook is the fifty-fourth issue in a series of annual compilations of internationally comparable statistics summarizing world energy trends. which commenced under the title World Energy Supplies in Selected Years, 1929-1950. Annual data for 215 countries and areas for the period 2007 to 2010 are presented on production, trade and consumption of energy for solid, liquid, and gaseous fuels, electricity, and heat. Per capita consumption series are also provided for all energy products. Graphs are included to illustrate historic trends and/or changes in composition of production and/or consumption of major energy products. Special tables of interest include: international trade tables for coal, crude petroleum and natural gas by partner countries; selected series of statistics on renewables and wastes; refinery distillation capacity; and a table on selected energy resources. \"\nhttp://unstats.un.org/unsd/energy/yearbook/default.htm\n", "education - Collection of messy data": "\nI collected some answer from friends on twitter, they suggested:\n\nMG-RAST: http://metagenomics.anl.gov/\nInteraction web database: http://www.nceas.ucsb.edu/interactionweb/ - just a collection of csv files basically, same general format but all from diff researchers\nInteresting dataset of 2127 articles that the Wellcome trust paid for in Europe for open access article fees to publishers\n\nlots of messy data see this blog post: http://biomickwatson.wordpress.com/2014/03/25/biologists-this-is-why-bioinformaticians-hate-you/\nthe data http://figshare.com/articles/Wellcome_Trust_APC_spend_2012_13_data_file/963054\n\n\n", "products - Where can I find data from released computer models?": "\nI think in your search you should use either the term \"specifications\" or \"specs\", as in \"computer specs\". \"Information\" is too vague and in the jargon of technology specs/specifications is the way to go.\nTo get you started, I would suggest looking for non-commercial groups that are collecting data on hardware. For example, the groups doing the Energy Star rating, groups collecting data for import and selling purposes, groups conducting safety tests, etc.\nOne quick example: As part of the old EU Energy Star program, there is an Excel file with the type of specs you are looking for. Unfortunately, the manufactorers are masked. But it's a start.\n\nOverview\nAll Downloads\nComputers version 5.0 - Final - Masked Dataset [XLS]\n\nIf there is any newer data, you'll have to register: link.\n", "data request - Dataset of personal names": "\nI'm going to expand on the comment of @Joe above because there isn't room to add too much in the comments.\nIMDB offers textfile datasets with enormous amounts of data (details). Choosing one site at random, you see a list of the files (link). Choose biographies.list.gz and uncompress. The biographies.list file has sections incuding:\n\nNM: 'K', Murray the\nRN: Murray Kaufman\nNK: The Fifth Beatle\nDB: 14 February 1922, New York City, New York, USA\nDD: 21 February 1982, Los Angeles, California, USA (cancer)\nBG: Murray the K was born Murray Kaufman in New York, New York, on 14... \n  (abbreviated)\nSP: * 'Jacklyn Zeman' (qv) (14 February 1979 - 1981) (divorced)\nTR: * Legendary disk jockey who made his name at WINS (New York) in the 1950s... (abbreviated)\n\nSome categories are easy to decode (BG = background). Here is the full list of possible categories, with some annotation based on my skimming. It seems there are about 500k unique people included. The data is a little dirty, but you can surely get names, nationalities, countries, ages, genders, marital and family status after a little text mining.\n\n2050717 BG: (biography)\n1438983 TR: (trivia)\n585662 NM: (name)\n560287 OW:\n373177 DB: (date of birth)\n329587 QU: (quotes)\n155847 HT: (height)\n138481 RN: (real name)\n132678 AT:\n130380 SP: (spouse)\n119293 DD: (date of death)\n96901 BY: (biography written by)\n74378 CV:\n66683 NK: (nickname)\n61704 PT:\n52134 IT:\n29016 BO:\n27863 TM:\n11012 PI:\n6348 SA:\n6042 BT:\n1 WN:\n\nOverview stats via some awk/grep/sort/uniq/sort:\nawk '{print $1}' biographies.list | grep -v '^$' | sort | uniq -c | sort -rn -k1\n\n\nUpdate: Instead of downloading the file and parsing it, you can use one of the unofficial APIs: discussion.\n\none\ntwo\nthree\n\n", "usa - What do the summary files in the ACS FTP drive mean?": "\nTo answer your question very specifically: here is the technical documentation for the ACS2012-1yr summary files: http://www2.census.gov/acs2012_1yr/summaryfile/ACS_2012_SF_Tech_Doc.pdf\nBut, the comments on your question provide sage advice: if you don't know why you need the summary files, you may be better off using the American Fact Finder. \nAlternatively, I've been part of a project called Census Reporter, which aims to make ACS data easier for journalists to use. We hope that it's useful to non-journalists as well. Here's our profile page for New Orleans You can also get bulk data for different tables. We're gradually working on some help documents that try to put census concepts into clearer, if sometimes less technically precise language.\nIf you do want to go deeper with the summary files, the first question is \"why do you want data for both the 1-year and the 3-year\"?  They are not meant to be compared to each other. If you want to compare data for New Orleans this year and three years ago, use the 2012 and the 2009 1-year data sets.  If you want data for different areas inside New Orleans, you'll need to use the 5-year dataset to find census tracts or block groups inside the city.\nThe root of the Census Bureau's ACS documentation is at http://www.census.gov/acs/www/\nJust to get a general idea of what goes on in the ACS data, I found the ACS Handbooks for Data Users quite helpful, and there are several versions each slightly tailored to a different audience. Coming from a journalism background, I was impressed that the \"media\" version included quotes from several respected expert census journalists.\n", "data request - Coding homework dataset": "\nIncluding to other answers, here is one that may help to create a database.\n\nProject Euler is a series of challenging mathematical/computer\n  programming problems that will require more than just mathematical\n  insights to solve. Although mathematics will help you arrive at\n  elegant and efficient methods, the use of a computer and programming\n  skills will be required to solve most problems.\n\nAs the about section said, project Euler is a collection of many mathematical problems and users try to solve them one by one with programming languages. Unfortunately, there isn't a limitation on the programming language you have to use. But if someone track them and collect only those that want, it would create a nice database.\n", "linked data - Why isn't RDF more popular within the private sector?": "\nWhile there is clear power with RDF and other formal ontologies, web technologies are showing a tendency towards simplicity -- things that are easy to code, read, manipulate, etc. RDF has none of those qualities. So while a language like Ruby might evolve on its own, it gains more power, popularity and community when a platform that makes development more simple (like Rails) starts to use it.\n", "usa - Data mining for US demographics": "\nYou can find demographic data on Metropolitan Statistical Areas through American FactFinder. Depending on how accurate, timely, or complete you want your data to be, you can decide between 1-Year averaged estimates, 3-Year averaged estimates, or 5-Year averaged estimates. The 1-Year averaged estimates cover areas with populations over 65,000 (with an average sampling rate of 1.5% of the population); The 3-Year averaged estimates covers areas with populations over 20,000 (with an average sampling rate of 4.5% of the population; and the 5-Year averaged estimates will have the largest sample with an average sampling rate of 7.5% of the population.\n", "usa - Where can I find data by \"census tract numbers\"": "\nI think I know what is going on with the weird Census Tract numbers you are getting. Census tract numbers come in many forms. However, the number you are shown in the Seattle city violent crimes appears to be both the tract number and the block number (e.g. \"4700.4003\"). I am guessing that the first number prior to the period is the tract number, which is expressible in its GEOID form as \"004700\", which also shows up as \"Census Tract 47\", or tract number 47.00. It is inherently 6 digits long, although the last 2 digits can be thought of as decimals and the leftmost numbers may disappear because they are leading zeroes. The numbers after the period are 4 digits long, because block numbers are always 4 digits long no matter what.\nGiven this information, I would forget about worrying what the Census tract number is. If you have the latitude and longitude I would suggest taking a different approach to matching the locations to areas with demographic data.\nMake an account with NHGIS.org (They are a free publicly-provided service) and download the latest data from the 2008-2012 ACS along with corresponding shapefiles to go with it. With these shapefiles in your hands, and the lat/long coordinates of the crime incidents, you may be able to perform some type of intersect analysis to find which Census tract each crime was located in. If you are so inclined, you can do this instead by Census block group which is a smaller geographic division than Census tract, but may suffer from a larger degree of statistical error than Census tracts.\nI hope this was helpful.\n", "usa - Archive of ZIP code changes": "\nI've never used data that old for addressing. One place I would start is with the US Census 1970. It used real zip codes back then (vs. ZCTA), but it is before the census published tigerline shapefiles. So without the geographic shape, I am not sure how you can solve the problem. I did some googling the census archive. I think this is where the census data for this is:\n\nCensus of Population and Housing, 1970, Fifth Count File 5B National\n  Archives Identifier: 594232 Data Files: 10 (one per ZIP Code Area,\n  based on first digit of ZIP code) Geographic Areas: 12,500 ZIP code\n  areas for Standard Metropolitan Statistical Areas (SMSAs) Technical\n  Documentation: 44 pages and 82 supplemental pages\n\nI would google to see what \"commercial\" providers who have converted data from archives into shapefiles.\n", "usa - USDA Plant Hardiness Zone Map (PHZM) data": "\nThere's an Open Plant Hardiness Zones (OPHZ) project on Github where various people have reverse-engineered a pdf of the hardiness zones (pdf, really large) to produce a GIS file (SHP) of the zone boundaries.  The ophz-c version is the latest. It's public domain.\nYou could use GIS software or tools, and a list of the center point of each zip, to find the hardiness zone for each zip. (you're no doubt aware there are no firm boundaries for each ZIP, so this would just be an approximate answer, the latitude and longitude of a street address would be better). \nDoesn't even have to be GIS software. You could convert the SHP file to geojson if needed, import it into a geo-aware database such as MongoDB or Postgres, import the zip code list as well, and use the database GIS queries to give you \"zip is within .. zone\" answers.\nProbably a more complex solution than you'd like.  Maybe somebody has done these steps already and released the results, but I'm not aware of it.\n", "computing - Where to find key log data for keyboard usage?": "\nThere are a few academic papers that have used keylogger data set. An example from \n\"A Case-Study of Keyloggers and Dropzones\" (PDF link):\n\nWe found more than 33 GB of keylogger data, containing stolen information from more than 173,000 victims.\n\nFor security reasons, the authors gave the data to AusCERT (Australia\u2019s National Computer Emergency Response Team), and it's mostly impossible that this data set is ever released. But the paper is very interesting in its explanation of how they collected the keylogger data.\n\nAnother paper, \"A Metric for the Evaluation and Comparison of Keylogger Performance\" (PDF link): \n\n(The authors have) developed a framework to assess the performance of a keylogger. This paper provides the documentation on how such a study can be conducted, while the required source code is shared online.\n\nThe analysis code (python) and keylogger data (sql database format) is available as a download from the Security in Telecommunications group at TU Berlin. The data is from tablets and smartphones.\n\nThe toolkit also holds the test-data from the performed use-case study and a demo keylogger.\n\nThe git page is here, and the direct download link is here. The data files are called KeyStrokesExperiment.sql and KeyStrokesRealExperiment.sql in the \"data\" folder.\n", "How much data is available on the Internet?": "\nHave a look at the Measurement Lab.\nOne estimate I've heard is that researchers at the Measurement Lab (M-Lab) publish over 750Tb of data under a CCZero licence.\n", "licensing - What open data about tunes and melodies exist? Do copyrights apply here?": "\nIt seems like http://echoprint.me/ is the service you want:\n\nEchoprint is a music fingerprint or music identification service. It\n  listens to music signals and tells you what song is playing. It\u2019s\n  backed by a huge database of music that grows with the community and\n  further partnerships. On launch we\u2019ve partnered with Musicbrainz. \n...\nDoes it work \u201cover the air\u201d, identifying songs over a microphone?\nYes - Echoprint has been designed from the ground up for OTA, and our\n  informal tests have demonstrated many successful and promising results\n  for this scenario. The system still needs a little more tuning,\n  however, and is under continued development to further improve\n  accuracy and performance.\n\nData license (at http://echoprint.me/data):\n\nEchoprint data (for ingestion into your own server) is available under the \u201cEchoprint Database License.\u201d The intent of the license is simple: Use our data for whatever you want (commercial or non, research, personal use); If you download our data and then add to it, you are required to contribute data back to us. ; There is a good reason for this. We want Echoprint to be able to resolve every song in the universe. If you add to the database of resolvable tracks, the Echoprint community needs to know about it.\n\n", "data request - Population movement datasets?": "\nIt's only a subset of population, but many bikesharing programs openly offer their data.\nOne example, Capital Bikeshare in Washington DC has a data page. This data will include geolocation of each bike with exact time, as well as status of each station and more. There was a recent presentation about using KNIME with the data (PDF link), and you may find their analysis an interesting start.\n\nOther cities: New York City, Chicago, Bay Area, etc...\nList of bike sharing programs (Wikipedia)\n\n", "usa - How to make a successful FOIA request?": "\nThe basics for FOIA are:\n\nbe very clear in your request\nbe respectful\nbe tenacious\n\nRemember that while there are government officials who are hostile to records requests, not all of them are. There are many people in government who want to do the right thing.\nThere are a few websites designed to help you file FOIA requests. In the US, Muck Rock has been around for several years, and FOIA Machine is another newer one. In preparing this answer, I also learned about iFOIA.org. There is also a project called Alaveteli, an open source project to make it easier to set up sites like these. Their GitHub wiki has a list of running sites around the world.\nOne way to have a hint about data systems that must exist is to consider forms that you already know are filed. You can use what you infer from the existence of the form and the fields it collects to explain what you are requesting, and to make your request more confidently. As mentioned in the other answer, requesting data schemes or other system documentation can also sometimes be helpful.\nIn response to Albert's answer: While there are always going to be cases where people in power work to contain information, there is a long history of important news being broken based on information obtained via FOIA. Here are just a few lists:\n\nNoteworthy News Stories made possible by FOIA\ndocuments\n(National Security Archive) \nThe FOIA Files: Stories that FOIA made\npossible\n(Sunshine in Government Initiative)\n\nBut also, in respect for Albert's point, I offer this tweet from ProPublica's Jeff Larson: \n\nDon\u2019t use \u2018related to\u2019 in your FOIA request. Just got:\n  \u2018all documents 'relate' to others in some remote fashion\u2019 as a reason for denial.\n\n", "data request - Is there a database that provides lengths of books?": "\nYou can get page counts from OpenLibrary and word counts from the linked editions on Internet Archive.  Of course the latter is only going to be for public domain editions and if you are focused on review sentiment (as a proxy for purchase desirability?), you are probably more interested in modern non-public domain editions.  The other drawback to IA word counts is that  you'll need to download the full text to get them, but if you pull down a compressed text-only file, it shouldn't be too bad.\nHere's the chain of links you need to follow:\n\nOL work page https://openlibrary.org/works/OL3686173W/Lorna_Doone\nOL edition https://openlibrary.org/books/OL13522117M/Lorna_Doone\nOL API https://openlibrary.org/books/OL13522117M.json\nIA page https://archive.org/details/blaclornadooneromanc00rich\nIA files https://ia600308.us.archive.org/9/items/blaclornadooneromanc00rich/\nIA text file https://ia600308.us.archive.org/9/items/blaclornadooneromanc00rich/blaclornadooneromanc00rich_djvu.txt\n\nCounting words is as simple as:\ncurl https://ia600308.us.archive.org/9/items/blaclornadooneromanc00rich/blaclornadooneromanc00rich_djvu.txt | wc\n  34410  280815 1488652\n\nSo this edition of Lorna Doone is 687 pages with (approximately) 280K words.  The word count is done over OCR'd text, so it will not be 100% accurate, but it should be close enough for this type of project.\n", "data request - A dataset of resumes": "\nindeed.com has a r\u00e9sum\u00e9 site (but unfortunately no API like the main job site). You can build URLs with search terms:\nhttp://www.indeed.com/resumes/data-science\n\nWith these HTML pages you can find individual CVs, i.e. link. You can search by country by using the same structure, just replace the .com domain with another (i.e. indeed.de/resumes)\nThe HTML for each CV is relatively easy to scrape, with human readable tags that describe the CV section:\n<div class=\"work_company\" >\n...\n<p class=\"work_description\">\n\nCheck out libraries like python's BeautifulSoup for scraping tools and techniques.\n", "data request - Looking for an UTF-8 table": "\nWith python you can lookup each unicode character by its integer code using unichr.\nimport sys\nwith open('unicode.csv','wb') as output:\n    for i in xrange(sys.maxunicode):\n        output.write(unicode(i))\n        output.write(u',')\n        output.write(unichr(i).encode('utf-8'))\n        output.write(u',')\n        output.write(unichr(i).encode('ascii', 'xmlcharrefreplace'))\n        output.write(u'\\n')\nprint sys.maxunicode\n\nThis gives you a file (unicode.csv) which has the unciode integer, unicode representation, unicode character, and HTML escaped character (for non-ascii).\nFor example, each line looks like this:\n64058,u'\\ufa3a',\ufa3a,&#64058;\n\nI put the code and the unicode.csv file on github for easier access.\nNote: Because the unicode character set includes newline characters, CSV is not really the best format. (See lines 10 to 13.) I also added a python code to generate a JSON file, which is more safe than CSV for storing unicode characters.\n", "government - Where can I find the 2014 release of Medicare data from 2012?": "\nIt looks like this might be the data:\nhttp://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html\n", "data request - Where can I find all cities, regions and cantons of Switzerland?": "\nFor detailed statistics, bfs.admin.ch is the official Swiss source of population stats and data. They have many English-language resources, but the one you are looking for is in German or French only. Excel files are easy to understand without speaking German or French.\nThe source page is here and the Excel file is here. It is called \"Bilanz der st\u00e4ndigen Wohnbev\u00f6lkerung nach Bezirken und Gemeinden\" or \"Bilan de la population r\u00e9sidante permanente selon les districts et les communes\".\nThe Excel file has the structure as follows 1. Kanton/Canton (i.e. Zurich), 2. Bezirk/District/Distretto (i.e. Affoltern), 3. Municipality/Gemeinde/Commune (i.e. Aeugst am Albis). Note that the municipality code is not the PLZ.\n- Z\u00fcrich\n>> Bezirk Affoltern\n......0001 Aeugst am Albis\n......0002 Affoltern am Albis\n......0003 Bonstetten\n......0004 Hausen am Albis\n......removed for clarity\n......0014 Wettswil am Albis\n>> Bezirk Andelfingen\n......0021 Adlikon\n......0022 Benken (ZH)\n\nFor basic lists of municipalities (cities) and their PLZ (zip code) and Canton (two-letter code), you can use the resources from post.ch. See the download link here and this particular file here. They also have a list of municipalities by PLZ and municipality number.\n\nIf you are looking for mapping data, the government swisstopo site offers tons of data. There is a GitHub repo called swiss-maps where you can find tools to create geoJSON and topoJSON files based on the swisstopo data.\n", "data request - List of public holidays by countries?": "\nJust came across Azure open dataset:\nhttps://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/\n\nWorldwide public holiday data sourced from PyPI holidays package and Wikipedia, covering 38 countries or regions from 1970 to 2099.\nEach row indicates the holiday info for a specific date, country or region, and whether most people have paid time off.\n\n", "sql - What is the best tool for doing explorative analysis of databases?": "\nlooks like possibly same question on stackoverflow:\nhttps://stackoverflow.com/questions/12835572/how-to-use-pure-sql-for-exploratory-data-analysis \nmost results i found recommend using R\nhttp://www.renzeconsulting.com/presentations/exploratory-data-analysis-with-r.pdf \nlooks like these two solutions cost:\nhttp://science.nature.nps.gov/im/units/arcn/documents/documents/nps-arcn-dmsop-2009-03_exploratory_data_analysis_using_sqlserverstudio_v1.0_20090210.pdf \nhttp://spie.org/Publications/Proceedings/Paper/10.1117/12.907097\n", "data request - Source for photographs of sample checks": "\nThis is an interesting question. Here is a non-direct path (assuming you are using Windows).\nTL;DR print hard copies with check font, scan/photograph them, use OCR.\nI used Python to write the few MICR characters in Unicode. I use the utf-16-le (little endian) encoding with BOM.\nimport codecs\nwith open('micr.txt','wb') as output:\n    output.write(codecs.BOM_UTF16)\n    for ichar in xrange(9280,9280+11):\n        line = ( u'1'+unichr(ichar)+u'\\n').encode('utf-16')\n        output.write(line)\n    output.write( (u'0123456789'+u'\\n').encode('utf-16') )\n\nThe output looks like this (I suggest using Notepad++ or another high-level text editor). It's required to have some character before each of the non-numeric MICR unicode characters, I just chose '1' for simplicity:\n1\u2440 \n1\u2441 \n1\u2442\n1\u2443\n1\u2444\n1\u2445 \n1\u2446\n1\u2447\n1\u2448\n1\u2449\n1\u244a\n0123456789\n\nThese are all the possible unicode characters for MICR. Of course, the font is wrong. (A simpler method would be to copy paste from here)\nThe MICR font costs money, unfortunately, but there is an open version: GnuMICR. You can download the font directly here: GnuMICR.ttf. To install, just double click and then restart and running programs where you want to use it.\nI then took my output text from above, and pasted it into LibreOffice Writer (other MS Word-like programs will work, too). Highlight all the text and change the font to GnuMICR. I've shared the LibreOffice Writer .ODT file here (you must have the GnuMICR font already installed).\nThe screenshot below shows what my screen looks like.\n\nFrom here, you can use the Python code or lots of copy/paste to generate unlimited routing and account numbers. Let me know if you want a python snippet to generate fake routing/account numbers.\nTo really test the OCR, generate your own MICR documents by physically printing your digital \"checks\" and then scan/photograph (re-digitize) them. Between printing and scanning you can have some fun by folding, spilling coffee, etc.\nYou can also digitally damage the images, for example, by using Python Imagine Library (PIL) and methods such as discussed here.\n", "data request - Where can I find historical aerials for some areas in Dona Ana County, New Mexico?": "\nUSGS EarthExplorer is often a fantastic resource for this.\nCheck out, in particular, these datasets:\n\nAerial Photo Single Frames (usually pre-1980)\nDOQ (1990s)\nNAPP (1980s-1990s) \nNHAP (1970s-1980s)\n\n", "data request - Administrative boundary of Colorado State Forest": "\nbest way to do this is search for locality admin site. then search it for gis. that gave me this:\nhttp://www.arcgis.com/home/group.html?owner=rsacco&title=Colorado%20Parks%20and%20Wildlife%20-%20Administrative%20Boundary%20Data\n", "socrata - Contains filter in SODA2 API call": "\nUpdate: I realized my answer is for the browser and not related to Socrata. I'll probably delete it soon.\n\nYour core URI is \nhttp://data.cityofchicago.org/resource/4ijn-s7e5.json\n\nwhich is actually just a large JSON file.\nIt's possible to use the browser to filter the JSON file. After the core URI above, you put a question mark (?) and then the filters. For example, to get inspection_id # 1434955, the URL will be constructed as follows:\nhttp://data.cityofchicago.org/resource/4ijn-s7e5.json?inspection_id=1434955\n\nThe key from the JSON file is inspection_id and the value is 1434955. These strings must match exactly, so it's most useful to use the complete JSON as a reference.\nYou can combine filters with ampersand (&). For example, this URI gives a JSON that combines all Zip Codes being 60649 and with Inspection Type being \"Complaint\"\nhttp://data.cityofchicago.org/resource/4ijn-s7e5.json?zip=60649&inspection_type=Complaint\n\nThe keys are things like zip and inspection_type, and the values are things like 60649 and Complaint. There is also the ability to give the browser JSON with more complicated filters (greater than, less than), but those will require some documentation from the data source. Normally you put a JSON-like piece of text in your URI (for example). If you use a browser tool like discussed here, I think then you can easily search text in the JSON file.\nIf you don't know programming, consider learning cURL (don't mind the tacky website, it's really a core resource). Also, there are browser extensions that may make the process simpler (for example).\n", "data request - Cafe, library ambience noise database": "\nnature sounds offers kitchen audio...which may not be close enough. it allows for exporting 30min file\nhttp://naturesoundsfor.me/ \nok. i can't find anything longer than a few minutes. check out some of the results in the open audio community on archive.org:\nhttps://archive.org/details/opensource_audio\n", "data request - Database of English words difficulty": "\nThe New General Service List (updated just a few weeks ago) is a list of \"the most important words for second language learners of English\".\nIt ranks the top three thousand headwords (plus more words within each headword, such as plurals and tenses).  \nSo it won't cover all the words in your list, and it's really indicating \"usefullness\" rather than difficulty.  But I think useful words are more likely to be used, so more familiar, so \"easier\".  It's certainly something you could use as a major factor, along with other factors, to determine difficulty.\nThe data is in a spreadsheet, and the license is CC-BY. \nFor (more difficult) words beyond that list, you could use word frequency as a major component of a difficulty rank.  One source is subtitles word frequency lists (CC-BY).  That lists a few hundred thousand words by how often they appear in English-language TV and film subtitles.  Again not the same as difficulty (and it's spoken English, rather than written), but it's an indication of how likely it is that somebody has heard the word before.\n", "data request - Database of Japanese words difficulty": "\nYou can start by gathering Japanese books or documents which you know the difficulty level of (e.g. reading level of the book). Match your list of words to your set of books/documents to derive the difficulty of your words. This will probably give you a pretty decent classification to start with. You can then improve your classification by looking at additional things like the ones on your list.\n", "data request - IRS Statistics of Income, machine-readable": "\nBeen slightly inattentive here, but for posterity's sake, I wanted to post the results of my cleaning effort: machine-readable SOI data.\nLike most intensive data cleaning I've been a part of, it wasn't the result of a single programming effort. For example, a fair bit of consideration went in to how to reconcile the individual annual series. The coverage.csv in the repo shows the series coverage for 1997-2011. Enjoy!\n", "data request - Dataset of domain names": "\nIf this still makes sense, there's a dataset that I'm maintaining:\nhttps://github.com/tb0hdan/domains\nTLD kinds: 1522\nCountry TLDs: 245\nGeneric TLDs: 1277\nTotal domains in dataset: 1,789,946,688\n", "data request - Structured movie/TV dataset": "\nAbout the TV shows I know a great site (eztv) with a list of airplay, info, trailers etc, but I cannot find anywhere about copywrite. Maybe you can contact with them and ask them about it.\nLink: https://eztv-proxy.net/showlist/\n", "tool request - Wiki site for data tables": "\nscraperwiki lets you do this, plus push to ckan databases. there are limitations for free accounts. it will break your huge file up into multiple csvs.\ngoogle drive also lets you do this, a little bit more limited, but you can still do what you want by creating a new doc (spreadsheet) and then importing your data.\ntabula is another sweet tool, published by knight foundation, that you can use on your desktop...i've had minimal problems with file size, but for each one, if i simply wait it out, as in let the program consume my resources as much as it needs....it always works.  \nas for hosting: datahub.io, google drive, will host off the top of my head. so i guess on that note, amazon's free service and dropbox too \n", "data request - IP addresses of backbone routers": "\nI suggest you use data from Caida (Cooperative Association for Internet Data Analysis).   In particular their Macroscopic Internet Topology Data Kit (ITDK) may suite your needs.\nhttp://www.caida.org/data/internet-topology-data-kit/\nQuoting from the above page:\nThe ITDK contains data about connectivity and routing gathered from a large cross-section of the global Internet. This dataset is useful for studying the topology of the Internet at the router-level, among other uses.\nThe latest ITDK release, 2013-07, currently consists of\n\ntwo related router-level topologies,\nrouter-to-AS assignments,\ngeographic location of each router, and\nDNS lookups of all observed IP addresses.\n\n", "government - Accurately Using Census Tract Data and Total Population": "\nNote that your API request is for the population of all tracts in Allegheny County, not just in Pittsburgh. But I don't think the API supports tracts-in-place right now.\nI don't know the region that well, but I don't see any problems with your method. Note that you're using 2010 ACS estimates, when you could be using 2012.\nhttp://api.census.gov/data/2012/acs5?key=XXXX&get=B01003_001E&for=tract:*&in=state:42+county:003\nRemember that the ACS data are estimates, so there is always a risk of sampling error. I don't think you can get the error as part of the Census API query, but you could get it from the American FactFinder, or from our project, CensusReporter: here is B01003 for all tracts in Allegheny county.\nOur (Census Reporter's) advice to journalists is to use caution when the error is > 10% of the estimate. That is the case in 123 of the 402 tracts in Allegheny county. But also, the total of the tract-level population estimates from your API call is 1,223,066, which is quite close to the 2010 population from the Decennial census, listed as \"estimates base\" in the Allegheny County Quick Facts.\nDo you have personal knowledge of the places with low population that makes you suspicious?\n", "data request - Change of real estate ownership": "\nIt would be difficult to track a single individual's home ownership pattern and reasons over time, primarily because these are considered personal data in most countries and would not be released as such.\nHowever, in the U.S., the following datasets may be helpful:\n\nU.S. Department of Housing datasets\nData related to families moving to areas of opportunity\nThe American Housing Survey\nFactFinder from the U.S. Census on housing information\nDetailed data for New York City for specific reasons for moving (via @EreiSegalHalevi)\n\n", "data.gov - Looking for data on bankruptcy prediction": "\nIn the U.S., bankruptcy data is found in several places. Depending on which parameters you decide are important for your tool and algorithm, some of the following datasets should be helpful to you:\n\nRecent bankruptcies, findings, and maps from the U.S. Courts\nBankruptcy statistics from the U.S. Department of Justice\nPublic bankruptcy cases from the U.S. Securities and Exchange Commission\n\nThe challenge with any predictive modeling is not where to find the data, but which data will be true indicators of the item you are trying to predict. In this case, the data above show the companies that have become bankrupt. Your challenge is finding those factors that led to their bankruptcy...a much more difficult problem.  In this case, it may be that data such as:\n\nChapter 7 Trustee reports from U.S. Department of Justice\nUCLA Bankruptcy Research Database\n\nThese may point to the underlying causes and complexity in the area you are researching.\n", "web crawling - Why and how are certain websites able to state in their terms of service that you can't crawl their data?": "\nIANAL, TINLA. If you really need to know, get a lawyer.\nIn the United States, adult human beings are presumed competent to enter into contracts while protecting their own interests. Ergo, contracts can say almost anything you can imagine and the law will enforce them. Terms of Service agreements are contracts. Ergo...\n(There are certain specific limitations to the power of contract. If you want to know more, talk to a lawyer.)\nRe copyright: in the United States, compilations of facts are not copyrightable (Feist v Rural Telephone Service) because they do not meet copyright's standard for originality/creativity. So base facts about a restaurant... not covered by copyright. Reviews, however, likely pass the originality test, so scraping them has copyright-infringement implications.\n", "data request - Where to download a dictionary of medical terms": "\nMerriam-Webster's Medical Dictionary with Audio\nFree for noncommercial (which here means not generating any revenue) use provided you submit less than 1000 queries a day. Other forms of licensing are available upon request.\ndescription, licencing and fees\n", "data request - Dataset of researchers CV": "\nI don't know if it would have all of what you're looking for, but there are a couple of efforts to try to give identifiers to researchers.  Most of them link to publications, and some have CV building tools.  They might not be 100% what you're looking for, but they may satisfice your need:\n\nhttp://orcid.org/\nhttp://researchgate.net/\nhttp://www.researcherid.com/\n\n(note that I haven't looked at their terms for extracting their data; I know when I signed up for ORCID, it asked me what info I wanted to make public.)\n", "data request - diabetes complications dataset": "\nUCI Machine Learning Repository provides three datasets for diabetes:\n\nDiabetes Data Set\nPima Indians Diabetes Dataset\nDiabetes 130-US hospitals for years 1999-2008 Data Set\n\nIf these don't meet your needs, you may want to go through the papers that cite these datasets and see what else they have used.\n", "data request - Dataset: text and music": "\nHave you seen http://freemusicarchive.org/search/ which has an API (http://freemusicarchive.org/api/docs/) as well as categories like eerie, happy, and sad\n", "data request - Age of consent per country database": "\nThe UN publishes such as list for age of consent for marriage:\nhttp://data.un.org/Data.aspx?d=GenderStat&f=inID:19\nNote, the age of consent for sex is in many countries lower than consent for marriage. The UN publishes this as well, but I was not able to find it at the moment.\n", "data request - Are there any open datasets of board games that allow commercial usage?": "\nDepending on what type (and the structure) of the metadata you're looking for, have you thought of tapping a more general product API from the likes of Google, Amazon, eBay, etc.?\nAlso: consider contacting the API you found that has terms not to your liking. Perhaps you can create a special arrangement with them.\n", "data request - Historical Weather Forecasts": "\nThe NOAA / National Weather Service has an archive from the Weather Prediction Center, but it only started archiving most products a few years ago:\n\nhttp://www.hpc.ncep.noaa.gov/archives/web_pages/wpc_arch/get_wpc_archives.php\nhttps://www.wpc.ncep.noaa.gov/archives/web_pages/wpc_arch/get_wpc_archives.php\n\nMost of what's archived are weather maps (like what you'd see behind the weatherman on the news as he's giving his forecast), and not specific temperatures.  Their 'national high/low' product just reports where the highest & lowest temperature on a given day was recorded, not the high/low for lots of cities.\nFor many years, they only maintained a 'rolling archive', where they'd keep maybe 30-60 days of forecasts, and then purge them.  It's possible that something like archive.org might have some of it, but it'd likely be an incomplete record.  If you wanted to go that route, take a look at the list of NOAA's national centers:\n\nhttp://www.hpc.ncep.noaa.gov/html/othersites.shtml\nhttps://www.wpc.ncep.noaa.gov/html/othersites.shtml\n\n", "network structure - Data sets with both social and attribute data": "\nI'm not sure if you care what domain the data are from, but I suspect the Add Health dataset may have what you're looking for. It contains a bunch of variables on health and child wellbeing and includes data on respondents' school-based networks.\n", "film - Movie/Tv Show api (with posters) for commercial user": "\nA question like this was asked on Stack Overflow a few years ago. \nBesides TheMovieDB (which was the accepted answer), and OMDB, which you've found,  people suggested the Rotten Tomatoes API, which appears to have posters, but which also looks like it's integrated into OMDB...\n", "data request - african newpapers for the past 20 years for machine learning": "\nHere's a good starting place (compiled by Columbia University):  \nNigeria:\nhttp://library.columbia.edu/locations/global/virtual-libraries/african_studies/countries/nigeria/online.html \nTanzania: \nhttp://library.columbia.edu/locations/global/virtual-libraries/african_studies/countries/tanzania/online.html\n", "data request - Word list for Common European Framework of Reference for Languages (CEFR)": "\nThere is a Wictionary list for Spanish A1 vocabulary that would be easy to parse in the print version. \n", "data request - Standing and seating capacity of clubs, bars and restaurants": "\nI've only came across one dataset that contained the seating capacity. It is the restaurant inspections for Seattle and King County:\nhttp://info.kingcounty.gov/health/ehs/foodsafety/inspections/search.aspx\n", "usa - How to construct a database with the underlying real estate data displayed by Redfin, Zillow, or Trulia?": "\nI don't know specifics on how Zillow, etc acquired their data - other than assuming they (or 3rd party) obtains the data on a per county basis.\nAs cities/counties open up open data portals, this data will become more accessible to developers.\nWhen searching these portals, you want to look for tax lots, property tax, or parcels.\nA lot of these datasets are still in Shapefile formats, but some are being provided in more readable formats (CSV). Some examples:\nDenver, CO\nhttp://data.denvergov.org/dataset/city-and-county-of-denver-parcels\nMadison, WI\nhttps://data.cityofmadison.com/Property/Assessor-Property-Information/u7ns-6d4x\nNew York, NY\nhttp://www.nyc.gov/html/dcp/html/bytes/dwn_pluto_mappluto.shtml\nVancouver, BC\nftp://webftp.vancouver.ca/OpenData/csv/property_tax_report_csv.zip\n", "Looking for a large data set of French data": "\nI also think that Twitter data is the best way to get >100Gb french dataset.\nOpenStreetMaps gives really great open data but you can't reach 100Gb easily : the Integral Metropolitan France dataset is 2.6 GB.\nThere is no open weather data by MeteoFrance (you have to pay the licence) but there is an Open Meteo Forecast that you can try. I never had the opportunity to use it but I guess it can provide you large amount of data.\nYou also have the dataset \"Tous les documents\" from the BNF that is 1.5GB and looks quite interesting.\nHope it can help.\nCheers\n", "usa - Fatality Analysis Reporting System": "\nFirst you will need to define the schema of your final database and determine how you will map each file to your schema.\nThen I recommand using ETL tool (Extract Transform and Load) such as Talend Open Studio (TOS) or Penthao (both are open source solution). Using such tools will allow you to:\n\nautomate the process (and not process each file individually)\ncreate mapping for each file between the original format and your SQL schema\n\nIn the case of talend you will do something like this:\n\nfind the right component to connect to the input file (I don't know the format made available by FARS)\nconnect this component to a tmap\nconnect the tmap to a tSQLout component to update your database\n\nThen using a tLoop and a putting your file name as a variable, you can loop through the different file and process them and define a different route depending on the file format from FARS (and update the tmap accordingly).\n", "data request - Dataset of international marriages": "\nThe United Nations Statistics Divisions publishes tables on marriages per country. While they do not break it down by nationality, they do provide a variety of demographic data.\nhttp://unstats.un.org/unsd/Demographic/sconcerns/mar/mar2.htm \nThe BLS also publishes tables on marriage that contain some ethnicity information for the US:\nhttp://www.bls.gov/opub/mlr/2013/article/marriage-and-divorce-patterns-by-gender-race-and-educational-attainment.htm \n", "data request - Dataset of allergies": "\nA list of statistics is available for U.S. persons:\n\nMedical treatment triggered by allergies and hay fever. \nAllergy by age and ethnicity for 1998-2012 (specifically at 205.207.175.93/HDI/TableViewer/tableView.aspx?ReportId=59)\n\nAn interesting related article about the need for more data on allergy prevalence in developing nations may be informative.\n", "data request - Dataset of crosswords": "\nThere was a really super visualization recently from vizual-statistix. See the whole post here.\n\nAnyway, his data source was XWord Info, which shares all the NYTime Daily Crossword Puzzles. You can see an example (most recent) of JSON format:\nhttp://www.xwordinfo.com/JSON/Data.aspx?date=9/11/2008\n\nDetails here. With a snippet of code you can search over all valid calendar dates, then download and parse.\nThere is also the XPF format which is designed for crossword puzzles in order to fit unusual features.\nFor NYTimes puzzles, the difficulty generally increases from Monday to Sunday.\n", "analysis - Open Data for Economic and Business Research within Sport Management": "\nHere are the slides from the vienna.rb talk (I did not attend it) about Open Football(soccer) Data and the World Cup 2014. The presentation is really interesting and it gives a lot of ideas I think.\nOpen Football on GitHub proposes a large collection of open football datasets.\nHere is a brilliant article called \"ANALYZING A NHL PLAYOFF GAME WITH TWITTER\". It's not really Open Data but it also gives ideas and some pieces of code !\nAn interesting article about data future in football (it's more economy than tech but I thought it's what you asked).\nMy selection is a little football-oriented, and I'm sure there are a lot of data in sports like baseball or NBA.\nHope it could help !\n", "data request - Converting Geographic Coordinates to New York City neighboorhood names": "\ntamu has a killer geocoder, as well as a definitive list of other free geocoders. all you have to do is upload your data\nhttp://geoservices.tamu.edu/Services/Geocode/BatchProcess/\nhttp://geoservices.tamu.edu/Services/Geocode/OtherGeocoders/ \n", "data request - Dataset of adulteration incidents": "\n\nHere is a Food Fraud Database. I haven't used it, but for some\nreason it was in my bookmarks.\n\n\nThe first publication of the database in 2012 includes more than 1300 entries based on more than 650 articles. This includes information on more than 350 different food ingredients. Each record in the database is a publicly reported unique combination of food ingredient, adulterant, and where available an analytical detection method published in one literature reference.\n\n\nHere is a Wikipedia list with a few dozens of incidents.\nHere is a paper (Development and Application of a Database of Food Ingredient Fraud and Economically Motivated Adulteration from 1980 to 2010). They said that they would created a Database. Maybe you could contact with them or read the paper and find any citation.\n\n", "data request - Human Mortality database around the world": "\nThe World Health Organization Mortality Database is probably what you're looking for.\n", "finance - Data set for operating cost and revenue for small business?": "\nIn Infochimps, you will find hundreds of business related datasets. A few of them are from USA about tax return, receipts and net income. You will find also about revenues for certain business categories.\nVisit here: Website Link\nP.S. If you cannot find what you need in the link, please update your question with more details. Where you have already searched or/and an example of what exactly the dataset you want to contain.\n", "Survey Questions for Data Mining Project JUSTNN": "\nI would consider asking questions similar to the data collected in the IPEDS surveys by the US Department of Education. Each year, every accredited university/college and trade school fills this survey out. It includes a large number of questions, including: tuition, books, housing on/off campus, degrees offered, size of freshman class, student population by ethnicity, etc.\nYou can download standard or customized tables from the US Dept of Ed here:\nhttp://nces.ed.gov/ipeds/datacenter/ \nI also have a number of these tables converted into CSV format, which anyone is free to use:\nhttp://www.opengeocode.org/cude1.1/US%20Education/IPEDS/\n", "contributing data - How do I add a topic to Freebase?": "\nCopied directly from Freebase (source)...\n\nMany times when you think you need to Create a New Topic, you probably will NOT need to.\nIF YOU'RE SURE YOU NEED TO CREATE A NEW Topic in Freebase:\nThink about what sort of thing your topic is. For instance, is it a Person, Film, or Skyscraper.\nOn Freebase.com, navigate to that Type (the Search interface at the top of each page is an easy way to do this)\nWhen you are on the type page, click \"Add more topics\" in the upper right of the page\nEnter the name of your new topic\nOnce it is added, you can click through to it and then edit it to add more information.\nNew topics usually appear in a Search within a minute or so, but this can be longer if the system is under high load.\nSEE ALSO: FAQs about Topics\n\n", "data request - Download list of the name of every country in Western languages": "\nA list of the names of each ISO country code in up to 114 languages was produced by Jonah Ellison in 2011 (the number of languages available varies by country).\nThere's a link to the UTF-8 CSV file on his page: 21,640 Translated Country Names.\nEach line contains the ISO country code, the ISO language code, and the name of the country in that language.  \nFor example, here's Luxembourg (country code LU) in around 75 languages, after loading that file into a spreadsheet:\nLU  af  Luxemburg\nLU  am  \u1209\u12ad\u1230\u121d\u1260\u122d\u130d\nLU  ar  \u0644\u0648\u0643\u0633\u0645\u0628\u0648\u0631\u062c\nLU  az  L\u00fcksemburq\nLU  be  \u041b\u044e\u043a\u0441\u0435\u043c\u0431\u0443\u0440\u0433\nLU  bg  \u041b\u044e\u043a\u0441\u0435\u043c\u0431\u0443\u0440\u0433\nLU  bn  \u09b2\u09be\u0995\u09cd\u09b8\u09c7\u09ae\u09ac\u09be\u09b0\u09cd\u0997\nLU  bo  \u0f63\u0f40\u0f0b\u0f5b\u0f58\u0f0b\u0f56\u0f7c\u0f62\u0f92\u0f0d\nLU  ca  Luxemburg\nLU  cs  Lucembursko\nLU  cy  Lwcsembwrg\nLU  da  Luxembourg\nLU  de  Luxemburg\nLU  el  \u039b\u03bf\u03c5\u03be\u03b5\u03bc\u03b2\u03bf\u03cd\u03c1\u03b3\u03bf\nLU  en  Luxembourg\nLU  eo  Luksemburgo\nLU  es  Luxemburgo\nLU  et  Luksemburg\nLU  eu  Luxenburgo\nLU  fa  \u0644\u0648\u06a9\u0632\u0627\u0645\u0628\u0648\u0631\u06af\nLU  fi  Luxemburg\nLU  fo  Luksemborg\nLU  fr  Luxembourg\nLU  fur Lussemburc\nLU  ga  Lucsamburg\nLU  gl  Luxemburgo\nLU  gsw Luxemburg\nLU  gu  \u0ab2\u0a95\u0acd\u0a9d\u0aae\u0aac\u0ab0\u0acd\u0a97\nLU  he  \u05dc\u05d5\u05e7\u05e1\u05de\u05d1\u05d5\u05e8\u05d2\nLU  hi  \u0932\u0915\u094d\u0938\u092e\u092c\u0930\u094d\u0917\nLU  hr  Luksemburg\nLU  hu  Luxemburg\nLU  hy  \u053c\u0575\u0578\u0582\u0584\u057d\u0565\u0574\u0562\u0578\u0582\u0580\u0563\nLU  id  Luxembourg\nLU  is  L\u00faxemborg\nLU  it  Lussemburgo\nLU  ja  \u30eb\u30af\u30bb\u30f3\u30d6\u30eb\u30b0\nLU  ka  \u10da\u10e3\u10e5\u10e1\u10d4\u10db\u10d1\u10e3\u10e0\u10d2\u10d8\nLU  km  \u179b\u17bb\u1785\u17a0\u17d2\u179f\u17c6\u1794\u17bd\u179a\nLU  kn  \u0cb2\u0c95\u0ccd\u0cb8\u0c82\u0cac\u0cb0\u0ccd\u0c97\u0ccd\nLU  ko  \ub8e9\uc148\ubd80\ub974\ud06c\nLU  ln  Luksamburg\nLU  lo  \u0ea5\u0eb8\u0e81\u0ec1\u0e8a\u0ea1\u0ec0\u0e9a\u0eb5\u0e81\nLU  lt  Liuksemburgas\nLU  lv  Luksemburga\nLU  mk  \u041b\u0443\u043a\u0441\u0435\u043c\u0431\u0443\u0440\u0433\nLU  ml  \u0d32\u0d15\u0d4d\u0d38\u0d02\u0d2c\u0d30\u0d4d\u200d\u0d17\u0d4d\nLU  mr  \u0932\u0915\u094d\u091d\u0947\u0902\u092c\u0930\u094d\u0917\nLU  ms  Luksembourg\nLU  mt  Lussemburgu\nLU  my  \u101c\u1030\u1007\u1004\u103a\u1018\u1010\u103a\nLU  nb  Luxembourg\nLU  nds Luxemborg\nLU  ne  \u0932\u0915\u094d\u091c\u0947\u092e\u092c\u0930\u094d\u0917\nLU  nl  Luxemburg\nLU  nn  Luxembourg\nLU  or  \u0b32\u0b15\u0b4d\u0b38\u0b47\u0b2e\u0b2c\u0b30\u0b4d\u0b17\nLU  pl  Luksemburg\nLU  pt  Luxemburgo\nLU  ro  Luxemburg\nLU  ru  \u041b\u044e\u043a\u0441\u0435\u043c\u0431\u0443\u0440\u0433\nLU  se  Luxembourg\nLU  sk  Luxembursko\nLU  sl  Luksemburg\nLU  so  Luksemboorg\nLU  sq  Luksemburg\nLU  sr  \u041b\u0443\u043a\u0441\u0435\u043c\u0431\u0443\u0440\u0433\nLU  sv  Luxemburg\nLU  sw  Luksemburg\nLU  ta  \u0bb2\u0b95\u0bcd\u0bb8\u0bcd\u0b9a\u0bae\u0bcd\u0baa\u0bb0\u0bcd\u0b95\u0bcd\nLU  te  \u0c32\u0c15\u0c4d\u0c38\u0c02\u0c2c\u0c30\u0c4d\u0c17\u0c4d\nLU  th  \u0e25\u0e31\u0e01\u0e40\u0e0b\u0e21\u0e40\u0e1a\u0e34\u0e23\u0e4c\u0e01\nLU  tr  L\u00fcksemburg\nLU  uk  \u041b\u044e\u043a\u0441\u0435\u043c\u0431\u0443\u0440\u0433\nLU  vi  L\u00fac-x\u0103m-bua\nLU  zh  \u5362\u68ee\u5821\n\nScroll down the above list to see all the languages.\nA more authoritative source, but in a less convenient form, would be the Unicode CLDR files you mention. In the file for Portuguese (for example) country names are listed as \"names / territory\" starting at about line 2340. \n", "data request - High Resolution Mineral Maps": "\nThe US government portal for this data is mrdata from from USGS:\nMineral Resources On-Line Spatial Data.\nThey also include some world maps with a list of external databases.\nIn particular you may be interested in the Mineral Resources Data System (MRDS).\n\nIn addition to interactive maps, they also offer a list of API web-services.\nJust a couple of the services:\n\nData records near a geographic location\nData catalog records for a topic category\n\n", "english - Relationship Advice/Advice Column Corpora?": "\nEnglish Language&Usage has some material that might interest you, under some of its tags like this one:\nhttps://english.stackexchange.com/questions/tagged/politeness\nThe data is licensed under cc by-sa 3.0 with attribution required.\n", "data.gov - Weather radar data for Europe, especially Latvia": "\nWeather radar data specific to Latvia can be found using NOAA's Radar Data tool.  Just enter \"Republic of Latvia\" in the search box and you're off and running.\nThere are a large number of datasets covering international areas at the U.S. National Oceanographic and Atmospheric Administration (NOAA), which maps some of the global daily indicators. The Global Historical Climatology Network-Daily (GHCN-D) is the data you are seeking.\nInternational indicators of climate change and data collected from many organizations are available as well. \n(Disclaimer: I am the Evangelist for Data.gov)\n", "images - Where to contribute pictures of election leaftlets?": "\nUnlock Democracy runs a project called Election Leaflets.\nYou can take photos, scan the leaflets, or even post them to their address.\nThey've got a good repository of leaflets.\n", "tool request - Best way to convert Excel Files to Open Data Formats": "\nI don't believe that you will find a \"ready\" library that will be the case for all your excel files. My advice is to create a script by yourself in base of your file structures. Especially if the updates will still have the same structure.\nI cannot help you in PHP, but in Python you can find an answer for what you need here: A Python guide for open data file formats. If you decide to do it, you can update your question with an example of your files and let users help you with the code.\n", "IP Geocoding Data Sources and/or APIs": "\nThere are three sources that may be helpful:\n\nThe answer on Stack Overflow on how to Geocode an IP Address\nFree GEOIP has a RESTful API for this specific task\nThe Google Map API\nA geolocation XML API from PInfoDB\n\n", "What decentralized open data project has the most contributors?": "\nI would say Wikipedia, with 21,436,641 named editors (and more anonymous editors), but there might be bigger projects.\nThe only other hard-numbers references I could find are the Ohloh project, which has tons of similar data, but it is only about open source projects, not all open data.\n", "programming - PHP script automatically convert content in open data formats": "\nI found this Wordpress Plugin WP-CSV\nDetails:\n\nMore than 50000 lines can be imported/exported (the only limit is\nyour server) \nPosts, pages, and custom post types\nTags, categories,   and custom taxonomies\nCustom fields (simple and complex)\nThumbnails\nFlexible filter system to easily control which fields export\nSimple User Interface (if you know Excel or another spreadsheet\nprogram, you will find this plugin quite easy) The plugin should now\nbe usable with most plugins that are fully WordPress compliant.\n\n", "data request - Domain Name System Record A database": "\nThe DNS Census 2013 is an attempt to provide a public dataset of registered domains and DNS records.\nThe torrent is about 15 GB (uncompressed: 157 GB).\nI am pretty sure it contains domain names and IPs, but I am not 100% sure as I have not downloaded it to check. Please let us know, thanks!\nYou can extract TLD from domain name.\n", "data request - Indian caste dataset": "\nSince the caste system has been officially banned, there have changes in Indian society that seem to complicate a straightforward connection between caste and last name.  Some of those changes include people choosing to change their last name, inclusion of other cultural norms in naming, and mobility within society.\nHowever, there is still data available, but you might need to merge several datasets and sources together to achieve what you are looking for. There is a strong relation between region, caste, and name, and so sorting by region should give you better insights. The following might be helpful:\n\nScrape region, name, and caste from the nice compilation at, unexpectedly, Wikipedia and in Edgar Thurston's Ethnographic Notes in Southern India.\nDatasets on caste-related information at Data.gov.in (but in general these do not include the last name of respondents or surveyed individuals)\nThere is additional data on caste in relation to many other variables from the Indian National Election Study (ranging from 1967-1985). \n\nThese guidelines may be helpful in looking at the structure of Indic name data sources.\n", "geospatial - Redistributable Twitter-like data": "\nstackexchange data has practically all of those requirements:\nhttp://data.stackexchange.com/stackoverflow/query/new\nflickr does all of that, although i'm not sure if their use of tags constitutes hashtags, but i'm sure thats easily convertible.\nyou could also use my personal twitter history that i've downloaded. i could careless. maybe you could get a few more people to download theirs also, combine them into a repo on github?\nyou could also do the same with my delicious account, although there are no geotags in the set.\ni think you could actually apply this to all of the social networks that you can download your personal data from and whose datasets match your requirements. \nidenti.ca comes to mind. although i think they are diaspora now.\n", "data request - Examples or datasets of evolving networks": "\nOne of my favorite (dynamic) networks to study is Wikipedia. You have plenty of data to analyze and it is evolving every second :)\nHere is an example of the edits in Wikipedia articles: http://rcmap.hatnote.com/#en\nHowever, if you want more options, in this repository, you will find dozens of networks and you could sort the table by \"Dynamic\" networks to find only those you need.\n", "Video game meta-data (supplement for Steam API)": "\nThere is an unofficial API for metacritic which can provide most of the data you request. You'll have to register.\nHere is an example cURL request of the 'find game' endpoint:\ncurl --include --request GET 'https://byroredux-metacritic.p.mashape.com/details?url=http%3A%2F%2Fwww.metacritic.com%2Fgame%2Fplaystation-3%2Fthe-elder-scrolls-v-skyrim' --header \"X-Mashape-Authorization: <mashape-key>\"\n\nTo loop over all desired games, you'll have to pass this type query for each game URL at metacritic. Creating that list of URLs may be a small project in itself (although at least they are logical).\nThat gives a JSON output:\n{\n  \"result\": {\n    \"name\": \"The Elder Scrolls V: Skyrim\",\n    \"score\": \"92\",\n    \"rlsdate\": \"2011-11-11\",\n    \"genre\": \"Role-Playing\",\n    \"rating\": \"M\",\n    \"platform\": \"PlayStation 3\",\n    \"publisher\": \"Bethesda Softworks\",\n    \"developer\": \"Bethesda Game Studios\",\n    \"url\": \"http://www.metacritic.com/game/playstation-3/the-elder-scrolls-v-skyrim\"\n  }\n}\n\nTo get more than one result per query, you'll have to construct some clever searches using the 'search games' endpoint.\nThey provide code for common languages (i.e. python), so you can perhaps use search to get and store multiple games.\n", "data request - 3-dimensional dataset to test tri-clustering method": "\nA good place to start looking for datasets is the Machine Learning Repository from UCI.\nThere are a few datasets that stand out for your method. Although they may not match exactly, you may get some ideas.\n\nThe YouTube Codedy Slam Preference Data\nCharacter Trajectories\nUser Identification from Walking Activity\n\nYou can use the table columns \"Default Task\", \"Attribute Types\", and \"# Attributes\" to help narrow down more possible data sets.\n", "data request - Total screen time and first on-screen appearance by actors in films": "\nI don't have an answer but your request reminds me of a Slate.com article about characters shared scenes in Friends. The journalist explained his method:\n\nTo determine which characters shared scenes, I downloaded transcripts of all 236 episodes from this remarkably comprehensive fan site. For the purposes of this inquiry, I treated these transcripts as authoritative, as watching all 10 seasons was impractical. Due to inconsistencies in the transcripts, which do not always list all of the characters present in a given scene, I used the following methodology: If a character spoke a line in a scene, I marked him or her as present. Admittedly, this is an imperfect approach, as even with this gregarious group, there were scenes in which a character was present but did not have a line. However, in most instances, all parties present in a scene end up uttering a line, so I\u2019m confident my analysis is sound, even if it missed a moping Ross here or a pouting Rachel there.\n\nInterestingly, the author mentions the limitations of his method.\nThis method won't help you to measure exactly how much time an actor appear on-screen. But it may be a good basis to calculate how much time an actor is present in a scene, by measuring the time between his first and last sentence in a scene, then repeating it for every scene to get the total time in the movie. It may be a good approximation of the time an actor spent on-screen.\n", "CKAN database scheme": "\nCAVEAT: I am not an experienced user of the CKAN software.\nFrom reviewing the online documentation, it appears that CKAN supports storing data in two methods: FileStore and DataStore. FileStore is storing and retrieving an entire datasets (e.g., CSV file, PDF document, etc). It does not involve a database.\nThe Datastore uses what they call an ad-hoc database for storing and querying individual elements of a structured dataset. In one mode, it appears to be fairly generic, creating a table and entries based on the column names in the structured data, and supporting an API query based on the column names.\nhttp://docs.ckan.org/en/1117-start-new-test-suite/datastore.html\nI believe it has more advanced capabilities and relationships in the structured data is linked data or RDF and is constructed using a vocabulary supported by CKAN, which include the Dublin Core, DCAT, VoID and SCOVO.\nhttp://docs.ckan.org/en/1117-start-new-test-suite/linked-data-and-rdf.html\n", "government - Now I have an API key, how do I access OpenFDA?": "\nI'm one of the core team members for openFDA. Really sorry to hear that you find the documentation cryptic - one big goal that we have is to make the API as easy to understand as possible!\nThe main documentation for drugs has a number of sample queries: http://open.fda.gov/drug/event/\nYou can hit \"Run Query\" next to any of those and it will show the results right there in the page. You can edit the query, hit Run Query again, and it will update accordingly.\nIn regards to the API Key, it is optional. When you sign up, you will get an email from api.data.gov, who provides API keys for the entire federal government. We're working on customizing those emails to include an FDA example.\nHow would you recommend we improve the documentation?\n", "json - Data In CSV Format from OpenFDA": "\nJoe, we don't support CSV downloads for the time being. Drug adverse event data is highly relational, a format that does not easily lend itself to being represented in a format like CSV. For instance, a given record may have 5-10 different drugs associated with it and an additional 5-10 different reactions, each of which all have their own additional metadata.\nIf you have a recommendation on how we could easily represent this data in CSV, please do let me know!\n", "data request - Are there any open datasets for Wrestling statistics": "\nFILA keeps an online database of all matches for Amateur/Professional wrestling tournaments worldwide. These are \"Olympic\" class wrestlers (not WWE). It does not though contain collegiate events. It does have all Olympic related records dating back to 1896\nhttp://www.fila-official.com/index.php?option=com_content&view=article&id=768&Itemid=100236&lang=en\nThe NCAA publishes it's national championship results here:\nhttp://i2.turner.ncaa.com/dr/ncaa/ncaa7/release/sites/default/files/external/gametool/brackets/wrestling_di_2014.pdf\n", "api - How to get total count of adverse effects events by manufacturer in openFDA?": "\nDirect count queries are supported: e.g.\nhttps://api.fda.gov/drug/event.json?count=patient.drug.openfda.manufacturer_name.exact\nworks without an accompanying search.  \nI can confirm that it's not returning the full list of manufacturers.  The 1000 entry cap is to avoid the backend server overloading, but it should only be relevant for non-count queries: count queries are supposed to return all of the results by default, which is why skip is disabled for them.\nThanks for reporting this -- I'll file an issue to figure out what's going on.\n", "api - Format for exchanging open data catalogs": "\nI would propose that each site that hosts a Catalog of Open Data Portals has a downloadable version of the catalog in CSV format. To ease exchangeability, I would recommend the following layout:\nBasic Level\nurl, short name, formal name, political entity, admin division, category, license\nurl : Url to the open data portal\nshort name: A short name for the data portal (abbreviation, acronym, etc).\nformal name: formal name of the open data portal (e.g., City of Madison Open Data ....)\n\npolitical entity: country or dependency (e.g., Canada)\nadmin division: administrative division type (e.g., state, province, county, town,...)\ncategory: type of open data portal. some examples:\n\nData Portal (multi-category)\nTransparency Portal (government finances, budget, payroll, expenditure)\nGIS/Gazetteer (Geographic Information Systems, Maps)\nCensus/Demographics (Population Statistics)\nHealth\nEducation\nCommerce/Transportation\nScience\nHistorical\nMilitary\n\nlicense: type of license to use the data. examples:\n\nCC0 (public)\nCC-BY\nODbl\nUK OGL\nPDDL\n\nLevel II\nThis level would add information for geolocation and geographic scope that the portal covers.\nurl, short name, formal name, political entity, admin division, category, license, coord, area\ncoord : lat/lng of area centroid\narea  : total area in sqkm\nThis should be fairly easy to looked up and added.\nLevel III\nThis level would add population scope that the portal covers.\nurl, short name, formal name, political entity, admin division, category, license, coord, area, pop, year\npop: population\nyear: year of population statistic\nLevel IIII\nThis level would add information on the amount and scope of information in the data portal:\nurl, short name, formal name, political entity, admin division, category, license, coord, area, pop, year, ndatasets, nrecords, subcategories\nndatatsets : the (approx) number of datasets in the portal\nnrecords : the (approx.) total number of data records in the portal.\nsubcategories: a list of subcategories of the type of data available (e.g., crime, bus routes, business licenses, etc).\nThis would take a crawler to periodically crawl the portal to determine size and scope.\n", "data.gov - Where do I type in my query in the API in openFDA.gov": "\n@Joe Germuska - we built something like you mentioned (user-friendlier web interface to search Open FDA adverse events API) at http://searchopenfda.socialhealthinsights.com\n", "government - Search for False advertising/drug name associated with patient complaint of false advertising": "\nJoe is correct here. We don't currently offer data on patient claim of false advertising. In our scope for the future, we plan to offer data on product recalls and product labels. We're definitely listening to the community, though, and will add patient claims of false advertising to our list of potential datasets for the future!\nSean Herron, openFDA Team Member\n", "Australia District/County level data needed": "\nThe link below is to the National Geospatial Agency (NGA) Geographic Name Server (GNS) for geographic features of Austrialia. The dataset has been converted to a linked CSV format and should be easy to parse. Your interest will be in records that are administrative divisions (NGA/GNS FCFC=A). The value of NGA/GNS DSG will tell you the type of administrative division:\nADM1: provinces and territories\nADM2: 2nd level divisions (e.g., shires)\nADMD: other smaller division (I think this includes districts)\nhttp://www.opengeocode.org/cude1.1/NGA/GNS/AS.zip\nThe link below is to the Australian Statistical Geography Standard fact sheets. These PDF documents explain how the Australian Census is broken down into geographic areas and contain links to various downloads. The smallest unit are mesh blocks, which consist of approx. 30 to 60 dwellings. \nhttp://www.abs.gov.au/websitedbs/D3310114.nsf/home/ASGS+Fact+Sheets\n", "data.gov - Seriousness values on OpenFDA": "\nIf you take a look at our API documentation (http://open.fda.gov/drug/event/reference/), you'll see a complete reference for every field we return and the range of possible values. We follow the E2b specification for adverse drug reporting (http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/UCM350390.pdf).\nFor the specific examples you provided, here's what our documentation reads:\nseriousnesshospitalization\n\nThis value is 1 if the adverse event resulted in a hospitalization,\n  and absent otherwise.\n\nserious:\n\n1 = The adverse event resulted in death, a life threatening condition,\n  hospitalization, disability, congenital anomali, or other serious\n  condition.\n2 = The adverse event did not result in any of the above.\n\nprimarysource.qualification:\n\nAn encoded value for the category of individual submitting the report.\n1 = Physician\n2 = Pharmacist\n3 = Other Health Professional\n4 = Lawyer\n5 = Consumer or non-health professional\n\n", "programming - What is a good Python CKAN Tutorial?": "\nHave you looked at https://github.com/ckan/ckanapi ? It is the official and current python client for CKAN. \n", "data.gov - Inconsistency in API results?": "\nThe first query only returning 1 result is probably a bug with the API. I would suggest reporting it. The total number of records for this query currently is 286,115. You can get more records using the limit parameter. This query will get you the first 50 records.\nhttps://api.fda.gov/drug/event.json?search=patient.drug.openfda.pharm_class_epc:\"nonsteroidal+anti-inflammatory+drug\"&limit=50\nYou can use the &skip parameter to walk through the results. Yesterday, you could request fairly large limits. I noticed that today you get an error message if you set the limit to above 100 records. I think they retuned the number of records and limit after they launched on Monday.\nThey added this to the API Basics on the &limit option:\nReturn up to this number of records that match the search parameter. Large numbers (above 100) could take a very long time, or crash your browser.\n", "openfda - Search for deaths by a specific drug": "\nHere are two ways to do your query:\n\nUsing the OpenFDA API directly by specifying a brand_name and reactionmedrapt value: https://api.fda.gov/drug/event.json?search=patient.drug.openfda.brand_name:(tysabri)+AND+patient.reaction.reactionmeddrapt:(death)&limit=50&skip=0\nUsing a tool I helped build which is a searchable web interface to the OpenFDA data at http://openfdasearch.herokuapp.com/?drugbrandname=tysabri&patient_reaction=death\n\nHope this helps!\n", "geospatial - Data on business locations": "\nCan't comment so sharing advice in an answer: Can you be more specific about what type of location data?\n\nIf every type of category, you're probably best using APIs/licensed data from the likes of Infochimps, Google Places, Yelp, Foursquare, etc.\nIf you're doing something smaller scope like hospitals and clinics or maybe gas stations, you can probably find some open data about it that covers the majority of the data points and ultimately saves you money.\n\n", "government - Model documents for presubmission meeting with FDA about mhealth pilot trial and risk assessment": "\nWhile I am not part of the Open FDA team (I am a member of the community), I am pretty sure this is not part of the current Open FDA initiative which is at http://open.fda.gov. \nIf I were you, I would start at the following links:\n\nRock Health's \"FDA 101: A Guide for Digital Health Entrepreneurs\"\nFDA page about \"Mobile Medical Applications\"\n\n", "api - OpenFDA adverse event counts by dates do not add up": "\nMark is correct - you need to specify the date field that you want to search on. receivedate tends to work best. \n", "api - How to get all the DrugList": "\nThe current OpenFDA Adverse Event dataset isn't really well suited for a query like that. An API for the drug SPLs (Structured Product Labels) is coming soon according to https://open.fda.gov/about/ though.\nFor now, check out the following resources for the data you're looking for:\n\nNLM Pillbox API (http://pillbox.nlm.nih.gov) - recommended - most developer friendly \nNIH DailyMed bulk downloads and APIs (http://dailymed.nlm.nih.gov/dailymed/help.cfm#webservices)\nFDA National Drug Code Directory bulk download and web search (http://www.fda.gov/drugs/informationondrugs/ucm142438.htm)\n\n", "geospatial - High resolution population density maps in the US": "\nAs part of PL94-171, the canonical source for population data for drawing legislative districts is the Decennial census. \nTo get the high resolution data you want, you can download shapefiles of blocks by state with population included from http://www2.census.gov/geo/tiger/TIGER2010BLKPOPHU/\nOf course, if you want to get into demographics of the population of those blocks, that is also available from the decennial census. You can get race and basic age from the files released for PL94-171 -- that data is the first release from each decennial census so that redistricting can get started promptly. Of course, now that the decennial census is several years in the past, you can get complete demographic data, although dealing with it at the block level is not a simple task.\n", "data request - Dataset with a specific demographic distribution due to user interface": "\nNot a user-interface problem, but a related consequence: in Blue states watch more porn than red states, bad data collection led to the conclusion that Kansas consumes more porn (from pornhub.com) than any other state. \nFurther review revealed that the IP geolocation service defaults to Kansas when it can't locate an IP address, skewing the data.\n", "data request - Demographics by Zipcode": "\nThe Census Bureau has far easier avenues of extracting data through the American FactFinder (AFF). There are many tutorials for how to use AFF, but it is generally not been 'intensely hard' to use for me. By downloading the information in .csv format, you would be able to download the latest information from the ACS, or the 2010 Census.\n", "data.gov - Weekly raster data for temperature and precipitation in US": "\nNOAA puts out a weekly division dataset on temperature, precipitation, and drought that should answer your need.\nIn addition, weather radar data an be found using NOAA's Radar Data tool, and you can request data for any date range, including weekly.\nThere are a large number of datasets covering all areas of the world the U.S. National Oceanographic and Atmospheric Administration (NOAA), which maps some of the global daily indicators including the Global Historical Climatology Network-Daily (GHCN-D).\nInternational indicators of climate change and data collected from many organizations are available as well.\n", "economics - Should we open our data?": "\nQuick answer with what comes to my mind. They might be several others.\nAdvantages\n\nSince you lower the barrier to access the data (no more scraping), you will see more people using it and new usage of your data will appear\nYou will also receive feedback regarding your data quality (since there is more eyes looking at it under different angles)\nThis can be a marketing opportunity to introduce your company to new audience via partner website and application (I used your data on my website / application in return I mention you as the source)\n\nDisavantages \n\nA competitor can reused those data and present it in a more user friendly was, with nice functionality and get straight after your current business.\nDepending on the format used someone can reverse ingeneer your internal process / data structure and reuse it on a different market.\n\nI think the next question you will need to asnwer is the license and release format (API, csv or database dump ...). Those can be different questions on OpenData SE)\n", "Access to text entry speed data?": "\nI am not aware of any dataset like this one. But if you Google \"typing speed test data\", you will see many websites that does exactly this. And most of them have a contact page. You shall contact with them (maybe all of them) and ask if they are willing to share their data.\n", "data request - List of all ecommerce websites": "\nBased on checkout usability performance these are the top 100 E-Commerce websites\nhttp://baymard.com/checkout-usability/benchmark/top-100\n", "data request - Resolve company name / TLD to industry?": "\nDUNS is the best business list that I can think of -- but I have no idea if it has industry info in it.  Years ago, when you wanted an SSL certificate, registrars wanted your DUNS number to verify you weren't mascarading as another company.  Unfortunately, it's not an open list.\nFor something that's more likely to be free, for the U.S., there's a the EIN, which the government uses to track employers.  I'm not aware of any searchable list, though.   The SBA's website   points to the SEC's EDGAR for public companies, and GuideStar for non-profits.  This would miss private companies, though.\n", "OpenFDA API : can we count on several fields?": "\nNot at this time. I'm not 100% sure how the team behind OpenFDA is fielding feature requests but I've been putting ideas and requests like this on Github and they have been labeling them as \"enhancements.\".\nLink to OpenFDA github: https://github.com/fda/openfda/issues\nEDIT: While OpenFDA was in beta, I built a drill down tool (see http://recordit.co/Hn65jq.gif for screencast) but it's currently broken because the API changed quite a bit.. if something like that seems useful to you I can work to resurrect it :)\n", "usa - What data on violence against women in the U.S. is available?": "\nThe FBI provides summary data on crime statistics. These tables cover a variety of categories and geographic regions (national, state, county, Metropolitan Statistical Areas (MSA)).\nThe start page is here:\nhttp://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/violent-crime\nThe world bank has a Excel spreadsheet for violent crimes against women on a per country basis. The data is summary (percent/totals). \nhttp://web.worldbank.org/WBSITE/EXTERNAL/TOPICS/EXTSOCIALDEVELOPMENT/EXTCPR/0,,contentMDK:22488819~menuPK:6835249~pagePK:148956~piPK:216618~theSitePK:407740,00.html\n", "Where is the list of fields available through the openFDA API?": "\nThere are a few sources for the information you're looking for (essentially a data dictionary if I am understanding your question correctly):\n\nList of OpenFDA Fields (data OpenFDA has added/annotated for developer and user convenience on top of the base FAERS data fields)\nFAERS/AERS Mapping Notes which are part of the Github project and document some caveats about fields like primarysource.reportercountry\nLastly, the one of the great parts of the OpenFDA team using ElasticSearch as a backend is that you can query by literally any field you see returned to you. So you could look at the sample queries, look at the fields, and query that way.\n\nHope this helps! \n", "weather - What is the best source of open data on sea level rise in Bangladesh and India?": "\nThe NOAA National Oceangraphic Data Center (NODC) maintains data (and datasets) for sea levels and tides. You can get current (hourly) or historic data back to 1994. The data is collected from 289 stations around the world.\nhttp://www.nodc.noaa.gov/General/sealevel.html\n", "api - openFDA: Can we print results for multiple patients at once?": "\nYou're looking for the limit URL parameter which maxes out at a value of 100. \nFor example: https://api.fda.gov/drug/event.json?search=receivedate:[2012-08-09+TO+3000-01-01]&limit=100\nThis is explained at the \"Query parameters\" section of the API documentation at https://open.fda.gov/api/reference/#query-parameters\n", "data request - What is the national median salary in Poland?": "\nHave a look at Eurostat's earnings database. There you will find the mean, median, 10th percentile and 90th percentile for hourly, monthly and annual earnings. These are gross earnings. For the moment there is data for 2002, 2006 and 2010. According to Eurostat, an update is due in 2016, with figures for 2014.\n", "usa - USGS Various API Calls": "\nI looks like you have your left/right longitude values mixed up. Try fixing this and post back your results.\n", "medical - Is there a way to extract the brand names of drugs available in USA?": "\nAfter downloading the zip file, use only the \"prescribable\" portion. (that are drugs currently approved for use, without many \"historical\" drugs)\nThe main file is RXCONSO.RRF which is 28MB.\nIt has 113290 rows and contains many different types of rows (e.g., synonyms).\nTo filter only ingredients, using R, do this:\nconso<-read.delim(file = 'RXNCONSO.RRF', sep='|',header = F, stringsAsFactors=F)\n\nconso[3:7] <- list(NULL)  #not used in this distribution, only in full UMLS\n\ncolumn names can be seen here \nhttp://www.nlm.nih.gov/research/umls/rxnorm/docs/2014/rxnorm_doco_full_2014-2.html#s12_4\nnames(conso)[1] <- 'rxcui'\nnames(conso)[3] <- 'rxaui'\nnames(conso)[7] <- 'sab'  #12\nnames(conso)[8] <- 'tty'  #13\nnames(conso)[10] <- 'str' #15\n\nUse TTY term type column - value BN=Brand Name\n#only brand names\nbn<-subset(conso,tty=='BN',select=c('rxcui','str'))\n\nand in bn data.frame is your result.\nSample result would look like this:\n> head(bn,5)\n    rxcui       str\n1      38  Parlodel\n73    332  Adipex-P\n107   479   Alfenta\n187   756 Anafranil\n188   769   Anaspaz\n\nIn fact, to get only generic ingredient names, a similar query can be used like this:\n#get only ingredients and CUI and name for each intredient\ningr<-subset(conso,tty=='IN',select=c('rxcui','str'))\n\n", "usa - Docket Information call returns \"Not Acceptable\" error in windows form app": "\nUse a network sniffer like Wireshark to compare:\n\nThe request that fails\nThe request that succeeds\n\nThere is probably a tiny difference in the request, that you can find out and fix.\nNetwork sniffers are easier to use on HTTP, but if you are forced to use HTTPS, there are still various solutions.\n", "dataset - 'Panel' data without unit identifier - Cross Validated": "\nThis is what is called a \"pseudo-panel\" data set.\nLook up\nPseudo Panels and Repeated Cross-Sections\nand \nVerbeek, M.: 1996, Pseudo Panel Data. Chapter 11 in: Matyas, L. and Sevestre, P.(eds.): The\nEconometrics of Panel Data. A Handbook of the Theory with Applications.\nIf you have a model, then as an estimation benchmark I would suggest to run also basic pooled OLS (which is an estimator that doesn't really care whether your data are well ordered or not).\n", "api - Results data for Basketball World Cup": "\nThere is a question about a soccer API which has answers with many sports APIs - LINK.\nAlso, check out my answer there about the ESPN Developer Center API, which should include the FIBA World Cup.\nNote: The ESPN Developer APIs have been retired.\n", "Is the Healthcare Finder API broken?": "\nSorry about that - there was a firewall rule blocking download of that file. We have fixed this and the link to the schema document should now be working.\n", "How can I get raw occupational data from the census long form?": "\nThe American Community Survey (ACS) Public Use Microdata Sample (PUMS) is likely what you are looking for. The ACS includes occupational codes for respondents based on questions that people responded to in this questionnaire (page 11). In the PUMS dataset, there is the INDP variable which houses 2012 NAICS occupation codes which you look at in a data dictionary (e.g. The 2012 1-Year ACS PUMS (page 60)). The raw files are present on the Census Bureau's FTP server here.\nHowever, if you are looking for the most approachable method to diving into the ACS PUMS, I recommend doing it through the University of Minnesota's iPUMS project. The data is open access, registration is free (although there is wait time for registration approval wait period that might take about a day). It allows you to download data that you can import into Stata and SAS.\n", "openfda - open.fda.gov JSON search terms": "\nSee my answers to a question similar to yours here: 'Where is the list of fields available through the openFDA API?'. You could also just do a query for a random event report like https://api.fda.gov/drug/event.json?limit=5&search=morphine and then use a JSON visualizer such as jsonviewer.stack.hu look at the structure and fields in a more friendly way.\nHope this helps!\nEDIT: Also, see https://open.fda.gov/drug/event/reference/ which includes an anatomy of a typical response. \n", "data request - List of English words": "\nYou can download an Aspell Dictionary, then  convert it to simple list of words: \naspell -d en dump master | aspell -l en expand > my.dict\n\nA few other dictionaries.\n", "data.gov - What are open data sources about current sea state?": "\nI answered a question similar to this that has a useful resource for current sea level information:\nWhat is the best source of open data on sea level rise in Bangladesh and India?\n\n>\n  The NOAA National Oceangraphic Data Center (NODC) maintains data (and datasets) for sea levels and tides. You can get current (hourly) or historic data back to 1994. The data is collected from 289 stations around the world.\n>\n  http://www.nodc.noaa.gov/General/sealevel.html\n\n", "data request - High resolution, small area, maps that characterise natural terrain": "\nElevation Data\nFor high resolution DEMs, which I define as 1/9 arc-second (approx. 3m) or better in the horizontal plane, i use a combination of the National Map viewer (http://viewer.nationalmap.gov/viewer/) where you can select and view the availability of 1/9 arc-second data across the US.  Here's a screenshot of the availability:\n\nLIDAR collection is ongoing at the state and federal level (search USGS 3DEP program) but won't be complete for years.  Individual states can sometimes provide higher resolution LIDAR or DEM data, and most have GIS clearinghouses.  NY State's: https://gis.ny.gov/   Image of LIDAR availability at present for NY State:\n\nAreal Imagery\nThe National Map viewer can also display the 1-foot aerial imagery coverage:\n\nAnd and example of NY State's coverage:\n\nLand Use\nOther than the NLCD data set, which is too course for your purposes, I only know about the USFS Urban Tree Canopy assessment program (look it up, i can't post more links cause of my reputation).  Here's a snapshot of what that data can look like:\n\nThe bottom panel is what I think they create.\nDoesn't appear to be readily accessible; may have to request downloads.  Only focused on urban/metropolitan areas.  May provide the plant ground cover data you are looking for.  You could also use the (non-spatial I think) Forest Inventory Analysis data from the USFS (http://www.fia.fs.fed.us/tools-data/), which provides species information, I think, and randomly assign it to different locations within either the UTC maps or other maps?  Just a thought.\nEdit: Just noticed the Philadelphia imagery is 4 inches!!  I think this is a link to it, from checking out the metadata: http://www.pasda.psu.edu/uci/MetadataDisplay.aspx?entry=PASDA&file=PhiladelphiaCityMosaic2010.xml&dataset=1040\nAlso, now you have a link to the PA GIS clearinghouse.\nGround level photography?\nThe google street view API?  Have no idea if that would be useful for you.\nHope this helps.\n", "data request - World city database (with longitude, latitude) and population per year": "\nThe United Nation Statistics Division publishes population totals and by demographics per country on an annual basis. This is called the UN Demographic Yearbook. It is normally in PDF format, but there are various areas on the unstats.un.org site that you can download EXCEL and CSV files.\nA good start is here. This has downloadable tables between 2007 and present.\nhttp://unstats.un.org/unsd/demographic/products/dyb/dybcensusdata.htm\n", "data request - Programming titles": "\nI would take a short list of programming langauges and then search those terms on a job website.\nFor example, searching indeed.com for 'python' returns job titles of 'Data Developer', 'Build Automation Engineer', 'Software Engineer - Data Infrastructure', and 31502 more.\nI mention indeed.com because they have an API that you can use code to perform searches - LINK. You have to register to get the key, but it's free.\nOnce you've collected the thousands of job titles (and summary, descriptions, location, etc) for each programming language, you can do some basic stats to see which job titles are showing up most commonly (maybe the top 20 or something).\n", "data request - List of pop music genres, with sound samples, easy to download, open": "\nI'm afraid the curation of a dataset like this is an expensive proposition, especially if you take into account the complexity of copyright.  I certainly think it's optimistic to hope that it's already assembled into the format you describe.\nThe Free Music Archive (FMA) is an extensive source of free audio files, and it has 15 genre tags (including spoken). You'd be on your own to produce clips. This page on their site also has links to articles about the FMA, some of which sound as though they might also cover other free music resources.\nOtherwise, I'd suggest a literature review of academic study of music genre. Maybe a paper will provide a lead on a resource like you describe that is not as readily found by Google directly.\n", "data request - open database of elementary, middle, and high schools in Latin America, Africa and Asia?": "\nThe Education Policy and Data Center (EPDC) collects and summaries education related data from 200 countries in the world. While I don't believe they publish school location data, they do publish the sources of their data, which include link's to the countries dept. of education, census/demographic data, etc.\nhttp://www.epdc.org/about-help/data-sources\n", "tool request - Search engine for graphs where we can specify the axis we are looking for": "\nAlthough not strictly open data, the in my experience most fruitful way to search for something like this is:\n\nSearch for the raw data (timeseries, tables, values) in studies/statistics oneself and prepare the graph oneself or\ndo a simple Google image search like graph hdd storage price.\n\n", "machine learning - Free data of connections between role and skill": "\nThere are two free data sources that you can use, albeit both are pretty noisy and the job title is a part of a free text field, not an enumerated one. \nThe first is crossing the user profile page and tagged posts from the Stackoverflow public dump. See Stackoverflow downloads from the internet archive and \nDatabase schema documentation for the public data dump.\nThe second is scrapping the LinkedIn public API for profile properties. These include both job descriptions and skills. Contrary to the first, you may have to wait for a while, because of usage limitations.\n", "data request - Pokerstars hands database": "\nAs I know, buying hands are illegal for several poker sites. However, there are a few free databases that probably will help you.\n1) University of Alberta A database with more than 10 million of poker hands\n2) HandHQ.com A 70GB database of poker hands.\n", "usa - What would be the cleanest / easiest method of collecting rental property data?": "\nHUD offers a lot of rent related information (by County). For example, the 50th percentile estimates contain the medium rent per studio, 1, 2 and 3 bedrooms per county.\nhttp://www.huduser.org/portal/datasets/pis.html\nI also keep a copy, converted to our linked CSV vocabulary, of some of these datasets here:\nhttp://www.opengeocode.org/cude1.1/HUD/PHA/index.php\n", "data request - Dataset for Named Entity Recognition on Informal Text": "\nAlthough the entity set is more restricted than you are looking for, the following might be useful:\nhttps://github.com/sandeepAshwini/TwitterMovieData\nThe data is referenced in the following paper by Ashwini and Choi, which discusses and evaluates the general approach: http://arxiv.org/abs/1408.0782\n", "Multiple attributes available in a single OpenFDA query": "\nI think your question is very similar to this one: OpenFDA API : can we count on several fields? but here are some additional thoughts for you:\nThe short answer, as you will see in that question's answers and the asker's own edit, is that no, this is not currently possible in the openFDA API. You have to do multiple API calls which is how we* do it on ResearchAE.com. I can't speak about how they do it on the openFDA site though but if you take a screenshot or send a URL of what you're talking about I can try and dig into their site's code and let you know how they are doing it (the source code for open.fda.gov is on Github for anyone to see).\nFull disclosure: my business partner and I built ResearchAE.com and presented about it and the power of the openFDA API at Health Datapalooza alongside the openFDA team.\n", "data request - Dataset on donations to charities?": "\nI am not sure if this is something that is required to be reported. \nData like does this does exist for campaign/PAC contributions and lobbying money spent by corporations. The Sunlight Foundation has a great site called InflueceExplorer.com (which has a web UI, bulk downloads, and API). For example, a search for Microsoft.\nI also heard from a contact well entrenched in the NPO space that major donors might be shown on NPOs' Form 990 that they are required to file with the IRS but I am not seeing it on the sample 990 at http://www.irs.gov/pub/irs-pdf/f990.pdf. You can contact and look at data from 990++ aggregators such as GuideStar, the National Center for Charitable Statistics, and CitizenAudit.\nEDIT: Found this useful FAQ: \"Q: Where can I find out who has donated money to a particular nonprofit organization?\". Short answer: \"The list of donors filed with Form 990 is specifically excluded from the information available for public inspection, except for donors to private foundations and political organizations.\"\n", "data request - Dataset of football (soccer) penalties": "\nI think FIFA used to keep this level of details on its archived statistics page. But now when you go to the url (http://www.fifa.com/worldcup/archive/) you get a blank page! I found this reference from The Guardian that aggregated the statistics per team going back to 1930 thru 2006 and made available to download as a spreadsheet. It does not have individual player stats:\nhttps://docs.google.com/spreadsheet/ccc?key=0AgdO92JOXxAOdGtRLThiUUhYSnhackhXVm9qbm5aQ0E#gid=0\nIf somebody can find copies of the archive, Brenna Curley, Iowa State University, wrote a paper (April 2012) for analyzing the dataset using R.\nhttp://www.public.iastate.edu/~curleyb/Stat585_Project_FinalPDF.pdf\n", "data request - Where can I find sales figures on pharmaceutical drugs listed by manufacturer?": "\nThis is not open data but from my experience looking for data like this for quite a while even on a site with a lot of terms and conditions, the following is a relatively robust source:\nhttp://www.drugs.com/stats/top100/units has sales and unit figures for the top 100 drugs of each quarter going back to 2011 and then top 200 from 2003 through 2010. The source is listed as IMS Health (Midas). \nHere are three Quora answers that were somewhat helpful to me while researching IMS' offerings a little bit further:\n\nhttps://www.quora.com/IMS-Health/How-much-does-IMS-data-cost\nhttps://www.quora.com/IMS-Health/How-is-a-contract-with-IMS-Health-structured-How-does-it-charge-pharma-companies-100M+-each-year\nhttps://www.quora.com/IMS-Health/How-does-IMS-Health-get-their-prescrption-data\n\n\nIn terms of actual open data, the best I can find is usage by prescription drug class from the CDC: http://www.cdc.gov/nchs/hus/contents2012.htm#092 . It is table 92 ('Selected prescription drug classes used in the past 30 days, by sex and age: United States, selected years 1988-1994 through 2007-2010') of Health, United States, \"an annual report on trends in health statistics\"\n", "Where is data on stable currents of world ocean?": "\nI am by no means a subject matter expert on the type of data you are looking for but am good at finding open data :)\nHave you tried http://www.oscar.noaa.gov/datadisplay/oscar_datadownload.php?pagetype=nonjava by any chance? Data comes down as a NetCDF file but there are file readers for different operating systems (I used Panoply for Mac) and then you can export to CDL which could be parsed by any number of programming languages. \nNOAA has other ocean current data sets and information at http://www.nodc.noaa.gov/General/current.html too\nIs this on the right track for the data you're looking for?\n", "data request - Fortune 500 CEOs/executive boards": "\n\nFor publicly traded companies, Google Finance or another similar site will list the officers and directors. For example, GOOG: https://www.google.com/finance?q=google\nFor private companies, I have found investing.businessweek.com to be a good source. For example, Socrata: http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=41354884\n\nEither of these could be screen-scraped within reason or maybe they even have APIs.\n", "openFDA adverse reaction Serious vs Expedited?": "\n\nFor serious values mappings, see this similar SE question: Seriousness values on OpenFDA. The short answer is (quoting Sean Herron):\n\n1 = The adverse event resulted in death, a life threatening condition, hospitalization, disability, congenital anomali, or other serious condition.\n2 = The adverse event did not result in any of the above.\n\nFor fulfillexpeditecriteria and other variables you may come across, I suggest referencing the PDF both Sean and I mentioned in the linked SE question: http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/UCM350390.pdf . That document is not specific to OpenFDA but instead the source system upstream from the API OpenFDA exposes. The short answer is (quoting the PDF):\n\nValue is 1 (1=yes) for identified expedited reports (15-day); 2 for all others. \n\n\nEDIT: You can see more information on the fields at http://www.fda.gov/downloads/Drugs/DevelopmentApprovalProcess/FormsSubmissionRequirements/ElectronicSubmissions/UCM149932.pdf as well. For example, the following for fulfillexpeditedcriteria:\n\nDoes this case fulfill the local criteria for an expedited report?\n\nBe sure to remember that different countries might have different standards\n", "data request - Song lyrics and timings": "\nSheet music and MIDI comes to mind. You could use XML versions of sheet music to get the timing... http://www.musicxml.com/music-in-musicxml/\nYou need to add personal touches to music  composition to make it soulful. Otherwise, it can come off as being very cold.\nYou could also look at music videos with closed caption or subtitles as a way to extract lyrics and timing.\n", "analysis - Examples of useful applications that are being developed using open data": "\nI'm so glad you bring up this question. Please take a look at the following links as well:\n\nhttp://www.opendata500.com/\nhttp://www.datainnovation.org/\nhttp://www.socrata.com/products/custom-web-and-mobile-apps-government-data/ (scroll down to 'Featured Apps')\nhttp://www.pinterest.com/socrata/open-data-applications/\n\n", "data request - Scraper for Openstreetmap: all south-american schools to mysql-db": "\nHere is some information on how to get closer to what you might be looking for.\nOne well-organized source for information like this is a site called GeoNames.org. GeoNames has a number of APIs as well as downloads (\"dumps\") which are discussed on their export page.\nFor your use case, an export seems to make sense. Each country's POIs (points of interest) are downloadable in .zip frhttp://download.geonames.org/export/dump/. Each includes a README file describing the data. The data should be able to be opened in a spreadsheet application such as Microsoft Excel or Apple's iWork Numbers and similarly imported into database and analytics engines such as MySQL, MongoDB, MS Access, Tableau, and much much more.\nIf you're interested in only schools, you will want to determine which GoeNames feature codes you are interested in and only import those into your database/collection. These codes/classes correspond to the text files' seventh and eighth columns as documented in the README file under the heading \"The main 'geoname' table has the following fields :\"\nI imagine someone may be willing to write a program to download the exports for each South American country and import only the schools (note there are several different type of codes which map to schools depending on your definition of what a school is) into a MySQL server or just CSV file. To make it easier, I would suggest you define what exactly you're looking for and maybe attach a bounty to it.\nHope this helps!\n", "data request - Rocket attacks dataset in Israel and State of Palestine": "\nTry the Global Terrorism Database from the National Consortium for the Study of Terrorism and Responses to Terrorism (START) project at at the University of Maryland. Here's a brief snip from their about page\n\nThe Global Terrorism Database (GTD) is an open-source database\n  including information on terrorist events around the world from 1970\n  through 2012 (with additional annual updates planned for the future).\n  Unlike many other event databases, the GTD includes systematic data on\n  domestic as well as transnational and international terrorist\n  incidents that have occurred during this time period and now includes\n  more than 113,000 cases. For each GTD incident, information is\n  available on the date and location of the incident, the weapons used\n  and nature of the target, the number of casualties, and--when\n  identifiable--the group or individual responsible.\n\nThe GTD database can be downloaded after filling out a form at http://apps.start.umd.edu/gtd/contact/ -- so \"kind of open data.\"\nSTART got the contract to collect terrorism data for the US State Dep.\nafter WITS: the Worldwide Incidents Tracking System was discontinued in 2012.\nSome of the original WITS website is in the Wayback Machine This page of reports has a link for \"exports\" at the top, but they didn't seem to work for me.\nAlso, I found the RAND Database of Worldwide Terrorism Incidents, although that only covers 1968-2009 according to their database scope page.\n", "data request - Natural hazards in California - Historic time series, spatial resolution (lat/long) & Climate Model Forecasts": "\nCheck out the content at Data Basin, which has a lot of California data.\n", "data request - Colors of political parties": "\nHere is a list for UK political parties - (link).\n\nBy doing similar searches, you can find other lists hosted on Wikipedia.\nhttp://en.wikipedia.org/wiki/Wikipedia:Index_of_Minnesota_political_parties_meta_attributes\n\n\nI think with a few of these lists you can make a joined list of major parties. Because there are so many parties, the colors will start to overlap and be indistinguishable, so I'd stick to just a few main parties.\n", "geospatial - Seeking Water Quality Data for Lake Ontario that includes Dissolved Oxygen, Nitrogen, Phosphorus?": "\nThe only data I could find on some of these parameters is from http://ontario.ca's 'Provincial (Stream) Water Quality Monitoring Network' which can be accessed at http://www.ontario.ca/environment-and-energy/provincial-stream-water-quality-monitoring-network .\nThe raw data is available from http://www.ontario.ca/environment-and-energy/provincial-stream-water-quality-monitoring-network-pwqmn-data in MS Access, Shapefile, and MS Excel formats where each row represents a sensor readings which include the following sensor descriptions:\n\nPHOSPHORUS,UNFILTERED TOTAL\nNITROGEN,TOT,KJELDAHL/UNF.REA\nDISSOLVED OXYGEN\n\nHope this helps!\n", "data request - Standardized tests questions databases": "\nThanks @Unihedron\nNational Center for Education Statistics releases a subset of questions.\nI would not call this truly open, but it is a start.\n", "openfda - \"hypotension\" due to carbamazepine in open FDA?": "\nOne way to query for that is with the following URL:\nhttps://api.fda.gov/drug/event.json?search=patient.drug.openfda.generic_name:carbamazepine%20AND%20patient.reaction.reactionmeddrapt:hypotension&limit=10\nIf you are looking for a graphical representation, you can use a tool my company developed which is completely free and shows this information in a web page at the following URL:\nhttp://www.researchae.com/adverseevent?from_date=1900-01-01&to_date=2014-07-16&from_age=&to_age=&country=GLOBALLY&patientsex=&manufacturername=&drugbrandname=&druggenericname=carbamazepine&medicinalproduct=&reactionmeddrapt=hypotension&drugclass=&drugindication=&indsubmit=&productndc=&safetyreportid=\n", "sports - Any open public data sets for the English Premier League (EPL)?": "\nThere is a list of soccer datasets and APIs here from this related SO question. Hope that helps.\nEDIT: Quoted text from the article linked above, as requested:\n\nopenfootball has started a free (open source) public domain football\n  database. The data is historical data, meaning no lives scores but the\n  data does include the schedule, teams and players for the upcoming\n  2014 World Cup along with global league data. This is a very promising\n  project and has the potential to be the definitive source for\n  historical data for the public. The data is stored in various repos on\n  github. Start browsing and contributing at github.com/openfootball.\n  See the opensport Google Group for discussion and questions.\nfootballsquads.co.uk has current and historical squad details for\n  clubs and national teams from all across the world for many leagues\n  and competitions, including the 2014 World Cup squads.\nRec.Sport.Soccer Statistics Foundation (RSSSF) has massive collection\n  of formatted plain text statistics. An example of English Premier\n  leagues results.\nESPN API has an API for registered users (free). You can get a list of\n  all the players in the EPL. However they are very limited in their\n  data. They restrict all fixtures and scores to \u201cstrategic partners.\u201d\n  However, you can get lists of players and teams.\nopta Playground has a developer program that provides very limited\n  access to historical data. The site reads \u201cOpta can provide data for\n  programmers wishing to develop a mobile app or website with selected\n  historical data available to download.\u201d You have to request permission\n  in an email. I applied and they sent me the xml data set for 10 rounds\n  of games from the start of the 2007/2008 Bundesliga 2. The more\n  detailed game data had either x,y coordinates of game events. A very\n  impressive dataset but it felt more like an advertisement. The data\n  provided I had no interest in and I\u2019m not sure why an indie developer\n  would spend time working on a data set they could never afford.\nStatsFC used to have an restful JSON API of all EPL scores and\n  fixtures. It was about $8 us dollars a month but was recently shut\n  down. There is no doubt it was related to data rights. See their\n  official statement.\nCrowdScores beta is UK company trying to crowd-source the football\n  data collection process. You sign up for an account and report game\n  events to their servers. They have web/iphone/android interfaces for\n  reporting. They reward the top reporter with a season ticket. They\n  data collection process is ideal but they might have to work on the\n  incentives. I believe a better incentive would be to allow the\n  reporters who contribute access to an API of all the data collected.\nopenfooty API had promising API documentation but a quick look at the\n  developer forums shows a stale community and questions about why no\n  one seems to actually be able to get a developer key.\nfootball-data.co.uk has made a lot of historical league data available\n  as csv files. The data includes results and a lot of betting/odds\n  related data. I have tried to aggregate and clean up the data in the\n  following repo github.com/jokecamp/FootballData\nwww.european-football-statistics.co.uk is a visually dated website but\n  has a lot of historical football data (mostly an overview of\n  league/tournament results) displayed in nice clean HTML tables. Looks\n  like they already have 2014 EPL stats.\nopenligadb.db has an old-school windows asmx web service with methods\n  such as \u201cGetGoalsByMatch()\u201d\nLinked Soccer Data is a white paper on one group\u2019s attempt to \u201ccreate\n  a dataset including reliable information about soccer events covering\n  as many historical data as available including recent competition\n  results.\u201d Some dead links but worthwhile to skim.\n\n", "legal - Web crawling to create a business": "\nIn the UK the Computer Misuse Act from 1990 (section 1) states that it is an offence to access a computer where:\n\nthe access to the data is unauthorised\nthe offender knows that it is unauthorised \n\nThis may extend to web scraping because, in some circumstances, you are accessing the website intending to use the data in a way that is not authorised by the website owner.\nThus, the conservative and recommended way is to ask the owner for permission.\n", "Are patient ages in openFDA specific to months of age (for infants) or only age in years?": "\n\nFor patient age in months, yes, it is possible but might not be as easy as you'd like it to be. As you will see from https://api.fda.gov/drug/event.json?search=receivedate:[1900-01-01+TO+3000-01-01]&count=patientonsetageunit (and referencing the values for patientonsetageunit from https://open.fda.gov/drug/event/reference/, the majority of reports in the database have ages defined in years but 40,000 or so have an age unit in months, weeks, days, or hours. You could, of course, convert those non-month units into months.\nIn terms of gestational age, I do not believe that data is available in the public data files which openFDA is built off of.\n\n\nUse caution no matter what you end up doing. It looks like 46.19% of the 3,643,453 reports in the openFDA API are missing an age\nHope this helps!\n", "releasing data - Datasets for smaller towns or villages": "\nI was a Town Commissioner for 6 years of a small town (population ~640, but we're also the County Seat, so our daytime population is over 3000; exact number dependent upon how many people have jury duty that day)\nDue to the limited staffing, I'd recommend that small municipalities only collect the data that they're being asked for or obligated to provide, and not attempt to guess what data might be useful to people in advance.\nIn our monthly newsletter, we list crime statistics, breaking them down by general category and if they're in a residential area or downtown.  Originally, we just broke them down by type, but then someone started asking if it was downtown or in one of the neighborhoods.\nWe report our income and expenditures on a monthly basis, based on the categories that are defined in our detailed budget, along with the status of all cash accounts (amount in the accounts and status of collatoralization agreements).\nDue to our small size, we've had to take a number of measures to mask some data.  For example, when we moved towards being self-insured for health insurance, we consolidated line items that had been in each of the departments.  The reason being that we had a department with only one full time person, and so if any money came out of that item, we felt it may leak too much information about that employee's health.  We also have each employee as a separate line item, so it's possible for anyone looking at the budget to determine an employee's pay.  It's doubtful that a larger municipalities would have these issues to be concerned with.\n", "tool request - Open data & perl": "\nI don't know if there are any specifically in this field ... however, as catalogs go, there are Koha and Evergreen, which are ILS systems (Integrated Library Systems).\nIt might be possible to modify them for use in data cataloging ... you'd likely need to change out the underlying schemas, as I assume they'd be using MARC.\n", "data request - French equivalent of the brown corpus": "\nVia Twitter, I asked a friend with expertise in computational linguistics and French. She stated that the French Treebank is the largest tagged corpus. Since both it and the Brown corpus are described as about 1M words, I don't think you'll find another French one which meets your final condition.\nIt also seems, if I understand correctly, that the French Treebank is all sourced from Le Monde, so even it fails the second condition (\"various sources\").\nUltimately, her question was why the French Treebank can't be licensed, as the answer might help to make a better recommendation.\n", "openfda - Dietary Supplement data": "\nI do not have an official answer (I am just a community member passionate about the project)\nDietary Supplements - YES\nIf you search for a recent dietary supplement report from http://www.fda.gov/ForConsumers/ConsumerUpdates/ucm153239.htm on the https://api.fda.gov/drug/enforcement.json API endpoint, you do get results. Note this is the drug enforcement report endpoint, not the food enforcement report endpoint.\nFor example: https://api.fda.gov/drug/enforcement.json?search=bee%20pollen&limit=1 matches the most recent update from the aforementioned FDA link (http://www.fda.gov/ForConsumers/ConsumerUpdates/ucm401676.htm).\n\nVeterinary Drugs - NO\nAnimal and veterinary recalls and market withdrawals appear to be listed at http://www.fda.gov/AnimalVeterinary/SafetyHealth/RecallsWithdrawals/default.htm. When I do a search on the food and drug enforcement report API endpoints, I don't get any results.\nWhen I do a search on the device enforcement report endpoint, I get some results when I search  keywords for recent animal drug recalls... however they have older dates so they must be previous ones. Bottom line: doesn't seem like recent animal/veterinary drug recalls are in any openFDA at this time.\n", "openfda - Data about the safety of Da Vinci Robotic Surgery": "\nJerry,\nWhile I am not comfortable offering advice on the saftey of a device I will offer a method in which you can do your own research leveraging the OpenFDA API. Here is a search of medical device recalls for the term \"Da Vinci\" - http://www.researchae.com/recalls?reporttype=device&from_date=2004-01-01&to_date=2014-11-30&search=Da+Vinci\n", "CMIS compatibility: TCK reports data for all ECM products": "\nI found nothing so I created one: http://cmissync.org/CmisCompat\nIt contains for each ECM server:\n\nThe raw TCK test data\nSome metrics\nA calculated score\n\n\nCreative Commons CC-BY-SA.\nData is hosted on Github, so collaboration is via pull requests.\n", "data request - List of programming languages": "\nOhloh is the worlds largest tracker of open source projects, it compares activity level as well as programming languages by lines of code, might be useful if your interested in use stats and trends. https://www.ohloh.net/tools\n", "web crawling - How to crawl data from a library website": "\nFirst, you should check for any license for using the site to see if you can use their data for any reason (commercial and non-commercial).\nSecond, you should read the terms of use and find anything about scraping website. Many websites don't allow scraping.\nThird, if non of the above is a problem, you should use a programming language to crawl and scrap the content from the website. Do you have any knowledge of any programming language?\n", "data request - Where can I get historic prices for a commodity?": "\nIndex Mundi has a lot of commodity data (present and historical) available to the public.\nBut I am not sure if you would find toilet paper on the list (lol) - it does not exactly fit the definition for a commodity:\n\"a raw material or primary agricultural product that can be bought and sold, such as copper or coffee.\"\nhttp://www.indexmundi.com/commodities/\n", "data request - Is there any dataset for problems common people are facing to build apps against?": "\nIt's generally difficult to build applications to scratch someone else's itch without having a vested interest in it (such as getting paid).\nYou're typically better off trying to find something that bothers you, and figure out how to solve or mitigate it.\nIf you really can't think of anything, there are hackathons and 'data jams' where they bring together people w/ programming skills & data experts to try to come up with uses for the data:\n\nNational Day of Civic Hacking\nCapital Code Data Jam (MN)\n\nYou may also have local groups working on projects that you can get involved with.  Some meet virtually, others get together for regular 'hacknight' events, instead of it just being an annual or non-reoccuring thing:\n\nCode For America\nCode For DC Hacknight\nRandom Hacks of Kindness\n\nIf you're a student, there are various 'summer of code' projects, where various projects solicit for things they need done, and students can get stipends to work on them.  (the other still needs doing, so if you're willing to volunteer, there's likely still work they need doing).  Here are a few, although there are other groups with similar programs:\n\nGoogle Summer of Code\nRails Girls Summer of Code\nESA Summer of Code in Space\n\nAnd tomorrow & Wednesday is the Mozilla Science Lab's Summer Code Sprint.\n... and these are just a sampling of all of the various groups out there -- there's a whole lot more.  You can also reach out to various community groups and see if they have need for some programming help.\n", "data request - Undirected graph datasets with node attributes?": "\nHere is short list of attributed networks I know:\n\nDBLP co-author\nSocioPatterns\n\nI'm new user restricted to two links...\nYou can also search for KDD2012 tencent weibo dataset.\n", "data request - Streets of Mauritius": "\nThey have a map (Google tiles) with waypoints but no additional road data: gov.mu/English/Map/Pages/default.aspx I didn't see a data download link on the page, but I'll keep digging.\nI did find that their police utilize services from SuperMap (which is not open source/data): http://www.supermap.com/en/html/solutions722161.html\nAnd for the sake of context, here is a paper discussing the need for more GIS in Mauritius not too long ago: http://www.ncgia.ucsb.edu/conf/gishe97/program_files/papers/beedasy.html\n", "programming - How do I access data from 3taps API in C#?": "\nI would suggest learning about what an API is from 'An Introduction to APIs' by Zapier and then some more specific information about consuming JSON APIs w/ .NET from 'Making JSON Web APIs with ASP.NET MVC 4 Beta and ASP.NET Web API'\nUnirest is a simple HTTP request client to look into as well.\n", "data request - Database of predominant religion by country?": "\nThe Factbook includes a religon entry, for example, the entry for Canada reads:\nRoman Catholic 42.6%, Protestant 23.3% (United Church 9.5%, Anglican 6.8%, Baptist 2.4%, Lutheran 2%), other Christian 4.4%, Muslim 1.9%, other and unspecified 11.8%, none 16% (2001 census)\nYou can get all the data in the public domain in JSON from openmundi/factbook.json for example. Canada example in JSON:\n\n \"religions\": {\n      \"text\": \"Roman Catholic 42.6%, Protestant 23.3% (United Church 9.5%, Anglican 6.8%, Baptist 2.4%, Lutheran 2%), other Christian 4.4%, Muslim 1.9%, other and unspecified 11.8%, none 16% (2001 census)\"\n    }\n\n", "data request - Database of neighborhood between countries?": "\nThe Factbook includes a land border entry, for example, the entry for Burkina Faso reads:\n3,193 km - Benin 306 km, Cote d'Ivoire 584 km, Ghana 549 km, Mali 1,000 km, Niger 628 km, Togo 126 km\nYou can get all the data in the public domain in JSON from openmundi/factbook.json for example. Burkina Faso example in JSON:\n\n\"land_boundaries\": {\n      \"total\": \"3,193 km\",\n      \"border_countries\": \"Benin 306 km, Cote d'Ivoire 584 km, Ghana 549 km, Mali 1,000 km, Niger 628 km, Togo 126 km\"\n    }\n\n", "data.gov - OpenFDA API for drug Label changes": "\nAs of right now, SPL (structured product labels) are not part of the openFDA list of APIs for drugs (https://open.fda.gov/drug/event/ - note how \"product labels\" is greyed out). You can learn more about the SPL standard, guidance, etc. directly from the FDA at http://www.fda.gov/ForIndustry/DataStandards/StructuredProductLabeling/default.htm.\n\nWhile you wait for the openFDA project to serve SPLs, there is a robust (and actively improved - latest update was February 2014) website (with web services!) maintained by HHS/NIH/NLM called DailyMed. Information on DailyMed web services can be found at http://dailymed.nlm.nih.gov/dailymed/help.cfm#webservices .\nFrom my intermediate knowledge of SPLs, they are versioned by a unique GUID called \"set IDs\" as well as an addition \"SPL version\". There is an API endpoint for getting the history of a specific SPL Set ID as well as an API endpoint for finding the drug record (including set ID) in the first place.\nAlso note that the DailyMed site maintains an RSS feed for each set ID (such as http://dailymed.nlm.nih.gov/dailymed/labelrss.cfm?setid=c5fdde91-1989-4dd2-9129-4f3323ea2962) as well as an RSS feed for all changes in the past 7 days (http://dailymed.nlm.nih.gov/dailymed/rss.cfm).\nHope this helps!\n", "Delay in openFDA's drug recalls/enforcement reports?": "\nAs has been updated on the github issue (https://github.com/FDA/openfda/issues/24), we're now updating this endpoint bi-weekly.\n", "data request - List of all universities by country": "\nWikipedia's sister project Wikidata provides data about more than 18,000 universities with a varying amount of detail.\nYou can use the Wikidata Query Builder to generate the following SPARQL query:\nSELECT DISTINCT ?item ?itemLabel WHERE {\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n  {\n    SELECT DISTINCT ?item WHERE {\n      ?item p:P31 ?statement0.\n      ?statement0 (ps:P31/(wdt:P279*)) wd:Q3918.\n    }\n  }\n}\n\nThis returns a list of all instances (P31) of university (Q3918) and any of its subclasses (P279) (such as institute of technology or law school).\nBy slightly tweaking the above query, you can even generate an interactive tree view of all countries and universities backed by live data from Wikidata.\n", "api - seriousnesscongenitalanomaly in openFDA Adverse Events": "\nThanks for the complement on our ResearchAE.com project. Glad you found it useful and as an example of what people can do with the openFDA data.\nTo answer your question, I believe the query you mean to be doing is this:\nhttps://api.fda.gov/drug/event.json?search=_exists_:seriousnesscongenitalanomali&limit=5\n\nClickable link for that openFDA query: https://api.fda.gov/drug/event.json?search=exists:seriousnesscongenitalanomali&limit=5\n", "data request - How can I retrieve a list of companies that deal in a specific field?": "\nYou can get a list of UK registered limited companies from Companies House that also contains the SIC Codes for each company.\nThese aren't always that informative, but they provide a starting point?\n", "europe - Open company data for the Netherlands": "\nopenkvk is a project to provide Dutch company data. The project has been running since 2009 and there's a mature API.\n", "government - Where can I find column definitions for ed.gov data?": "\nDid you download the dataset directly off of http://www.ed.gov/developers ? If so, a lot of metadata and data discussion can be found in the PDF file linked to the right of the orange CSV, JSON, XML, and API links. Attaching a screenshot below.\nEDIT 1: One more trick -- if you are using the API endpoint, switch rows.json in that URL to columns.json and you'll get some more information such as what the name of the column is as opposed to the lowercase/parameterized fieldName but the PDF should still have more information and explanation.\nDoes this help? If not, which data set are you referring to?\n\n", "economics - How to estimate subjective value?": "\nThis data-request is very interesting but not trivial at all. (Hence, the somewhat late answer.)\nWhat you refer to as subjective price, economists would often call reservation price. The reservation price is the maximal price a person would pay for a good, or put differently, her valuation for that good. As you rightly observed when you called it the subjective price, the reservation price differs from person to person.\nThis is where the difficulty starts. If a house is sold at 100,000$, for example, we do not know any person's reservation price. The person who bought the house might well have paid more if needed. Conversely, for the people who did not buy the house it is likely that their reservation price was below 100,000$ (as the seller would have accepted this price) but we cannot say what their exact reservation price is.\nThis being said, in the market for houses there are some tricks, you can use to get more information about people's reservation prices. Ideally, you could look at auction data. Auctions have the advantage that a lot of people make bids and thus, there will be more than one person that will at some point give away information about her reservation price. If person A buys the house for 100,000$ but before B has made a bid of 95,000$, you know that B's reservation price must lie somewhere between both numbers.\nBut it gets even better: for you data request it will be ideal to look at Vickrey Auctions (also called second-price sealed bid auctions). These are auctions where everyone gives a sealed bid, the highest bidder wins but only pays the second highest price. This is similar to an auction of ebay where the winner also pays only the second highest bid (and not her own) and where bids are essentially sealed if everyone votes in the last second. Standard economic auction theory shows that every person should truthfully report her reservation price in a Vickrey Auction. Thus, if you simply look at the bids people made in such an auction you will get a very good picture of people's subjective value.\nSo for your data request, my suggestion is that you look at the bids given in Vickrey Auctions in the housing market. I am not too familiar with the housing market per se, but I assume that especially for very expensive houses, auctioning them off would be one way how they are usually sold. Otherwise, if you are not interested in subjective value for houses but in general, arts auctions might be a good point to start. Of course, such data is not publicly available (without asking) and I cannot provide you with any link but if you (or anyone else, since this is quite old) is really interested in the matter for research, writing to auction houses would be a good starting point.\n", "OpenFDA API: Can I perform search by same field with various values": "\nHere is the correct query syntax for an 'OR' search:\nhttps://api.fda.gov/drug/enforcement.json?search=classification:(\"Class+I\"+OR+\"Class+II\")\n\nI took a look and the result count equals the count of Class 1 + Class 2 so this should be what you were looking for.\nMore information on openFDA \"API Basics\" are at https://open.fda.gov/api/reference/#query-syntax though there was not a specific example for an OR search that had spaces in it.\n", "usa - IRS Codes in machine readable format": "\nIt's difficult to know when to use the Stack Exchange \"answer\" field for an inconclusive answer, but after a review of the IRS's website, I don't see much sign of this, particularly for the 1f code on that form, but really, for anything. The most prominent source of structured data from the IRS that I found is the Tax Stats section, which isn't what you're looking for.\nThere's an extensive section for \"e-file providers\", including some PDF docs that articulate valid code values, but nothing machine readable that I see. Maybe they make that sort of stuff available to approved e-file providers?\nNote that the 1099 form instructions you referenced are also available as HTML as well as PDF, although you'd still have to scrape free text.\nThis seems like a case where you are best off asking the IRS directly, both to get an authoritative answer, as well as to register public interest in more structured data from them. Also, while I know that tech folks are often inclined to simply fill out a form or send an email, this may be a case where a phone call would pay off. Sometimes (but definitely not always), a real-time human interaction helps cut to the chase, and can result in social referrals to people with better information or more access than you can find by impersonal electronic means. However, the IRS has not been very forthcoming with publishing machine readable data thus far.\n", "data request - How many Veterans have been or are currently in Congress": "\nVital Statistics on Congress from the Brookings Institute has some data about this available in Excel.\nThe \"Vital Statistics Chapter 1- Demographics of Members of Congress\" workbook has a worksheet, \"1-8\", which has the title \"Prior Occupations of Representatives, 83rd - 113th Congresses, 1953 - 2014\"  Veteran is one of the lines.\n", "openfda - Class 1 and 2 medical device recalls": "\nThe short answer is as follows (here is a clickable link though) which searches for recalls initiated during June 2012, that were classified as Class I or Class II, and had the word \"radiation\" somewhere in the recall documentation.\nhttps://api.fda.gov/device/enforcement.json?search=radiation+AND+classification:(\"Class+I\"+OR+\"Class+II\")+AND+recall_initiation_date:[2012-06-01+TO+2012-06-30]&limit=100 \nThere are a number of caveats to keep in mind though:\n\nThis query relies on \"radiation\" being included in one or more of the fields listed at https://open.fda.gov/device/enforcement/reference/ such as product_description but there is absolutely no guarantee radiation drugs will mention that keyword.\nThis query also relies on recall_initiation_date be present which appears to only be there for 59% of all the device enforcement reports available in the API (4639 of the 7855). Note that field report_date seems to have 100% coverage. Maybe the recall_initiation_date field is only available after a certain date? More digging needs to be done to make that call.\n\n", "data request - Product Reviews": "\namazon:\nThis dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review.\nhttp://snap.stanford.edu/data/web-Amazon.html\nbeer advocate reviews:\nhttp://snap.stanford.edu/data/web-BeerAdvocate.html\nrate beer reviews:\nhttp://snap.stanford.edu/data/web-RateBeer.html\nmovie reviews via rotten tomatoes:\nhttps://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n", "How to work with RDF formats": "\nRDF has many different serializations (or formats); there is no \"RDF format\".\nAs you say it\u2019s XML, it\u2019s most likely the RDF/XML serialization.\nYou should not use a usual XML parser to work with it; you\u2019ll want to work on the RDF triple level instead.\nSo you need an RDF-aware tool that can input RDF/XML; or you can convert your file from RDF/XML to a more human-readable format like Turtle, and use a tool that inputs this serialization.\nTo query the data, you\u2019ll want to use SPARQL.\nThere are many tools (Redland librdf (C), Jena (Java), \u2026), and I can\u2019t give a recommendation, but the sister site \nSoftware Recommendations might help.\nSee also a list of tools in W3C\u2019s Semantic Web wiki (also with links to other off-site lists).\n", "data request - Tagged addresses": "\nhttp://openaddresses.io/\nIt's an initiative to collect open (CC0) addresses around Unites States and globally.\nYou could transform these as you like..\n", "Data sets for evaluating identity resolution": "\nIf you are looking to generate large data sets and don't mind putting in a bit of work you can use the data set generator in febrl from the Australian National University (Project at http://sourceforge.net/projects/febrl/ and documentation for dataset generator http://cs.anu.edu.au/~Peter.Christen/Febrl/febrl-0.3/febrldoc-0.3/node70.html).\nIt requires that you give it a dictionary of terms for each field with frequencies and possible misspellings etc and then input the probabilities that this row has an error/is a duplicate etc. There are some dictionaries bundled but they are Australian based but they give you an idea of how to create your own.\nIt then generates a file with the original record and the duplicates which it identifies for you. Could be useful?\n", "best practice - Open data build project examples": "\nI started the open football project - that collects public domain football data (e.g. World Cup in Brazil, English Premier League, Bundesliga, Champions League etc.).  Nothing new other than having public domain (license-free, no rights reservied) datasets in plain old text.  \nWhat's different is that all tools and scripts including, of course, the build scripts are public domain (and open sourced) too and, of course, part of the project.  \nFor example, to build a single-file SQLite database e.g.  worldcup.db that includes all World Cups from 1930 to 2014 use the Ruby make tool, that is, rake. Example: \n$ rake build DATA=worldcup\n\nThat's it. Cheers.\nPS: The same \"system\" works, for sure, for other topics. See, the open beer and brewery project as another \"real-world\" example that includes build scripts that let you build \"The Free World Beer Book\" (e.g. $ rake book).\n", "Census API data across superior geographies": "\nThe answer to your first question, \"Is there a way to issue a request with just tract/blockgroup/block FIPS codes?\" is no, as you found in the documentation.\nThe answer to your second question, \"If not, could one be implemented?\" is not really a great question for a Stack Exchange site. I don't think Census Bureau folks are monitoring this site, although other open data advocates in the US Government are, so maybe I'm wrong, or maybe that will change and they'll start coming around. (and having them use OpenData.StackExchange.com would be a big gain over http://apiforum.ideascale.com/)\nAs someone who has been working on a project (Census Reporter) which includes a Census data API, handling API requests for fine-grained Census data is hard to get right and make it perform well. I know that Census Reporter really doesn't handle bulk data for block groups well. The slowest part of our app is calling for arbitrary geojson for maps on our block group profile pages (example).\nIn short, this is an example when an API is probably not the right way to deal with this open data.\nIf you can at all handle it, I suggest you look into simply grabbing the bulk data that you need and updating your GeoJSON to include it. It doesn't change that frequently, so making API calls for it is, as you observed, complicated for you, and also likely to be hard to make perform as well as you would like.\n", "Where can I find data about sharing propaganda music videos on Facebook?": "\nI doubt that there's an open data set for this, since it's a new subject and so many parts of it are changing rapidly. Also, it's not terribly clear what kind of data you're looking for, how it would be quantified, etc. \nI'd suggest looking for academic research in the field. Some of the researchers may have posted their data, or they may be open to requests for access.\nGilad Lotan has recently published some interesting stuff about the network connections.\nhttp://globalvoicesonline.org/2014/08/04/israel-gaza-war-data-the-art-of-personalizing-propaganda/\nhttps://medium.com/i-data/israel-gaza-war-data-a54969aeb23e\nAlso, I found this one about YouTube: The YouTube Jihadists: A Social Network Analysis of Al-Muhajiroun\u2019s Propaganda Campaign\n", "usa - National Scale (contiguous US) weather data set for 1980 - 2010": "\nthis set doesn't get to 2010 but does start 20 years prior (1960), and its gridded:\nhttp://www.columbia.edu/~ws2162/dailyData.html\n", "Where can I find data sets that have no API?": "\nAnother option would be to look at the datasets registered to a US government agency on Data.gov and then contract that list against known APIs from that agency.  \nSo - for instance, filter for datasets from the US Department of Transportation on Data.gov and compare that list against the APIs listed for the Department of Transportation on this page (Cmd/Ctrl+F for 'Federal Aviation Administration' and start there).  \nDisclaimer: I am Sr. API Strategist at the General Services Administration. \n", "data request - AIS (Automatic Identification System) or The Long-Range Identification and Tracking (LRIT)": "\nSome time ago I was searching for shipping graphs to try playing with routing, but I found a little.\nHere's some links:\n- http://geocommons.com/maps/109850 (contains a density on the arc)\n http://gizmodo.com/see-the-global-shipping-revolution-in-these-beautiful-o-1556851187 (links to a dataset I couldn't process)\nI wonder if there are complete dumps somewhere...\n", "data request - 2014 Ebola outbreak dataset": "\nIt sounds like you're looking for a line listing. There are no case data available for this outbreak, but like Skram said, I collect and maintain data from various sources on github. Sierra Leone and Liberia release some case data on the province/county level; Liberia's data is quite good. The WHO used to include town names for Guinea, but it has since stopped doing that. Another thing to be aware of is that the only dates for this outbreak are report dates. We have nothing on onset dates, hospitalization dates, death dates, etc. My contact info is on my github, feel free to write if you want to chat more. -Caitlin\n", "data request - Any dataset containing the price/charge that patients \"actually\" pay for their health care service?": "\nYou are looking for the variables in MEPS (the Medical Expenditure Panel Survey from AHRQ) with the text slf in them.  There are both utilization-level files as well as a consolidated person-level file available every year.\nhttp://www.asdfree.com/search/label/medical%20expenditure%20panel%20survey%20%28meps%29\n", "data request - Large list of quotes": "\nHave you seen WikiQuote by the Wikimedia Foundation?\nThere is an API endpoint at http://en.wikiquote.org/w/api.php which uses the standard MediaWiki API for there are API clients in many different languages.\n\nEDIT: Two WikiQuote API-specific links: https://stackoverflow.com/questions/13762688/wiki-quotes-api and http://bwgz57.wordpress.com/2013/02/14/in-search-of-quotes/\n", "standards - Does an Authoritative Definition of \"Dataset\" Exist?": "\nNo, there isn't.\nThe most in-depth analysis that I'm aware of is the 2011 paper by Renear, Sacchi and Wickett, \"Definitions of dataset in the scientific and technical literature\", in which they decided that there were four basic concepts that reoccurred (grouping, content, relatedness, purpose), but weren't completely consistent when dealing with different communities.\nThere have been other attempts to deal with the term 'data', which has significantly different meaning in the hard sciences, information science, and commputer science communities.  See for instance, Ballsun-Stanton's 2010 paper, \"Asking about data: Experimental philosophy of Information Technology\".\n...\nAs it's been so difficult to get fixed definitions, a few years ago, Todd King and I attempted to collect terms that we saw as problematic when attempting to talk about data systems with folks from other science disciplines.  As it's been stable for more than a year now, we should probably try to publish it.  (I wanted to provide cross walks to terms in OAIS and other vocabularies, so I'm the one holding it up).  We specifically gave up on trying to define 'dataset' :\n\nPlease note that the terms dataset, data product and data series are not defined here. Although all refer to a grouping of data granules, the terms are used inconsistently across disciplines; in solar physics, a dataset is a collection of data products while in earth sciences, a data product is either a collection of similar datasets or a classification of datasets. These terms should be avoided, or clearly defined when used.\n\nThere's also an effort going on right now through the Research Data Alliance as part of their Data Foundation and Terminology Workgroup to come up with an overall data model ... I haven't been following their work as much as I should (and I still need to look at the draft they sent me a couple of weeks ago), but they should be releasing something for their plenary meeting at the end of the month.\n", "companies - Open company data for Germany?": "\nAswath Damodaran, Professor at Stern School of Business at New York University, has been compiling information on major corporations since 1998. His EU dataset contains data on 6,000 EU public corporations, including those in Germany.\nhttp://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html\nQuandl has free datasets on current and historical stock prices for companies listed on the Frankfurt Stock Exchange:\nhttps://www.quandl.com/FSE\n", "parsing - Parse Wiktionary Data Dump XML Into MySQL Database": "\nI found some hints for the schema here https://meta.wikimedia.org/wiki/Help:Export#Export_format\nTo read manually the XML, try using a viewer like:\n\nhttp://www.readfileonline.com/\nhttp://www.swiftgear.com/ltfviewer/features.html (windows)\nhead -n [numberoflines] dump.xml (gnu/linux terminal)\n\n", "data request - Open replacement for cfbstats.com NCAA football CSV's": "\nthe wayback machine is your friend\nhttp://web.archive.org/web/20140128204311/http://www.cfbstats.com/blog/college-football-data/ \nEDIT:\nthis data is now hosted on open data se's datahub.io account:\nhttp://datahub.io/dataset/college-football-statistics-2005-2013\n", "data request - College student suicide dataset": "\nHere are two half-answers that hopefully can lead to a full one.\nIt seems a data set doesn't exist, with the exception of @Skram's comment about the WISQARS database.\n\nBut there are many journal papers that have done studies, and by browsing them you can find lists. It's unstructured, but you can structure the data with the fields you provided in your question.\nTo start, the SPRC has a page on college prevalence (LINK), with links to pages of research studies. Their white paper (PDF) talks about the Big Ten study being the most comprehensive. \n\nThe Big Ten Student Suicide Study (Silverman et al., 1997), undertaken from 1980\n  to 1990 to determine the suicide rate on Big Ten campuses, was the most comprehensive\n  report on the incidence of suicides in undergraduate and graduate school populations\n  by age, gender, and race. The study collected demographic and correlational data on\n  261 suicides of registered students at 12 Midwestern campuses.\n\n\nSilverman, M., Meyer. P., Sloane, F., Raffel, M., & Pratt, D. (1997). The Big Ten\nstudent suicide study. Suicide and Life Threatening Behavior, 27, 285\u2013303.\n\nAs expected, that paper is behind a firewall. But I think this is the kind of data that will require some digging. Emailing authors and asking if they are willing to share the data would be a good start.\n\nAnother option would be to read news stories, for example, with the NYTimes API. Suicides reported in the NYTimes would be either high-profile stories or local New York / New Jersey colleges. Perhaps a network of news APIs could 'grep' stories from across the country.\n", "data request - Amount of people in each job U.S": "\nYou're looking for data from the Bureau of Labor Statistics. Probably the Current Employment Statistics, which provide a breakdown of all employees by NAICS industry/sector.\n", "best practice - Extending North American Industry Classification System (NAICS)": "\nI don't know of any existing standards (de facto or not) for extending NAICS, but what I would do is start with the NAICS Index File, which ties over 19,000 industry names to the standard 1,000 or so NAICS codes. Take the list of index entries for each NAICS, give each one a 2-digit sequential ID, and tack this onto the standard NAICS to make it 8-digit (assuming no more than 100 index entries for any given NAICS).\nTo keep with the spirit of NAICS, underground or illicit activities should be coded using similar standard codes, or under new codes in the hierarchy close to legitimate businesses that use similar inputs or production processes. E.g., \"herbal\" grow operations would go in \"11: Crop Production\". Smuggling operations would be in \"48-49: Transportation and Warehousing\". You may also want to look at a humorous anecdote involving a BLS recommendation for the SIC coding of legalized prostitution in Nevada in the 1970s.\n", "federal - Is ISO 19115 (Geographic Metadata) a closed standard?": "\nShort answer: because ISO makes money selling the technical specifications. It's not closed the standard per se, but the specification (so if you're writing software you'd need the document.\nSearching on the net I found the first edition of the standard ftp://podaac.jpl.nasa.gov/misc/outgoing/ed/pre_2013/GHRSST_metadata/ISO%2019115%20.pdf\nAlso from NOAA some info http://www.ncddc.noaa.gov/metadata-standards/\n", "data request - Street gang dataset": "\nbureau of justice stats, but i'm not sure how detailed (to the level you want) this will be:\nhttp://www.bjs.gov/index.cfm?ty=tp&tid=36\nnot datasets but you can find stuff:\nhttp://www.fbi.gov/about-us/ten-years-after-the-fbi-since-9-11/just-the-facts-1/violent-gang-initiatives\nhttp://www.nationalgangcenter.gov/\npossible winner:\nhttp://www.nationalgangcenter.gov/About/Related-Web-Sites\ncriminal stats archive has alot too:\nhttp://www.icpsr.umich.edu/icpsrweb/NACJD/studies?archive=NACJD&q=gang&permit[0]=AVAILABLE&x=0&y=0\n", "metadata - Where can I find a taxonomy of open data sites?": "\nFirst Caveat, I am a co-founder of opengeocode.org\nWe have a few resources that might help you:\n\nCatalog of Open Data sites around the world\nSpecification for open data vocabulary\nDetailed specification on representing open data\nPapers related to Open Data\n\n", "Searching for a Mergers and Acquisitions (M&A) panel data set.": "\nThe effects of mergers: an international comparison (Gugler, Mueller, Yurtoglu, Zulehner) indicates that its \"principal source of data\" is the Global Mergers and Acquisitions database of Thompson (sic) Financial Securities Data (TFSD). Their \"data description\" (section 4) doesn't describe substantial follow-on work.\nStarting from that reference, it appears that the Thomson database is the most highly regarded. (See \"Sources for M&A Information\" from the University of Chicago library. U of C library also refers to MergerMarket, which has free league table \"data\" (in PDF form) from 2011 on their site. The PDFs are computer generated, so would probably lend themselves to data extraction with a tool like Tabula.\nOther papers I found described their methodology as something which sounded more labor-intensive, but perhaps if you contact authors, you might get more information about what they did and if they'll share it.\n", "metadata - Open Data about Open Data": "\nI'm not really sure that a StackExchange site is the best way to organize the kind of information you're looking for. I actually think a Wikipedia category is more appropriate and, indeed, one already exists. The CC-licensed data subcategory looks particularly useful and could be expanded.\n", "openfda - Is it possible to receive complaint count in monthly format?": "\nHere is a small Python snippet that takes your URL and produces a output that you can load into Excel.\nimport requests\nfrom collections import defaultdict\nmonthly = defaultdict(int)\n\nurl = 'https://api.fda.gov/drug/event.json?search=receivedate:[20110101+TO+20150101]+AND+brand_name:lantus&count=receivedate'\nr = requests.get(url)\nr = r.json()\ndata = r.get(u'results',u'')\n\nfor item in data:\n    count = item.get(u'count',u'')\n    ddate = item.get(u'time',u'')\n    if ddate != u'' and count != u'':\n        dkey = ''.join(ddate[0:4])+'-'+''.join(ddate[4:6])\n        monthly[dkey] += int(count)\n\nfor item in monthly:\n    print item, monthly.get(item)\n\nGives as an output:\n2011-08 569\n2011-09 505\n2011-02 432\n2011-03 462\n2011-01 439\n2011-06 560\n....\n\nWhich is in an Excel-importable format.\n", "data request - Solf\u00e8ge (do re mi fa sol la si do) sung by human voice": "\nIf you know how to make midi files, then you can use a vocal sound font (like choir sound fonts) to play that midi file. You can find a collection of choir sound fonts here.\nHere are the steps:\n\nMake a midi file with the software of your choice (I don't exactly know how to do that, so can't recommend any, but maybe you can try Aria Maestosa)\nDownload a choir sound font. Or you can just Google for it.\nInstall VLC media player and then install the font you downloaded. An instruction can be found here.\nPlay the midi file.\n\n", "Data over artisan bakery sales and earnings": "\nI haven't seen the detailed data you are looking for. But, there are several datasets of farmer's markets in the US. Some of them contain contact information and general categories of goods sold.\nThe USDA American Marketing Service (AMS) maintains a dataset of 8,200 farmers markets in the US: http://search.ams.usda.gov/farmersmarkets/ \nI have my own version of this dataset converted into a Linked CSV format: http://www.opengeocode.org/cude1.1/USDA/AMS/index.php\nThe Federation of New York also has a published list of farmer's markets and details in the State of New York. This can be found at:\nhttp://www.nyfarmersmarket.com/farmers-market-profiles/markets/markets.html\nCalPoly also has a research dataset (available to the public) on one years worth of sales/goods from a bakery chain in 4 states:\nhttps://wiki.csc.calpoly.edu/datasets/wiki/ExtendedBakery\nThe UC Irvine Machine Learning Repository also keeps a restaurant related research dataset covering 130 restaurants:\nhttps://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data#\n", "medical - Where is the \"product problem\" field in the openFDA API?": "\nThis is a good feature request.  We are accepting these request via Github (https://github.com/FDA/openfda/issues) and we will keep you posted there.\n", "Open Data for Quiz Game": "\nI suggest you take a look at the following Q&As:\n\nhttps://stackoverflow.com/questions/11067191/public-domain-trivia-database-for-game\nhttp://ask.metafilter.com/16200/Where-to-download-Public-Domain-general-knowledge-trivia-question-bank\n\nHowever, I don't think you're going to find a data set for you to download and import into your game. One idea would be to do some NLP on open-ish sources like Wikipedia to extract facts and turn them into questions and answers.\n", "education - Open Source GRE Data": "\nAnki is a flashcard software.\nIt has a large collection of shared decks, curated by the Anki community.  \nIn particular, Anki has many decks of GRE content:\nhttps://ankiweb.net/shared/decks/GRE\nThat list contains a few false positive, but when you filter them out you can see that there are about 120 decks about GRE:\n\n\u6781\u54c1GRE\u7ea2\u5b9d\u4e66    \nVerbal Workout GRE 4th edition (The Princeton Review)   \nGRE Words (with examples, antonyms and notes)   \n\u8981\u4f60\u547d3000\uff08GRE\u6838\u5fc3\u8bcd\u6c47\u8003\u6cd5\u7cbe\u6790\uff09    \n\u6975\u54c1GRE\u7d05\u5bf6\u66f8(\u7e41\u9ad4\u4e2d\u6587)+\u8072\u97f3\u6a94  \n\u6975\u54c1GRE\u7d05\u5bf6\u66f8(\u7e41\u9ad4\u4e2d\u6587)  \nHigh Frequency GRE words with Bengali Meaning   \nGRE Words List  \nGRE Vocabulary in Sentences     \n\u6975\u54c1GRE\u7d05\u5bf6\u66f8(\u7e41\u9ad4\u4e2d\u6587)+\u8072\u97f3\u6a94 2014.05 \u65b0\u7248   \n\u65b0GRE\u6838\u5fc3\u8bcd\u6c47\u8003\u6cd5\u7cbe\u6790\uff08\u518d\u8981\u4f60\u547d3000\uff09  \nGRE Vocabulary  \nGRE&GMAT\u9605\u8bfb\u96be\u53e5    \nBarron's GRE    \nGRE\u8bcd\u6c47\u7cbe\u9009     \nGRE\u8bcd\u4ee5\u7c7b\u8bb0     \nGRE Vocabulary-High frequency words with example sentences  \nGRE Tara    \nGRE Wordlist    \nG.GRE\u7cbe\u9009\u5206\u9891(\u5254\u9664\u56db\u516d\u7ea7):5\u661f(1)+4\u661f(13)+3\u661f(78)+2\u661f(1129)   \nI.GRE\u7cbe\u9009\u5206\u9891(\u5254\u9664\u56db\u516d\u7ea7):0\u661f(2449)   \nH.GRE\u7cbe\u9009\u5206\u9891(\u5254\u9664\u56db\u516d\u7ea7):1\u661f(2959)   \nGRE High-Frequency Words    \nGRE for sis 4   \nGRE for sis 1   \nGRE Words List wj   \nGRE Words   \nAnthony's GRE Flash Cards   \nKaplan's GRE Word Groups    \nGRE Psychology Physiological Psychology     \nGRE for sis 2   \nGRE Vocab   \nGRE for sis 3   \nGRE Psychology Sensation and Perception     \nGRE High Frequency  \nGRE Deck 1 sept 7   \nGRE for sis 7   \nGRE Vocabulary  \nGRE     \nGRE\u7ea2\u5b9d\u4e66\uff08\u5b89\u5353\u7248\u4fee\u6539\uff09       \nGRE for sis 5   \nGRE Master Word List - 5000+ Words  \nGRE1    \nGRE Psych Deck 9/24/2013    \nWCG's GRE Words     \nGREFANG     \nGRE sentences part 3    \nGRE Psychology Cognitive Psychology     \nGRE Vocabulary 2    \nmath GRE    \nGRE Words with Similar Meanings     \nGRE Vocab   \nGRE High-Frequency Vocabulary   \nGRE Vocabulary  \nGRE Antonyms    \nClassical Mechanics - GRE   \nGRE vocabulary  \nGRE Red Bible - Freq 3+     \nGRE Psychology Social Psychology    \nGRE words   \nGRE Vocab   \nGRE Parts of Speech     \nGRE 490 - all words mattf   \nGRE Verbal 1100+Some Magazines  \nKaplan GRE 500 Vocab    \nAneesHussain GRE Wordlist Vocabulary    \nGRE (math verval)   \nOptics and Wave Phenomena - GRE     \n1736 Words for GRE  \nElectricity and Magnetism - GRE     \nBarron's 799 Essential Words for the GRE    \nGRE Gifty   \nKaplans GRE words   \nGRE 1100    \nGRE Psychology Research Design  \nGRE\u7ea2\u5b9d\u4e662012  \nGRE Vocabulary for Verbal Section   \nBarrons GRE 4813 wordlist pron, EN, RU  \nGRE Word Groups     \nPrinceton Review GRE Hit Parade     \nHigh Freq GRE   \nGRE May Vocab   \nGRE(set1 of 300)    \nGRE Roots (Graduate Record Examination Roots)   \nAngela's Top 200ish Vocab for the New GRE Verbal    \nGRE High Frequency Words    \nGRE words (with examples)   \nGRE(Set2 of 180)    \nGRE words part 2    \nGRE High-Frequency Words (K)    \nGRE word list 1     \nGRE vocabulary  \nGRE- Flashcards     \nGRE Vocabulary 2014     \nGRE words list 800 with 300 high frequency tags     \nGRE vocab 1     \nGRE Psychology Learning and Ethology    \nGRE Vocabulary List     \nGRE words part 3    \nGRE-Barron's    \nGRE Psychology Developmental Psychology     \nGRE Duy Anh     \nGRE-Personal Practice   \nGRE for sis 6   \nKaplan GRE words    \nMy GRE Wordlist     \nPhysics GRE     \nGRE Literature  \nGRE 3978 words with my own sentences added  \nThermodynamics and Stat Mech - GRE  \nSelf Input GRE Words    \nPsychology GRE  \nMy GRE  \nGRE Psychology Personality and Abnormal Psychology  \nGRE\u7ea2\u5b9d\u4e66\u5e26\u97f3\u6807   \n\nEach deck is a collection of questions and answers.\nQuality and focus varies from one to the other.\n", "Open data for chemical substances, structures and products?": "\nYou may try to download Structures, Sequences and Ligand free of charge from:\n\nProtein Data Bank (RCSB) by specifying PDB IDs which you looking for (e.g. 2bg9),\nPubChem by NCBI (BioAssays, Compounds and Substances),\nChemSpider (free chemical structure database providing fast access to over 28 million structures, properties and associated information),\nWikipedia (Chemical_substances) by downloading the whole database or specific category (e.g. Chemical substances, Chemical compounds, etc.) by using Special:Export into XML format.\n\nAlso check out few free chemistry softwares which could potentially contain some databases:\n\nYenka Chemistry (before Crocodile Chemistry) - complete virtual laboratory which offer free home licenses for personal, non-commercial, non-academic use only.\nGamess and Quantum ESPRESSO - Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale\n\nSee also:\n\nWhich Chemical Database Software Are Available?\nProgram that simulates basic reactions in organic chemistry\n\n", "data request - List of vocabulary (kanjis) for the JLPT1 exam": "\nHave you tried the AJet resources (Japanese Language Proficiency Test Resources)?\n", "data request - Open database of enterprise software prices": "\n\nA source at the US federal government level is GSA Advantage! which allows schedule holders (government contractors) to list products and services that government agencies can purchase much like an online shopping cart.\nThis is by no means clean or normalized data and runs into the issues described in the comment thread above about discounts/sales.\n\nAnother one at the US federal government level is USASpending.gov which has it's own data issues but is supposed to be comprehensive. This will show you prices actually paid under contract/grant/etc. rather than list prices. Unfortunately, usually only a top-line number is available and transparency of the actual contract terms is not what many wish they were. Here's a sample search for FileNet where you can filter to see only purchases directly from FileNet Corporation versus other resellers/service providers.\n\n\n", "data request - Free icons of country flags, reusable in both open source and commercial products": "\nThere is a Google Internationalization project called Region Flags that structures the Wikipedia data.\n\nThis package is a collection of flags for BCP 47 region codes. Most people think of these as country flags, but there are a few codes / flags that do not correspond to countries. The flags are in SVG and PNG format and named by their BCP 47 region code, which for countries is the same as ISO 3166-2 country code.\n\nThe license is based on the Wikipedia license, which are flag-dependent\n\nThe flags are downloaded from Wikipedia. When Wikipedia flags were copyrighted, we worked we Wikipedia editors to either relicense them, or drew / sourced and uploaded new public-domain versions.\n\nHow to get data:\n\nTo download missing flags, run download-wp.py.\nTo update to latest flags from Wikipedia, delete the html, svg, and png directories, then run make-aliases.sh followed by download-wp.py.\n\nData formats are PNG and SVG, where SVG can be easily and safely be used at any resolution.\n\n", "data request - Sports results datasets": "\nthe olympics site seems to have all of the data you seek:\nhttp://www.olympic.org/athletics\nbut it looks like the opposite of open...  \nthe guardian's data store has some of what you seek:\nhttp://www.theguardian.com/sport/series/london-2012-olympics-data\n2012 open data\nhttp://www.theguardian.com/sport/datablog/interactive/2012/aug/03/london-2012-results-open-data\nmore guardian data, in google drive spreadsheet, THIS actually looks close to what you seek:: https://docs.google.com/spreadsheet/ccc?key=0AonYZs4MzlZbdHlfd0F1QlAxYjgtOW53ZXNOZ0JzNVE#gid=0\nall records for 2012\nhttps://docs.google.com/spreadsheet/ccc?key=0AonYZs4MzlZbdFNaMTRsVDNiV1RZaWNGdmJDU1RSSGc#gid=0\nsome gis data for you:\nhttp://geocommons.com/overlays/16680 \nwikipedia looks like an optimal choice for data selection; but (i think) you're going to have to work for it:\npicked a medal sport: pentathalon; to vague; drill pentathalon down to a random year:\nhttp://web.archive.org/web/20091103091009/http://en.wikipedia.org/wiki/Modern_pentathlon_at_the_2008_Summer_Olympics\nstill not the detail wanted...but if you click on the details link in the medalists table, you'll find the information you seek:\nhttps://en.wikipedia.org/wiki/Modern_pentathlon_at_the_2008_Summer_Olympics_%E2%80%93_Men%27s\nnote: i only did this for one medal sport, so i'm not sure if its the same throughout, but knowing wikipedia, i'm sure its the same throughout ;).\noffhand, you should get a list off wikipedia off events with links, and use a scraper to ping them all, appending the appropriate details info onto each url (\"-Mens, -Womens\", etc); i'm willing to bet the table with the detailed info has an id attribute, which i would then nail down to in the scraper, scooping up it and its contents...  \nyou can ping nbc's olympics site via wayback machine for more data, at least going back to athens 2004; wayback had links even older, although i'm not sure if they're the same format\nhttp://www.nbcolympics.com/index.html\nhttp://web.archive.org/web/20040804003859/http://www.nbcolympics.com/index.html\nhttp://web.archive.org/web/20060504132456/http://www.nbcolympics.com/index.html\nhttp://web.archive.org/web/20140103124519/http://www.2008.nbcolympics.com/modernpentathlon/resultsandschedules/index.html\njust to show a few\nBONUS:\ndatavis of olympic data howto:\nhttp://datavisualization.ch/inside/how-we-visualized-112-years-of-olympic-games/\nolympic datavis example gallery:\nhttp://www.visualizing.org/galleries/peoples-choice-visualizing-london-2012-olympic-games\n", "licensing - Is it possible to use CKAN for commercial use?": "\nYes.\nCKAN is licensed under Affero GNU GPL v3.0. From the preamble to the license:\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nSources: http://ckan.org/developers/about-ckan/, http://www.gnu.org/licenses/agpl-3.0.html, http://www.gnu.org/licenses/gpl-faq.html#GPLCommercially\n", "data request - Global births for past 10k years?": "\nThis author (Scott Manning) reviews various sources for historical earth population\nhttp://www.scottmanning.com/content/year-by-year-world-population-estimates/\nAt this link, he has an excel file showing various sources for estimating back to 10000 BC.\nhttps://spreadsheets.google.com/pub?key=pbb0aoD3hdM-HgD-knTGXIA\n", "Daily european weather data of past few years for non commercial use": "\nIn the past I've used the Weather Underground API, which allows for 500 calls per day. See my answer here for some details - LINK.\nIn their TOS they say you can even use for commercial use (but read the small print!)\n\nWUL licenses to you a limited, worldwide, non-exclusive, non-transferable, revocable license for your personal or commercial use.\n\nMy python 2.7 code snippet (shared here). Because there are only 500 calls allowed for free per day, I retrieved one year per day:\nimport requests\ndef get_precip(gooddate):\n    urlstart = 'http://api.wunderground.com/api/INSERT_KEY_HERE/history_'\n    urlend = '/q/Switzerland/Zurich.json'\n\n    url = urlstart + str(gooddate) + urlend\n    data = requests.get(url).json()\n    for summary in data['history']['dailysummary']:\n        print ','.join((gooddate,summary['date']['year'],summary['date']['mon'],summary['date']['mday'],summary['precipm'], summary['maxtempm'], summary['meantempm'],summary['mintempm']))\n\nif __name__ == \"__main__\":\n    from datetime import date\n    from dateutil.rrule import rrule, DAILY\n\n    a = date(2013, 1, 1)\n    b = date(2013, 12, 31)\n\n    for dt in rrule(DAILY, dtstart=a, until=b):\n        get_precip(dt.strftime(\"%Y%m%d\"))\n\n", "data request - In what programming language(s) is software X written?": "\nWikidata has a property called programming language that should provide the data you are looking for.\nExamples: Skype, Tomcat\nYou can also have a look at a list of available items that contain the property \"programming language\".\n", "releasing data - Rating system for websites": "\nThe Better Business Bureau has a listing of \"Safe, Secure and Approved Places to Visit Online\", but I have no idea how they license their data as their 'terms of use' link redirects to their front page.  \n(and the cynic in me thinks that companies paying to be included, which is BBB's economic model, is a conflict of interest (as their 'customer' are the businesses, to de-list a company is to lose a customer)\n", "data request - Flashcards to remember all Ingress glyphs": "\nI've just created an anki deck based on the ingress.tv glyphs. It is available for download as an apkg here.\nhttps://www.dropbox.com/s/745sda96j070fyg/ingress.glyphs.apkg?dl=0\nOne of the differences is my deck contains known 2-5 string sequences as well (well, as shown on ingress.tv). Note that the deck you have found is based on glyphtionary - I previously used this as well but found it had some minor errors (I believe they have been corrected now)now.\n", "csv - Where to upload open data? (no online data editing needed)": "\nWell my comment received a number of up votes which I take as a signal of quality and I am posting the links here so they are more visible to future visitors:\n\nopendata.socrata.com - you can upload a number of different file types here, create visualizations, link to them, and take advantage of a very mature set of APIs for data consumption and publishing\ndatahub.io - like opendata.socrata.com, datahub.io is an open data portal but it runs CKAN which is free and open source unlike Socrata. CKAN is modular and you can write plugins for it but in my experience, out of the box, Socrata has better visualization and mapping tools\ngithub.com - You mention SourceForge and I would recommend taking a look at github instead. You will have limited analytics compared to the first two sites which are built specifically for what you are looking for but Github has a lot of advantages like the ability for folks to fork your code and they do render CSV and GeoJSON documents nicely now. Also take a look at this blog post by OKFN (folks behind CKAN) for some suggestions on how to use Git (and Github) for Data\nquandl.com and modeanalytics.com - these sites seem promising but I have not had a chance to work with them as much and they do not appear to be as feature complete or widely adopted\n\n", "best practice - Hackathons and opendata": "\nOne resource I have sent many people to (and they always find value in it) is Socrata's* \"hackathon in a box\" guide: http://hackathon-in-a-box.org/guide/\nThe Open Knowledge Foundation also has a guide which I have not read as closely at http://blog.okfn.org/2012/10/26/hackathons-the-how-to-guide/.\nAnd finally, please at least skim ChallengePost's blog post titled \"Hackathons \u2260 developer exploitation. A what-not-to-do guide for good hackathon organizers.\" \n\n*Full disclosure: the company I co-own is a partner of Socrata and they have sponsored hackathons we have helped organize.\n", "metadata - Data set for identifying columns as categorical or numerical": "\nTwo things:\n\nExisting functionality: The two main open data portal software platforms I have worked with, Socrata and CKAN, have this functionality built in. Socrata is closed source while CKAN is open source.\n\nThe add-on part of CKAN that handles this is called DataPusher (https://github.com/ckan/datapusher) and it \"pushes\" data from readable files like CSV to the DataStore extension. Documentation at http://docs.ckan.org/projects/datapusher/en/latest/. The code related to choosing column types is at https://github.com/ckan/datapusher/blob/6175c69e9ab7013272949e6e604fc45d21fce853/datapusher/jobs.py#L282 which uses the messytables python library which can be found at http://messytables.readthedocs.org/en/latest/\n\nCheck out data.gov, datahub.io, and others for a lot of open data to \"train\" from. Notably:\n\nFor Socrata API endpoints, such as https://open.whitehouse.gov/Government/2010-Report-to-Congress-on-White-House-Staff/rcp4-3y7g you can get column metadata at URLs such as https://open.whitehouse.gov/views/rcp4-3y7g/columns.json. Be careful though, this is no longer documented on dev.socrata.com from what I can tell and might be deprecated/unsupported\nFor CKAN, you need to find the resource ID of a datastore-backed resource, and you can create a URL such as http://54.204.10.90/api/3/action/datastore_search?id=7c0608e3-fb4d-4fb3-928c-5351e5b9b122 and it will show you the metadata of the columns as well\n\n\nHope this helps!\n", "data request - Dataset with a multivariate time series of circular and linear variables": "\nThere is an area of statistics called functional data analysis.  If you look through the documentation for these procedures (R and matlab) or the books/short courses, you should be able to find a dataset that meets your needs.\n\nhttp://faculty.bscb.cornell.edu/~hooker/ShortCourseHandout.pdf\nhttp://www.psych.mcgill.ca/misc/fda/\nhttp://cran.r-project.org/web/packages/fda/index.html\nhttp://www.amazon.com/Functional-Data-Analysis-Springer-Statistics/dp/038740080X/ref=sr_1_1?ie=UTF8&qid=1421760835&sr=8-1&keywords=functional+data+analysis&pebp=1421760797440&peasin=038740080X\n\n", "industry - open data in maintenance and repair": "\nI found this link:\nPredictive Maintainance and Sensor Data Analytics\n\nData from a semi-conductor manufacturing process\nKey facts: Data Structure: The data consists of 2 files the dataset file SECOM consisting of 1567 examples each with 591 features a 1567 x 591 matrix and a labels file containing the classifications and date time stamp for each example.\n\n", "usa - U.S. federal government data fetchable via SPARQL?": "\nyour best bet is to search specific data sites that you want to get the data from...i went to healthdata.gov and found this guy:\nhttp://healthdata.gov/cqld\nalthough their query point throws a 404\nOKFN also runs a sparql service to show available endpoints, though you'll have to pick through to find us ones:\nhttp://sparqles.okfn.org/availability \nthis slideshare set tells you how to do healthcare.gov's data:\nhttp://www.slideshare.net/george.thomas.name/clinical-quality-linked-data-on-healthdatagov\n", "openfda - Is there an easy way to access medical device approvals beyond recently-approved devices?": "\nI see you added the openfda tag to this question but this doesnt seem to be data that is available from openFDA. The data regarding devices that openFDA has is enforcement reports (https://open.fda.gov/device/enforcement/) and adverse events.\nIf you are determined to get this data from openFDA, you could.. sorta kinda. You can gather data about devices that have had at least one adverse event by getting a list of manufacturers by going to https://api.fda.gov/device/event.json?&count=device.manufacturer_d_name.exact&limit=1000 and the doing queries for all events related to a manufacturer (https://api.fda.gov/device/event.json?&search=device.manufacturer_d_name.exact:KENDALL). This is probably not a good idea because you will have to de-duplicate events from devices but it is an option for using currently open openFDA data for this case.\nMoving on from openFDA to the general info on FDA.gov including the link you posted, you are looking for data older than 2008 or are you in need of a way to search those data (in which case a scraper will likely need to be created)?\n\nEDIT: You might be interested in this news story from Modern Healthcare: FDA urged to encourage development of medical-device registries \n\nEDIT 2: My colleague found this, http://www.fda.gov/MedicalDevices/ProductsandMedicalProcedures/DeviceApprovalsandClearances/510kClearances/ucm089428.htm, on FDA.gov. It is 510(k) data going back to 1976.\n", "data request - Finding new and future song releases before they are found by Google Search": "\nI guess Amazon gets new song releases information directly from the labels, which they can do thanks their their reseller position. So it is probably not open data.\nIf open data existed, Google would know it and crawl it often, and would not be far behind.\nAZLyrics probably does not have any relationship with the labels, they just crowdsource their data. Their data is not open either.\nSo, my guess would be that unfortunately there is no open data resource for song releases which has info before Google Search. I hope to be proven wrong.\n", "How to get my data accepted in the Linked Open Data Cloud Diagram?": "\nessentially you need to:\n\nvalidate your dataset with ckan lod validator  \npublish dataset\nadd it to ckan \"so that it appears in the next version of the lod cloud diagram\"  \n\nReference: http://datahub.io/group/about/lodcloud\n", "geospatial - How does Google Maps get traffic info?": "\nGoogle uses crowdsourcing techniques for the collection of the traffic data, as they explain in this blog post. This basically means that everybody who uses Google Maps on their mobile phone automatically reports anonymous traffic data back to Google.\n", "transportation - Car Sales & Car Theft (Stolen Automobiles) Data": "\n\nFor the USA, the National Highway Traffic Safety Administration (NHTSA) has general information and fact sheets about theft as well as a search tool where you can get theft rates by year, production rates by year, look by manufacturer, and more. Unfortunately, it looks like data is only up to 2011 but it does go back to 1983.\nFor more USA data and aggregated info, check out the National Insurance Crime Bureau (see the 'Theft and Fraud Awareness' menu) which does seem to possibly be the source for the Honda Accord data point. \nInterpol has some international data\n\n", "usa - How can you get nationwide data of a particular type from the US Census website?": "\nTLDR: Census data is complicated.\n\nThe simplest actual answer to your question: If you type in \"united states\" on the front page of American FactFinder, you can get to the \"Community Facts\" page for the US, which has direct links to several kinds of data at the national level.\n\nProbably way more than you really wanted to know:\nMore generally, Census data is a very simple phrase behind which lies a ton of variety, nuance and detail. I've spent several years now working on various projects to make Census data easier to use, and it's harder than you would guess. Even a straightforward concept like \"population\" has multiple answers from the US Census: every ten years the Decennial census comes up with one count; annually, the American Community Survey (ACS) estimates the population using a different methodology, and every month, the Population Estimates program produces its own numbers. Oh, and also monthly the Census Bureau and the Bureau of Labor Statistics update the Current Population Survey (CPS) which has its own total population number.\n\nIs there no straightforward to say \"I want data X about region Y, grouped by subregion Z, give it to me now, please and thank you\"? Importantly, Y may be \"the entire United States\", and ideally, X may include multiple data points.\n\ndata X\nFor the American Community Survey alone, there are several hundred table variants, and several of the variants come in two forms, and some of them come in about 20 forms because they break down figures according to race or hispanic status. (In total, there are over 1400 tables in the ACS). Because the raw data can't be released for privacy protections, the Census needs to presumptively tabulate all the different kinds of cross-referenced data people might want. (And some of the data you may want is collected in the CPS or other programs.)\nregion Y\nthe Census Bureau data is tabulated into \"summary levels\" which represent different classes of geography. The ACS Glossary has a summary level list has 188 different kinds of value for Y, with hundreds of thousands of actual values. And there are more summary levels than on that list. The Missouri Census Data Center (MCDC) has delved into summary levels and tried to compile a master list which has 220 of them.\nThese lists are complicated for technical reasons (mostly to do with the next section) but even if you think in terms of simple shapes-on-a-map, there are about 30 basic geographies.\nAlso, for the US and for states, there's a separate concept (\"geographic components\") to do with \"give me the total of data X for just the people living in \"urban areas\", or \"not in a metropolitan or micropolitan statistical area\". There are about 20 of these components.\ngrouped by subregion Z\nonly some Census geographies are by definition properly contained by specific other geographies. Cities (sumlevel 160, formally known as \"places\") cross census tract and county boundaries, adding a bit of complexity to one commonly desired grouping. And in some parts of the country, locally prevailing government systems make \"places\" a less appropriate way to break things up: things are different in many ways in New England, especially.\nOh, and \"the census year from which I want the data.\" Census maps change every ten years, so data is not always comparable across Census releases. How you do your comparisons when that happens depends sometimes on whether you're looking at statistical areas like census tracts, or legal areas like states and counties. And the questions change too, not just the maps. In 2000, the Census Bureau allowed people to select more than one race, complicating comparison of new data with data collected before then.\nThird Party Projects\nA lot of people have worked on third-party projects to make Census data easier to use. I invite you to check out a project I've worked on called Census Reporter. It's limited to the American Community Survey for the foreseeable future.  I'm proud of what we've done, and we get great feedback, but I know for a fact we have a long way to go to make it as easy to find tables as you (and I!) want.\nBesides Census Reporter, there's a commercial project called Social Explorer which has done a lot of work to make the data more directly cross-comparable. You may be able to access it through your local library.\nThere are some other great projects which also take on these challenges, but which tend to be aimed at researchers who are more deeply invested. For example, the aforementioned MCDC, NHGIS, IPUMS, Census.IRE.org (which I also helped to make), and I'm sure more that I'm forgetting.\n", "releasing data - Where should I host/contribute a mapping from Stack Overflow tags to Wikipedia articles?": "\nThe best is to host your mapping directly on Wikidata. The property is https://www.wikidata.org/wiki/Property:P1482\nFor instance, https://www.wikidata.org/wiki/Q859221 represents Java Swing, and has https://stackoverflow.com/tags/swing as a Stack Exchange tag property:\n\n[...]\n\nLicense: public domain\n", "usa - Are there freely available equivalents to the HUD crosswalk data (zip code to county/census mapping) that go back farther in time?": "\nalthough zctas and zip codes are not perfect equivalents, they are pretty close.\nyou can generate any conceivable crosswalk with census 2010, 2000, or 1990 geographies here:\nhttp://mcdc.missouri.edu/websas/geocorr12.html\nhttp://mcdc.missouri.edu/websas/geocorr2k.html\nhttp://mcdc.missouri.edu/websas/geocorr90.shtml\n", "openfda - Wildcard searches": "\nYou cannot do a two-way table where you get a list of adverse events by drug using the current openFDA API. \nWhat you can do is find most of* this information in a two step process which can of course be automated:\n\nGet a list of drugs by generic name: https://api.fda.gov/drug/event.json?search=&count=patient.drug.openfda.generic_name.exact&limit=1000\nGet a list of adverse events that have shown up in event reports that contained that generic drug. For example, ASPIRIN: https://api.fda.gov/drug/event.json?search=patient.drug.openfda.generic_name.exact:ASPIRIN&count=patient.reaction.reactionmeddrapt.exact&limit=1000\n\n* \"most of\" because these count queries will return only the top 1000 results\nHope this helps!\n", "usa - Fiber (dark or lit) maps and data": "\nBroadbandMap.gov, by the FCC, has a raster map of fiber to the end user at http://www.broadbandmap.gov/technology/fiber-to-the-end-user\nUnfortunately it does not seem to have the data available as anything other than a tile service.\nEdit 1: You might consider requesting the data from a FCC Data Officer\nEdit 2: Also, have you seen http://www.telecomramblings.com/network-maps/usa-fiber-backbone-map-resources/ which at least tries to link to each carrier/provider's maps which could then be used to create a dataset from?\n", "medical - Where can I find open data on healthcare quality indicators?": "\nAs you know, there are a number of different quality improvement initiatives at all different levels of government and the healthcare ecosystem. Here are some of my favorite sources for hospital level data which has good coverage across CMS certified hospitals in the US:\n\nCMS Hospital Compare - http://www.medicare.gov/hospitalcompare/Data/Measures-Displayed.html - this page shows the different measures data is available for. You can download the data from data.medicare.gov here and/or through data.cms.gov by searching through the data sets. You can programmatically access all the great data on data.cms.gov through Socrata APIs which are detailed at http://dev.socrata.com.\nI personally think hospital patient experience (HCAHPS) data should be considered in conjunction with other quality data and so I suggest checking out the HCAPS data which can be accessed at https://data.medicare.gov/Hospital-Compare/HCAHPS-Hospital/dgck-syfz\nCMS Physician Compare - https://data.medicare.gov/data/physician-compare - similar to CMS Hospital Compare but with less measures. Mainly PQRS data which is at https://data.medicare.gov/Physician-Compare/Physician-Quality-Reporting-System-PQRS-Group-Prac/ddiw-pgap\nACO Quality Data - https://data.medicare.gov/Physician-Compare/Accountable-Care-Organization-ACO-Quality-Data/ytf2-4ept - this is sort of under the umbrella of the CMS Physician Compare data sets but I think it's worth noting separately too\n\n", "programming - R packages with open data in them": "\nHere is a human-friendly list of all 731 datasets in R, as well as CSV download links.\n", "licensing - Suggested formats for open data documents": "\nIdeally, like @Andrew - OpenGeoCode mentions, you would release it in multiple formats.\nI would really suggest you look into organizing it into a e-book written in Markdown and hosted on Github. There are several advantages to this such as a built-in change log, being able to let people to (publicly) fork your document(s) and share their changes with the public, give you a workflow to bring in suggested changes (pull requests), and much more. \nAn example of this can be seen at https://github.com/truevault/hipaa-compliance-developers-guide. \nThere are a number of ways to convert your Markdown to PDF, HTML, etc. (1, 2). You can also google for tools to convert your existing ODF/Word/etc documents to Markdown.\n", "data request - Reusable pictures of Chinese factories in the 1930~1940s": "\nHere are some of my findings.. no one source, lots of looking all over the net:\n\nSearch for \"factory\" on http://www.asu.edu/lib/archives/smedforeign.htm \n\n\"Chinese factory workers have spiritual  faces--faces filled with suffering.\"\n  1930s Photographer: Agnes Smedley Agnes Smedley Collection\n  MSS-122 Vol. 39\n\nhttp://commons.wikimedia.org/wiki/File:China_Motor_Corporation_top_crew.jpg\n\nTop managers, chief engineers, and American consultants in front of\n  the China Motor Corporation. China Motor Corporation was the first\n  Chinese factory of manufacturing jet engines. It was established with\n  American assistance. World War II, in Kuichow (current Guizhou),\n  China.\n\n1950s and not a photograph but: https://www.flickr.com/photos/chinesepostersnet/5099136754\n\nEmulation in the Patriotic production campaign  Designer: Huang Jun\n  ca. 1950\n\n\n", "data request - List of relase dates and prices of Adobe products?": "\nYou can get release dates of versions/iterations at http://en.wikipedia.org/wiki/Adobe_Creative_Suite but I think you'll need to use a combination of Google and The Wayback Machine by archive.org to find pricing.\nAlso see http://www.computerworld.com/article/2517120/enterprise-applications/adobe-creative-suite--the-history.html\n", "json - Any Open (Structured) Datasets for the World Factbook (Public Domain Country Profiles Published by the CIA)?": "\nYou can use factbook for data. A simple google search can get you raw json or csv. \nOfficially you can use their tool : \nhttps://github.com/factbook/factbook\nor download data directly from CIA website :\nhttps://www.cia.gov/library/publications/download/\nPrevious answer : \nThe links are dead - looks like data has been removed without any notice.\n\nCheck this out. https://github.com/factbook/factbook.json\nit has all data in json - I recommend using nosql database for importing. \nThey also offer sql dump https://github.com/factbook/factbook.sql\nCSV with facts. https://github.com/factbook/factbook.csv\n\n", "data request - Open resources about cosmetics and beauty/body products": "\nNot exactly what you are looking for but any cosmetic containing sunscreen is in the Structured Product Label dataset at FDA (http://labels.fda.gov/), which is part of the openFDA effort (https://open.fda.gov/drug/label/).\n", "Is there a vocabulary for linking weather data?": "\nWe (met.no) plan on publishing a JSON-LD vocabulary for climate/weather data, when we release our public portal later this year. We'll be publishing JSON-LD ourselves as our primary data format.\nTo the best of my knowledge, there isn't anything else stable out there yet suitable for this kind of use. The closest you get, AFAIK, is http://codes.wmo.int/\n", "licensing - Can I simply copy a CC-BY-SA 3.0 work as CC-BY-SA 4.0?": "\nI believe that a copy is also a kind of adaptation (opposite is not true, obviously).\nAnd yes you can take a CC-BY-SA 3.0 work and publish your \"adaptation\" as CC-BY-SA 4.0:\nThe CC-BY-SA 3.0 license says:\n\nYou may Distribute or Publicly Perform an Adaptation only under the terms of: (i) this License; (ii) a later version of this License with the same License Elements as this License; (iii) a Creative Commons jurisdiction license (either this or a later license version) that contains the same License Elements as this License (e.g., Attribution-ShareAlike 3.0 US)); (iv) a Creative Commons Compatible License.\n\nand:\n\n\"Creative Commons Compatible License\" means a license that is listed at http://creativecommons.org/compatiblelicenses that [...]\n\nI turn, http://creativecommons.org/compatiblelicenses says in its BY-SA Version 3.0 paragraph:\n\nYour contributions to adaptations of BY-SA 3.0 materials may only be licensed under: BY-SA 3.0, or a later version of the BY-SA license. [...]\n\nCC-BY-SA 4.0 is a later version of CC-BY-SA 3.0, so you can redistribute.\n", "linked data - What triplestore that can handle the largest number of triples?": "\nThe W3C hosts a list of large triple stores with documented deployments and numbers of triples.\nThe top contenders with more than 10B triples currently are:\n\nAllegroGraph (1Trillion+)\nStardog (50B)\nOracle Spatial and Graph with Oracle Database (48B+)\nOpenLink Virtuoso v6.1 (15.4B+) \nOntotext GraphDB (formerly BigOWLIM) (12B+)\nGarlik 4store (15B)\nBigdata (12.7B)\n\n", "rdf - What Linked Data serialization format to choose for our (now CSV) open data?": "\nIn addition to @enridaga's great answer, let me offer a few additional thoughts.\nRDFa is (only) useful if you already have HTML content that you want to enrich with semantic data.\nRDF/XML is very well readable for machines, but not so much for humans.\nN-Triples, Turtle or N3 are currently pretty much the default formats for Linked Data dumps. For example, have a look at the recently released official Wikidata RDF exports. N-Triples are preferable for large files because they can be easily processed line by line.\nJSON-LD is the new kid on the block with lots of potential and already some uptake. Its biggest advantage: Even programmers that have never heard of the Semantic Web can use it immediately.\nAs @enridaga mentioned, once you have your data in one of these formats, it can be converted automatically to any of the other formats. You can give it a quick try with the online RDF Translator.\nIf you only want to provide a simple dump of your data for the Semantic Web crowd, go with (gzipped) N-Triples. If your users prefer a different format, they can easily convert it themselves.\nIf you also want your data to be instantly accessible to regular programmers, make it JSON-LD.\n", "usa - Are zip-code-level IRS income tax data available for every year, in machine-readable fomat?": "\nAccording to @MichaelA (in the comments), zip code level IRS income machine-readable income data is not available for every year, and the IRS has no plans to do so/isn't very receptive to the idea  \nedit:\nYou can get all the data based on county/city (not zip) for the years 1989-2010 here:\nhttp://www.irs.gov/uac/SOI-Tax-Stats-County-Data\nSo not the exact format you asked for, but its something to work with and also covers the years you are missing and then some.\nI took this data and scraped out the information for Virginia, which you can view here:\nhttp://data.openva.com/dataset\n", "film - Is there an API for the Oscars/Academy Awards that lists past winners as well as current nominees?": "\nCheck out Wolfram Alpha API for things like:\nhttp://www.wolframalpha.com/input/?i=academy+awards+1998\n\nThis is the free (public) interactive form interface. You can get a download as raw data with a Pro Subscriber (paid) license.\n", "data request - Amazon ASIN and Category": "\nYou can use the Amazon Product API to find the category breakdown on Amazon and corresponding products 4 sale.\nThis link explains how items for sale are organized under the API:\nhttp://docs.aws.amazon.com/AWSECommerceService/latest/DG/CHAP_OrganizationofItemsforSaleonAmazon.html \nThis is the general link to the API:\nhttp://docs.aws.amazon.com/AWSECommerceService/latest/GSG/Welcome.html\n", "medical - Imaging Cost Data and Procedure Costs": "\nMedicare has a public dataset that may provide the information you are looking for:\n\nMedicare Provider Utilization and Payment Data: Physician and Other\n  Supplier \nAs part of the Obama Administration\u2019s efforts to make our healthcare\n  system more transparent, affordable, and accountable, the Centers for\n  Medicare & Medicaid Services (CMS) has prepared a public data set, the\n  Medicare Provider Utilization and Payment Data: Physician and Other\n  Supplier Public Use File (Physician and Other Supplier PUF), with\n  information on services and procedures provided to Medicare\n  beneficiaries by physicians and other healthcare professionals.  The\n  Physician and Other Supplier PUF contains information on utilization,\n  payment (allowed amount and Medicare payment), and submitted charges\n  organized by National Provider Identifier (NPI), Healthcare Common\n  Procedure Coding System (HCPCS) code, and place of service. This PUF\n  is based on information from CMS\u2019s National Claims History Standard\n  Analytic Files. The data in the Physician and Other Supplier PUF\n  covers calendar year 2012 and contains 100% final-action\n  physician/supplier Part B non-institutional line items for the\n  Medicare fee-for-service population.\n\nhttp://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html\n", "data.gov - Looking for openFDA datasets which gives JSON data": "\nThey way openFDA stands, it is an API which returns JSON for a search. I too have asked for bulk downloads.\nThe information you seem to be looking for is available by modifying the following API call:\nhttps://api.fda.gov/drug/event.json?search=patient.drug.openfda.brand_name:advil&count=patient.reaction.reactionmeddrapt.exact&limit=1000\nIf you are looking to do this with an offline data set (aka not need to query the API each time), you have a few options:\n\nCache the results for a week or two\nGo through a list of all drugs and download the results and store that in a database of your own\nCompletely bypass openFDA API and go to the source data at http://www.fda.gov/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/ucm082193.htm \n\n", "Open data sets about software development: code quality, defect rate, programming languages?": "\nHere are a few sources to take a look through:\n\nMost comprehensive and closest to what I think you're looking for is a project by the acronym of PROMISE (PRedictOr Models In Software Engineering). Go to http://promisedata.googlecode.com/svn/trunk/defect/ to go straight to their data which is organized by code base and into CSVs. For example, tomcat.csv. It looks like the project also has a number of tools to help analyze and create models from the data.\nEclipse Bug Data! is, as it sounds, a project to mine bug and version datas from the Eclipse project. \nNASA also publishes \"cleaned versions of publicly available NASA MDP software defects data sets\" over at http://nasa-softwaredefectdatasets.wikispaces.com/\n\nEDIT: there are also a number of academic papers on this topic that you might want to skim to see if they reference specific data sets or sources.\n", "Where to find spatio-temporal data?": "\nYou could look for climate data on the open data portals around Italy, Regione Liguria has it for meteorogical measurements (http://www.cartografiarl.regione.liguria.it/SiraQualMeteo/Fruizione.asp) but doesn't have an API.\nOther suggestions from https://groups.google.com/forum/#!forum/spaghettiopendata :\n\nhttp://toolserver.openstreetmap.it/carburantiMiSE/ (fuel data)\nhttp://www2.arpalombardia.it/sites/QAria/_layouts/15/QAria/IDati.aspx?v=1 (air pollution)\n\n", "Open Data formats used": "\nI don't know about worldwide, but for Europe you can find statistics on data formats on the ENGAGE portal.\n\n", "Data Ferret (Census.gov) will not load": "\nThere are two parts to this question that I can address. First, you'll need to make sure that javascript is enabled.\nSecond, if you're after CPS data you have three separate resources. You have the option of using the FTP site which allows you to download the raw data. A better resource is from the National Bureau of Economic Research which offers the raw data, as well import statements for SAS, Stata, and SPSS for the basic monthly CPS and CPS supplements. The third resource is iPUMS for the CPS, it allows you to do things that are similar to dataferret, although it does require free registration with a simple approval process.\n", "government - how does add123.com get their data?": "\nFrom add123.com NMVTIS page:\n\nThe National Motor Vehicle Title Information System (NMVTIS) is a federal database containing automobile information from states, insurance carriers and the salvage industry. The vehicle history information is available to the public, and can provide valuable information about a potential purchase.\n\nThe problem with cars is that they can be easily moved around, so being able to find registration history from the Texas DMV will only give you details for Texas cars, and not cars that were flooded in Lousiana, or something.\nAn alteranative to add123.com would be to contact each State's DMV. Here is the PDF list of contact info - LINK.\n", "data request - Birth dates vs. Due dates": "\nAllen Downey's free online book Think Stats uses data from the National Survey of Family Growth to consider the question \"Do First Babies Arrive Late?\"\nSection 1.3 of the book explains some use of the data. There is a newer version of the dataset (2006-2010) on the NSFG site.\n", "usa - Errors with running Ruby Government Data SDK Sample": "\nThis issue was caused by both a bug and a missing update in the Ruby SDK.  It has been fixed now.  You can find the updated Ruby SDK at the Ruby_DOLDataSDK Github repo.\nAlso, the sample app code you linked has been updated to remove the API_SECRET parameter. The \"shared secret\" is no longer necessary. So the updated version of the sample program will now work with latest version of the Ruby SDK.\n", "county - Sourcing or creating (using PHP), a lists of all countries and their relevant ISO codes?": "\n\nhttp://peric.github.io/GetCountries/ seems to have everything you want except the language code however it is in SQL format which could be fairly easily converted.\nhttp://cran.r-project.org/web/packages/ISOcodes/ISOcodes.pdf also seems to have all  your data but is in R format\n\n", "standards - Data showing what JavaScript operations are usable in what browser": "\nThe caniuse.com project includes compatabilities for many browsers versus CSS, HTML, JS API, SVG, and Other categories.\n\n\"Can I use\" provides up-to-date browser support tables for support of front-end web technologies on desktop and mobile web browsers.\n\nLicense is CC BY-NC 3.0.\nRaw data is on GitHub.\n", "machine learning - Regression problem data suitable for ML library unit test": "\nThe \"elemapi\" data set may be promising (or maybe it will help refine the question of what you're looking for). I don't see an explicit license, but it's used widely and is from a public source, so hopefully there is a friendly license somewhere.\nUCLA-affiliated course use:\n\nIn this chapter, and in subsequent chapters, we will be using a data\n  file that was created by randomly sampling 400 elementary schools from\n  the California Department of Education's API 2000 dataset.  This data\n  file contains a measure of school academic performance as well as\n  other attributes of the elementary schools, such as, class size,\n  enrollment, poverty, etc.\n\n", "data request - Sample datasets with known outliers for IQR, Q-test and Z-test math tests": "\nMy advice would be to look at the documentation for the relevant statistical procedures in popular statistics software, specifically in R or SAS.  Statistical function documentation usually has sample datasets and examples of the usages.\nhttp://www.rdocumentation.org/\nhttp://support.sas.com/documentation/onlinedoc/stat/\n", "data request - Looking for a database for cellular tower and antenna locations in the united states": "\nI have not looked into these datasets. The FCC (fcc.gov) has datasets on antenna registrations and the locations of cell towers:\nhttp://wireless.fcc.gov/geographic/index.htm?job=licensing_database_extracts\narchive.org link: https://web.archive.org/web/20141015140708/http://wireless.fcc.gov:80/geographic/index.htm?job=licensing_database_extracts\n", "api - Is it the right query on what I am looking for in openFDA?": "\nYes, that API query looks correct but note the following:\n\nYou can get a count of the adverse event reactions through the API by adding &count=.. to the end of your URL like this: https://api.fda.gov/drug/event.json?search=patient.drug.openfda.brand_name:advil+AND+patient.patientonsetage:64&count=patient.reaction.reactionmeddrapt.exact\nYou should be aware that 45% (1,734,133 of the 3,814,280) adverse events on openFDA as of right now do not have any age set. You can see this at https://api.fda.gov/drug/event.json?search=missing:patient.patientonsetage for openFDA in general and at https://api.fda.gov/drug/event.json?search=missing:patient.patientonsetage%20AND%20patient.drug.openfda.brand_name:advil for  your advil query\nAge is not always (but usually is) denoted in years by openFDA\n\n", "rdf - How to express that a schema.org/Restaurant is located in a given Wikidata geographical entity?": "\nWhat you can do is to create another entity that represents Abbeville, set it as schema:location (or maybe schema:containedIn?) of the restaurant and then use schema:sameAs to link to that Wikidata URI:\n<rdf:Description rdf:nodeID='Nb925d432-69b1-42e0-8063-d914e3504dde'>\n    <rdf:type rdf:resource='http://schema.org/Restaurant'/>\n    <schema:name>Chez Mel</schema:name>\n    <schema:address>63-65 rue Saint-Vulfran</schema:address>\n    <schema:telephone>+33 3 22 19 48 64</schema:telephone>\n    <schema:description>Hearty and family-friendly restaurant.</schema:description>\n    <schema:location>\n        <rdf:Description rdf:nodeID='NsomeotherGUID'>\n            <rdf:type rdf:resource='http://schema.org/City'/>\n            <schema:sameAs rdf:resource='https://www.wikidata.org/wiki/Q28520'/>\n        </rdf:Description>\n    </schema:location>\n</rdf:Description>\n\n", "data request - Mapping between Wikidata and Geonames": "\nApproximately since October 2014 there exists GeoNames ID external identifier on Wikidata. The current number of entities with this identifier is about 1 500 000.\nYou could retrive this identifier value for Antananarivo using the following SPARQL query:\nSELECT ?geoNamesID WHERE\n{\n   ?place wdt:P1566 ?geoNamesID \n   VALUES (?place) {(wd:Q3915)}\n}\n\nTry it.\n", "data request - Dataset of languages and where they are spoken (sub-national)": "\nThe CIA Word Factbook has a field listing for languages spoken per country and percentage of population that speaks it.\nhttps://www.cia.gov/library/publications/the-world-factbook/fields/2098.html\nFor finer grain, you can Google 'language spoken home dataset'. This will find census datasets related to the distribution of languages spoken. Below are some examples:\nUnited States (Census) by state:\nhttps://www.census.gov/hhes/socdemo/language/data/other/detailed-lang-tables.xls\nCity of Chicago, IL by neighborhood:\nhttps://data.cityofchicago.org/Health-Human-Services/Census-Data-Languages-spoken-in-Chicago-2008-2012/a2fk-ec6q\nCity of Cambridge, MA by neighborhood:\nhttps://data.cambridgema.gov/Neighborhood-Census-Data/2007-2011-Language-Spoken-at-Home-by-Neighborhood/sba5-5kgg\nMadison County, NY\nhttp://cnyvitals.org/madison/demographics/language-spoken-at-home/\nState of Hawaii:\nhttps://data.hawaii.gov/Culture-and-Recreation/English-ability-by-language-spoken-at-home/i9hq-hna6\nCanada (from their Census) by Province, Electoral District:\nhttp://data.gc.ca/data/en/dataset/81a2bd6e-622f-4f17-84c1-215216485992\nQueensland, AU by statistical area:\nhttps://data.qld.gov.au/dataset/language-spoken-home-sa4-qld/resource/9fa1fc3a-ab09-4c99-a60f-f95c59269492\nGreater London by borough:\nhttp://publicdata.eu/dataset/first-language-spoken-at-home-borough0e333\nUNESCO Atlas of the World's Languages in Danger:\nhttp://www.unesco.org/culture/languages-atlas/\n", "data request - Mapping between Wikivoyage article names and their Wikidata identifier": "\nThe dump that you want is wb_items_per_site.sql.gz. Though it contains mapping between article titles and wikidata ids for all wikis, so it's relatively big (~1.2 GB compressed).\nAnd it's an SQL dump, so you can either import it into a MySQL database and query that, or you can try to parse it based on the unspecified (but reasonably stable) format.\nSince you're interested in English Wikivoyage, look for enwikivoyage in the  ips_site_id column.\n", "openfda - help diagnose the query result? or a possible bug?": "\nThe problem was a minor one with query syntax: You're using equals signs = when you should be using colons :.\nFor example, adverse_event_flag=Y should be adverse_event_flag:Y.\nThe new query returns only 23 results instead of about 1300.\nThe openFDA API basics page has information about the correct syntax for these queries.\n", "Are there free APIs for searching news articles that I can use to collect trend data in news coverage?": "\nHere's some more news APIs you can look into (that are free or cheap - but may have limits on usage):\nUSA Today API - http://developer.usatoday.com/docs/read/Breaking_News\nThe Guardian - http://www.theguardian.com/open-platform\nThe New York Times API - http://developer.nytimes.com/\n", "geospatial - Expressing restaurant information in RDF/XML": "\nHow you can improve it depends very much on how you want to use it, and who you want it to be useful for. 'Valid' RDF is fine, but may not be useful depending on your intended application! If you want it to be helpful to open/linked types, publishing it using other 'accepted' vocabularies is good (although schema.org is the best catch-all, it is not always as expressive as you want):\n\nFOAF (Friend of a Friend)\nDublin Core\nW3 Geo Ontology\nDBpedia/Freebase\n\nThis also allows any internal applications you have to retrieve extra data from other sources (much as you have done with your wikidata links).\nAnother thing I'd suggest is that you consider how you're publishing the data, and have a look at the W3C Linked Data Platform principles. The two major points here are:\n\nHow you name your RDF resources. Try to give them a URI (in your rdf:resource) that means something rather than just a UUID - this is usually something affiliated with an organisation, or ideally re-using a URI for the same resource if it exists in DBpedia or Freebase.\nPublish your data in friendlier formats (not just RDF/XML) unless you have a reason to only concentrate on RDF/XML. It is a horrible format; Turtle is better in every way and easier to work in (but you should provide both, as well as N-triples). \n\nYou also need to work out where to publish it to (if you want it out in the open) - this varies depending on what you want to do...\n", "data.gov - How can I access FDA drug and medical device recall data from 1970 to 2014?": "\nThe process isn't as nice as the API at openFDA, but you can always file a Freedom of Information Act request:  http://www.fda.gov/RegulatoryInformation/FOI/HowtoMakeaFOIARequest/\n", "finance - Where to find household financial data for my research": "\nif you can use united states residents, you are looking for the survey of income and program participation.  this microdata set contains monthly income about persons, families, and households.  enjoy!\nhttp://www.asdfree.com/search/label/survey%20of%20income%20and%20program%20participation%20%28sipp%29\n", "data request - Database of all Ingress sentences": "\nakiraak has compiled such a database:\nhttps://gist.github.com/akiraak/b7c112e46b79dacfabf1\nIt contains 383 sentences.\n", "data request - Wickes UK stores' postcodes": "\nTo be exact, there are 231 stores listed in the dropdown. I was able to extract (fairly simple) the list of stores and corresponding store IDs. I've included the list in CSV format for your convenience. Each store detailed is listed with the following URL sequence:\nhttp://www.wickes.co.uk/store/\"STORE_ID\"\nIt should be fairly straight forward to write a program to read in the CSV data and step through each store page and extract out the detailed information (e.g. postcodes). When you accomplish that, please add the info to the CSV data and post in github for the benefit of others.\n8276, AINTREE\n8475, ALTON\n8321, ANDOVER\n8360, ASHBY DE LA ZOUCH \n8090, ASHFORD \n8418, ASHTON GATE \n8316, AYLESBURY EXTRA \n8291, BAGULEY \n8277, BANBURY \n8297, BARKING EXTRA \n8247, BARNSLEY \n8256, BARNSTAPLE \n8332, BASINGSTOKE \n8722, BATH KITCHENS & BATHROOMS \n8729, BATTERSEA KITCHENS & BATHROOMS \n8727, BECKENHAM K&B \n8264, BEDFORD \n8454, BEVERLEY \n8319, BEXHILL \n8318, BICESTER \n8298, BIRKENHEAD \n8457, BISHOPS STORTFORD \n8281, BLACKHEATH \n8296, BLACKPOOL \n8279, BLETCHLEY \n8467, BOREHAMWOOD \n8355, BOSTON \n8246, BOURNEMOUTH \n8312, BRACKNELL \n8055, BRADFORD \n8438, BRADFORD IDLE \n8381, BRAINTREE \n8409, BRENTWOOD \n8271, BRIDGEND \n8255, BRIDGWATER \n8230, BRIGHTON \n8077, BRISTOL \n8344, BROADSTAIRS \n8364, BULWELL \n8448, BURGESS HILL \n8404, BURTON SOUTH \n8284, BURY \n8347, CAERPHILLY \n8472, CAMBRIDGE \n8208, CANNING TOWN \n8261, CANNOCK \n8422, CARDIFF PENARTH \n8280, CARDIFF WEST \n8253, CARLISLE \n8083, CATFORD \n8372, CHADWELL HEATH \n8466, CHARLTON\n8233, CHATHAM \n8232, CHELMSFORD \n8074, CHELTENHAM \n8456, CHESHAM \n8262, CHESTER \n8283, CHESTERFIELD \n8413, CHILWELL \n8349, CHIPPENHAM \n8720, CHISWICK KITCHENS & BATHROOMS \n8431, CHORLEY \n8715, CHRISTCHURCH KITCHENS & BATHROOMS \n8724, CLARKSTON KITCHENS & BATHROOMS \n8236, CLIFTON \n8287, COLCHESTER \n8075, COVENTRY \n8717, CRAWLEY KITCHENS & BATHROOMS \n8250, CREWE \n8098, CRIBBS \n8072, CRICKLEWOOD \n8215, CROYDON \n8730, CROYDON KITCHENS & BATHROOMS \n8259, DARLINGTON \n8093, DARTFORD \n8079, DERBY \n8066, DEWSBURY \n8239, DONCASTER \n8240, DORKING \n8252, DUDLEY \n8365, DUMFRIES \n8258, DUNDEE \n8062, DUNSTABLE \n8414, EALING \n8382, EAST GRINSTEAD \n8265, EASTBOURNE \n8354, EDINBURGH \n8322, EDMONTON EXTRA \n8309, EPSOM \n8310, ERITH \n8336, EXETER \n8228, FAREHAM \n8052, FARNBOROUGH \n8479, FOLKESTONE \n8352, GLASGOW \n8366, GLOSSOP \n8377, GLOUCESTER \n8443, GRANTHAM \n8410, GRAVESEND \n8425, GRIMSBY \n8385, GUILDFORD \n8451, HAILSHAM \n8436, HALESOWEN \n8303, HALIFAX \n8235, HALL GREEN \n8216, HANDSWORTH \n8237, HANGER LANE \n8073, HANWELL \n8260, HANWORTH \n8337, HARLOW EXTRA \n8731, HARROW KITCHENS & BATHROOMS \n8338, HAVANT EXTRA \n8353, HAVERFORDWEST \n8222, HAYES \n8474, HEDGE END \n8096, HEMEL HEMPSTEAD \n8473, HENDON \n8367, HEREFORD \n8453, HERTFORD \n8313, HIGH WYCOMBE \n8458, HINCKLEY \n8263, HUDDERSFIELD\n8053, HULL \n8464, HUNTINGDON \n8295, INVERNESS \n8350, IPSWICH \n8326, KETTERING EXTRA \n8428, KINGS LYNN \n8266, KINGSTON\n8251, LANCASTER \n8094, LEICESTER \n8270, LETCHWORTH \n8359, LICHFIELD \n8061, LINCOLN \n8444, LITTLEHAMPTON \n8278, LOUGHBOROUGH \n8317, LOWESTOFT \n8415, MACCLESFIELD \n8314, MAIDSTONE EXTRA \n8449, MALDON \n8368, MANSFIELD\n8214, MERTON \n8429, MILTON KEYNES \n8225, MINWORTH \n8728, MUSWELL HILL K&B \n8348, NEWCASTLE EXTRA \n8290, NEWPORT \n8293, NORTHAMPTON \n8218, NORWICH \n8257, NOTTINGHAM \n8071, NOTTINGHAM CENTRAL \n8452, NUNEATON \n8374, OLDHAM \n8726, ORPINGTON KITCHENS & BATHROOMS \n8234, OXFORD \n8721, PAIGNTON KITCHENS & BATHROOMS \n8369, PENRITH \n8334, PERRY BARR EXTRA \n8248, PERTH \n8209, PETERBOROUGH \n8370, PLUMSTEAD \n8292, PLYMOUTH \n8267, PONTEFRACT \n8273, PRESTON \n8088, PUDSEY \n8082, RAYLEIGH \n8371, READING \n8220, REDDITCH \n8065, ROTHERHAM \n8078, RUISLIP \n8460, RUSHDEN \n8430, SALISBURY \n8064, SCUNTHORPE \n8411, SEVENOAKS \n8097, SHEFFIELD CENTRAL \n8468, SHEFFIELD CRYSTAL PEAKS \n8412, SHEFFIELD NORTH \n8268, SHREWSBURY \n8320, SITTINGBOURNE \n8340, SLOUGH \n8423, SOUTH GOSFORTH \n8441, SOUTH SHIELDS \n8328, SOUTHAMPTON EXTRA \n8719, SOUTHPORT KITCHENS & BATHROOMS \n8238, ST ALBANS \n8299, STAFFORD\n8416, STEVENAGE \n8417, STIRCHLEY \n8282, STIRLING \n8081, STOCKPORT \n8226, STOCKTON \n8231, STOKE \n8254, SUNDERLAND \n8067, SUTTON (ASHFIELD) \n8482, SUTTON (LONDON) \n8323, SWANSEA EXTRA \n8342, SWINDON EXTRA \n8331, TAMWORTH\n8311, TAUNTON EXTRA\n8343, TELFORD \n8357, THETFORD \n8091, TOTTENHAM \n8275, TROWBRIDGE \n8407, TRURO \n8723, TUNBRIDGE WELLS KITCHENS & BATHROOMS \n8455, TUNSTALL \n8405, TWICKENHAM\n8330, UXBRIDGE \n8327, WAKEFIELD \n8229, WALLSEND \n8243, WALTHAM CROSS \n8301, WARRINGTON \n8718, WARWICK KITCHENS & BATHROOMS \n8227, WATERLOOVILLE \n8245, WATFORD \n8345, WEMBLEY \n8716, WEST HAMPSTEAD KITCHENS & BATHROOMS \n8461, WEST WICKHAM \n8329, WESTON SUPER MARE \n8289, WIGAN \n8059, WIMBLEDON \n8465, WINNERSH \n8459, WINSFORD \n8080, WOKING \n8437, WOLVERHAMPTON \n8406, WORCESTER \n8434, WORKSOP \n8221, WORTHING \n8308, WREXHAM \n8274, YEOVIL \n8471, YORK\nI took a look at the HTML code at the store page. It is pretty easy to identify the store name, address and postcode from the javascript code for placing a pin on the Google Map. You get the extra benefit that it has the lat/lon coordinates as well. The snippet looks like:\n        var storelatitude = '51.157364';<br/>\n        var storelongitude = '-0.956997'; <br/>\n        var storename = 'ALTON';<br/>\n        var storeaddressline1 = 'WICKES UNIT 1, ALTON RETAIL PARK';<br/>\n        var storeaddressline2 = 'MILL LANE';<br/>\n        var storeaddresstown = 'ALTON';<br/>\n        var storeaddresspostalCode = 'GU34 2QS';<br/>\n        var storeaddresscountryname = 'United Kingdom';<br/>\n\n", "data request - List of common foodstuffs/meals?": "\nThe 2002 UK food nutrition study (McCance & Widdowson's Composition of Foods) lists 3000+ food items along with dietary information. The link below provides this info in an Excel spreadsheet (which can be converted to CSV) at the bottom of the page.\nhttp://tna.europarchive.org/20110116113217/http://www.food.gov.uk/science/dietarysurveys/dietsurveys/\n", "\"The API response was an error\" in OpenFDA search query": "\nWhen I run the query, using an ordinary web browser:\nhttps://api.fda.gov/device/event.json?search=date_received:[20120101+TO+20141231]+AND+device.device_report_product_code:LLZ+AND+product_problem_flag:Y+AND+event_type:\"Death\"&limit=1\nThe API returns:\n\n{\n  \"error\": {\n    \"code\": \"NOT_FOUND\",\n    \"message\": \"No matches found!\"\n  }\n}\n\nI believe the message the original poster was seeing is on the interactive query explorer on the openFDA website. It manually reprocesses no-results queries into the \"bummer\" error message. I'll log a bug to handle \"no results found\" cases.\nIn this case, neither the API nor the query are wrong. There are no results for the time period specified.\n", "data request - Is there a free list of English word phonetics?": "\nThe cmudict provides phonetic spellings of a sizable number of American English words.  The CELEX database is a similar project; you can select which data you want and download wordlists at WebCelex.  Part-of-speech data (word class) is also available in CELEX.  The CELEX download interface is somewhat frustrating, but you should only need to use it right once to be able to download what you want.\n", "licensing - Which licence and format should I use for the Norwegian language \"data\" I am creating?": "\nandrew's option for cc0 is spot on, if that is your case.\nas far as formats go, json is ideal in my opinion, especially for web. but csv is just fine. if you want to join team open, go ahead and convert it to open data format (odf) too, which helps encourage others to use .odf and open office.\n", "openfda - what is adverse_event_flag used for?": "\nReporters fill out a paper form when submitting adverse events to FDA; they may not check the right boxes in all cases.\n", "api - How do we relate Adverse Reactions and Drug in Open FDA?": "\nThere is no way to make the connection you seek using the data in the openFDA drug adverse events API. Reports include all suspected adverse reactions, and all known drugs the patient was taking. While the classification of drugs as suspect or concomitant may be helpful, it is not always present, and it is often impossible to know which drug caused a specific reaction, or whether a drug caused the reaction at all. A suspected adverse reaction may actually be part of a course of illness.\nThe documentation at https://open.fda.gov/drug/event/#adverse-event-reports clearly states:\n\nA report may list several drug products, as well as several patient reactions. When a report lists multiple drugs and multiple reactions, there is no way to conclude from the data therein that a given drug is responsible for a given reaction.\n\n", "usa - Querying OSHA Data Returning Unexpected Results": "\nThose datasets were originally set up to support an app challenge targeted specifically at the hospitality, retail, and restaurant industries.  The Enforcement Data site's search isn't as limited.  We've recently lifted that limitation for Wage & Hour's data.  I will be reaching out to the database owner to see if we can do the same with OSHA's.\n", "usa - BEA - how to get employment data by industry?": "\nIn July I wrote to BEA with a similar question--getting regional employment and wages by industry. They confirmed that these tables (specifically, I wanted SQ6N, SQ7N, and SA27N) aren't yet available via API. \"Later this year, I hope,\" they said.\nIn the meantime, I've been getting data from BEA's interactive tool and bulk download page (http://www.bea.gov/regional/downloadzip.cfm) and loading it to our internal API.\n", "government - Irrigating with recycled water: Permissible levels of Na Cl EC SAR BOD etc per water analysis of the water source": "\nThe Food and Agriculture Organization (FAO) of the UN has numerous datasets and papers relating to crop production. This paper 'Water Quality in Agriculture\" discusses recommendations in a wide variety of aspects of irrigation. - \nhttp://www.fao.org/docrep/003/t0234e/T0234E01.htm\nThe FAO's AQUASTAT datasets cover information on water resources per country:\nhttp://www.fao.org/nr/water/aquastat/main/index.stm\nWithin these datasets are data related to waste water treatment for crop production use.\nhttp://www.fao.org/nr/water/aquastat/wastewater/index.stm\n", "data request - Dataset about Japanese companies": "\nAswath Damodaran, Professor of Finance at the Stern School of Business at New York University, has been compiling corporate data on corporations worldwide into (FREE) datasets and providing them online since 1998.\nYou can find this information on Japanese firms (3258 companies), as well as other countries at this page:\nhttp://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html\nThe Japan Statistics Bureau publishes annual aggregated data on R&D expenditures by industry sector. The information is available online starting with 2004:\nhttp://www.stat.go.jp/english/data/kagaku/index.htm\n", "data request - Dataset of multiple judges or examiners giving scores": "\nIt may be hard to find a dataset that matches your exact criteria, but there are some promising open datasets with essay scoring.\n\n\nKaggle Automated Essay Scoring, (link to data, requires registration)\n\n\nThe data will contain ASCII formatted text for each essay followed by one or more human scores, and (where necessary) a final resolved human score.\nWhere it is relevant, you are provided with more than one human score, so that you may evaluate the reliability of the human scorers\n\nYou can find code for benchmarks here.\n\n\nInternational Corpus of Learner English, (link to data)\n\n\nData available on this page include annotated organization scores for 1,003 essays from the International Corpus of Learner English (ICLE).\n\n", "data.gov - Gasoline station dataset": "\nThere are several sources you could compile information from. \nCity data portals that list gas station locations:\nNew Orleans: https://data.nola.gov/Administrative-Data/NOLA-Gas-Stations-Map/ic3z-bztr\nWashington, DC: https://opendata.socrata.com/dataset/Gas-Stations-in-Washington-DC/tk2x-chx8\nState data portals that list alternative fuel stations:\nOregon (Clear Fuel): https://data.oregon.gov/Recreation/Local-clear-Gas-stations/if4z-s7kp\nMaryland (Alternative Fuels): https://data.maryland.gov/Energy-and-Environment/Public-Electric-Vehicle-Charging-Stations-and-Alte/7yut-5ayv\nThe Department of Energy has a dataset that lists the location of 19,000 alternative fuel stations:\nhttp://www.afdc.energy.gov/data_download\nAnd you could do a dump from openstreetmap for locations of fuel stations that have been crowdsourced. This link should give you info on how to do that.\nhttp://wiki.openstreetmap.org/wiki/Tag:amenity%3Dfuel\n", "data request - Electricity and (space) heating load curves for a city": "\nGreenButtonData.org (open source) is compiling this type of information from its users. They currently claim that upto 60 million households can access their monitoring/analysis system via partnerships with utility companies. \nThey have a REST API, but I do not know how much aggregated data is available yet. They have though given some datasets (or samples), including hourly loads, to openEI (crowd sourced project for energy usage/information - http://en.openei.org/wiki/Main_Page)\n32 day hourly sample - http://en.openei.org/datasets/node/901\nUPDATE: \nBelow is the dataset for monthly energy usage and CO2 emissions for Boston's municipal buildings.\nhttps://data.cityofboston.gov/dataset/Municipal-Energy-Data/bcnb-bux2\nBelow is the datast for annual energy and water usage and CO2 emissions for NYC's buildings > 50K square feet.\nhttps://data.cityofnewyork.us/Environment/Energy-and-Water-Data-Disclosure-for-Local-Law-84-/5gde-fmj3\n", "city - Data on demographics at the neighborhood level in Oakland": "\nOakland has an open data portal. This dataset contains the outlines (Shapefile or KML) for the neighborhoods.\nhttps://data.oaklandnet.com/Property/Oakland-Neighborhoods/7zky-kcq9\nUnfortunately it has very little data related to what you are looking for. Where it does, the datasets lack Lat/Lon coordinates. One would need to bulk geocode the datasets and check which neighborhood (e.g., KML polygons) the location fits into.\nYou can get the KML files for Census Tracts and Block Groups from the US Census at this link:\nhttps://www.census.gov/geo/maps-data/data/tiger-kml.html\nCombining the two you should be able to map any demographic resource based on Census Tracts/Blocks to the Oakland neighborhoods. If you do go through this effort, I would suggest making the mapping of the census tract/blocks mapping to neighborhoods publicly available in Github.\n", "data request - Enron Email Dataset in MySQL": "\nAfter some additional search, I found the worked link among of lots, which point to 404 pages.\nThe worked ones are:\n\u2014 https://s3.amazonaws.com/rjurney_public_web/images/enron.mysql.5.5.20.sql.gz\n\u2014 http://www.ahschulz.de/pub/R/data/enron-mysqldump_v5.sql.gz\nDetailed information about these datasets:\n\u2014 http://hortonworks.com/blog/the-data-lifecycle-part-one-avroizing-the-enron-emails/\n\u2014 http://www.ahschulz.de/enron-email-data/\nP.S. Don't know why, but for me the download process was very slow.\n", "Image sensor response data": "\nThis PDF contains a graph showing the quantum efficiency across all wavelengths from 380 nm to 1050 nm:\n\nNotes:\n\nData is for the MT9M034 1/3-Inch CMOS Digital Image Sensor\nIt is a graph, so values have to be sampled from it, and precision is not great\nIt is not open data\n\n", "data request - Database of bicycle stations in Minato (Tokyo)": "\nThe service's map shows a map of stations: http://docomo-cycle.jp/minato/map/\nReading the JavaScript source code leads to a JSON file which contains the data with location names in Japanese:\nhttp://docomo-cycle.jp/minato/system/data/portnavi.json\nThe location names in English are found by prepending 'en' in front of portnavi.json:\nhttp://docomo-cycle.jp/minato/system/data/enportnavi.json\nEach participating ward uses the same URL syntax. Replace 'minato' with the other ward(s) name for their information.\nThe wards are:\nchiyoda\nyokohama\nsendai\nThis data is not always up-to-date, though, some stations are missing.\n", "machine learning - Data for web security/database security issues": "\nCVE is a freely usable database of security issues, many of them related to web/database.\nThe CVE database can be downloaded at https://cve.mitre.org/data/downloads/index.html as XML, CSV, and other formats.\nA typical entry includes:\n\nName\nDescription\nStatus\nPhase\nReferences\n\n", "data request - Fairtrade open dataset, UK": "\nThe UK Department for Environment, Food and Rural Affairs publishes a weekly price index for commodities:\nhttps://www.gov.uk/government/statistical-data-sets/commodity-prices\nAnd here is the weekly dataset for weekly wholesale prices for vegetables and fruits:\nhttps://www.gov.uk/government/statistics/wholesale-fruit-and-vegetable-prices\nThe European Commission keeps data related to cost of goods vs. cost of labor for countries in the EU in the EuroStat database.\nhttp://epp.eurostat.ec.europa.eu/portal/page/portal/statistics/themes\nThe National Accounts (with GDP) will provides indexes on current prices relative to GDP.\nhttp://epp.eurostat.ec.europa.eu/portal/page/portal/product_details/dataset?p_product_code=NAMA_GDP_C\nThe Agriculture section contains data related to farm labor costs, income, price and production.\nhttp://epp.eurostat.ec.europa.eu/portal/page/portal/agriculture/data/database\nBeow is a link of the [historical] annual difference of price of consumables and builder's wages in England from mid 1200s and 1885. It is an aggregation from a number of compiled acacdemic sources. The chart shows that as government (and corresponding taxes) evolved in the development of England that the gap increases each year.\nhttp://esfdb.websites.bta.com/table.aspx?resourceid=11484\n", "data request - Federal and State Road Information": "\nFor the national highway system you will find all the locations/polylines in the National Highway Planning Network dataset at:\nhttp://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/publications/national_transportation_atlas_database/2014/polyline\nYou will also find a girth of geocoded data related to the national transportation system (hubs, railways, airports, etc) under the Bureau of Transportation Statistics - Geospatial Information\nhttp://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/subject_areas/geographic_information_services/index.html\nAs far as state routes, I think you will still have to get that data from each state's transportation or geographic survey sites. Here's a few:\nKansas: http://gisinventory.net/GISI-5224-Road-and-Street-Centerlines---Highways---This-dataset-is-a-single-centerline-highway-network-representation-of-the-10000-miles-Kansas-State-Highway-System-(Interstate-U.S.-and-Kansas-routes)..html\nMinnesota: http://www.dot.state.mn.us/maps/gdma/gis-data.html\nNew Jersey: http://www.state.nj.us/transportation/gis/metadatafaq.shtm\nNew York: https://gis.ny.gov/gisdata/inventories/member.cfm?OrganizationID=539\nUtah: http://gis.utah.gov/data/sgid-transportation/roads-system/\nIf you end up compiling all the state data together, I recommend placing it in github for the benefit of all.\nUPDATE: I had forgotten that I had a web page with a collection of state transportation sites (lol):\nhttp://www.opengeocode.org/opendata/traffic.php\nOne may also find other transportation related data portals at the crowdsourced catalog of open data portals (I am a co-founder of) by selecting the category Transportation:\nhttp://www.opengeocode.org/opendata\n", "data request - Open datasets for affiliation networks between African farmers?": "\nThe UN Food & Agriculture Organization (FAO) collects and publishes a lot of data on Africa related to farming. \nHere's the entry point into the FAO's Statistics Database (FAOSTAT):\nhttp://faostat.fao.org/\nHere's the link to the World Bank's datasets on Farming in Rural Areas of the World:\nhttp://data.worldbank.org/topic/agriculture-and-rural-development\nThe CGIAR/CCAFS program at Harvard University collects and publishes data surveys related to Agriculture in Africa:\nhttps://thedata.harvard.edu/dvn/dv/CCAFSbaseline\nHere's an example of where Columbia University has used the FAO data:\nhttp://iridl.ldeo.columbia.edu/maproom/Agriculture/Farming_Systems/Africa/\nHere's an example of where Australian Centre for International Agricultural Research has used FAO data on farming in Africa\nhttp://aciar.gov.au/files/node/14087/mapping_farming_systems_in_africa_21_june_2012_16871.ppt\nHere's a dataset from 2012 covering 35 million hectacres of farm land in 66 countries that has been bought up by foreign firms:\nhttp://www.grain.org/article/entries/4479-grain-releases-data-set-with-over-400-global-land-grabs\nFOR AFFILATIONS:\nNATIONAL AFRICAN FARMERS\u2019 UNION (NAFU)\nAfrican Farmers Association of South Africa (AFASA) - http://www.afasa.za.org/\nEast Africa Farmer's Federation - http://eaffu.org/eaffu/\n", "usa - Flood Plain API": "\nYou can also emulate/reverse engineer the form at https://www.floodsmart.gov/floodsmart/pages/landing_pages/landing0000_1.jsp using the following curl command. This isn't an API, per se, but it does get you the information you're asking for programatically.\ncurl 'https://www.floodsmart.gov/floodsmart/oneStepFloodRiskAddressSearch.action' -H 'Content-Type: application/x-www-form-urlencoded' --data 'nav_address=2100+Clarendon+Blvd&nav_city=Arlington&nav_state=VA&nav_zipCode=22201&nav_residential=Y&x=23&y=10'\nYou'll then want to look for <div class=\"hide-span flood-risk-profile-header clearfix\"> in the resulting HTML.\nYou can do this from your browser using a tool called hurl.it and following the screenshot below.\n\n", "data request - Infectiousness vs. deadliness for various diseases": "\nBoth the US CDC and the World Health Organization have databases for statistics on infectious diseases. The entry points to the online databases are:\nCDC http://wonder.cdc.gov/datasets.html\nWHO http://apps.who.int/globalatlas/DataQuery/default.asp\nhttp://who.int/research/en/\nThis is the EU Commission's portal on Communicable Diseases. I don't think you will find statistical data here, but lots of links to other resources:\nEU Commission http://ec.europa.eu/health/communicable_diseases\n", "data.gov - How can I find the missing information from openfda?": "\nFor example,\nhttps://api.fda.gov/drug/event.json?search=missing:patient.patientsex+AND+patient.drug.openfda.brand_name:advil\nand also please add up total reports=male(total) + female(total) + unknown(total) + missing(total). If you do it you might get the right answer.\n", "api - Is it possible to get the exact (particular) information from openFDA?": "\nAt this time it's not currently possible to have the API return only the meta section. You can, as others have pointed out, restrict the API to return only one record along with the meta section. Let us know if this proves to be a problem for your use case.\n", "finance - Where does Allrecipes.com get its \"On Sale\" data?": "\nAfter taking a look at the source code of the widget on a sample recipe page (selected at random by me), it appears that they are using http://corp.groceryserver.com/ as a data provider\n\nFor example, the recipe at http://allrecipes.com/Recipe/Sherry-Braised-Beef-Short-Ribs/Detail.aspx makes an AJAX request to http://allrecipes.groceryserver.com/groceryserver/service/w5fDoHvC... which returns JSON about the sales. Note: I tried to base64 decode that string and some variations but didn't get very far. \n", "data request - A list of cities of each country": "\nMy open data project (I am a co-founder) has a free list of all the cities in the world, along with their area centroid (lat/lng), as a CSV file. It is compiled from the USGS/GNIS (US) and NGA/GNS (non-US) databases. \nhttp://www.opengeocode.org/download.php#cities\nAs an alternate source, the United Nations Statistical Division publishes an annual yearbook on world statistics. Table 8 has the population of cities > 100,000\nhttp://unstats.un.org/unsd/demographic/products/dyb/dyb2012/Table08.xls \nWe have a version of it converted to ur Linked CSV format/vocabulary:\nhttp://www.opengeocode.org/cude1.1/UN/UNSD/dyb2012-pop100k.zip\nMETADATA (dyb2012-pop2k)\n\n(Empty)\nISO 3166-1 alpha-2 country code (e.g., US => United States)\nNational Geospatial Intelligence Agency (NGA) Geographic Name Server (GNS) Feature Code (e.g., P = Populated Place Type Feature)\nNGA/GNS Feature Designation Code (e.g. PPL = Populated Place (incorporated))\nExtended Feature Description (e.g., city, capital)\nTotal Area in Square Kilometers\nISO 639-1 language code for language that name field is in (e.g., lc = local language native to the country)\nLanguage Script for name fields (e.g., latin, arabic, chinese)\nShort Name (Gazetteer) for City\nYear of Population Statistics\nTotal Population (e.g., within city proper)\nUrban Population (e.g., within agglomerated area of city)\nTotal Male Population\nTotal Female Population\n\n", "data request - Historical forward exchange rate between $ and yen.": "\nIf you're just looking for FOREX data, http://ratedata.gaincapital.com/ has a ton of it. FOREX assumes you will be taking delivery in 2 days (or 1 day in some cases).\nI read your question as \"if I want to buy yen at today's price, but delivered (and paid for) in 6 months, how much will it cost?\". Is my understanding correct?\nIf so, you're talking about currency futures, which involve rollover rates.\nIf US banks are paying a higher interest rate than Japanese banks, there is a holding cost for the person who has the yen, even if the exchange rate remains the same. So, you have to adjust today's exchange rate for the interest the yen-holder will lose (compared to the dollar holder) in 6 months.\n", "data request - A DB of banks for each country": "\nThe FDIC has a data download of all finanically insured banks and their branch offices in the US. The data does not have though website and phone number. It does have name, address, institution type, deposits.\nhttps://www2.fdic.gov/IDASP/warp_download_all.asp\nThe FDIC dataset on Data.gov has some additional fields, including website (but not phone):\nhttp://catalog.data.gov/dataset/fdic-institution-directory-id-insured-insitution-download-file/resource/df80f510-8c30-421d-8f83-f90f0ebf887b\n", "Where can I find examples of open data being used in business?": "\nThe answers to the question on open data stories might also be useful to you.  I've summarized some of those and added some more (courtesy of Anastasios Ventouris, pattern-recognition, Charles Worthington, Alisha Green, Taal, tobip, fgregg, Diabolus, Rebecca Williams)\n\nThe Open Data 500 is a collection of 500 companies that have built a business model on open data (both government and non-government data). Note that the Open Data 500 is expanding to include other countries as well.\nHighlights on Data.gov show companies, civil society groups, non-profits, and citizens using open government data \nData.gov.uk has economic and civil society case studies \nMcKinsey report on Open Data: Unlocking innovation and performance with liquid information has many examples in education, transportation, consumer products, electricity, oil and gas, health care, and consumer finance\nCode For America's e-book Beyond Transparency \nData for Good, a platform for sharing data-driven projects for social good\nOpen Knowledge Foundation's community stories \nSunlight Foundation's Data Deep Dives \nDan Nguyen's Small Data Journalism Readings articles for his data journalism class at NYU\n\n", "data request - Where can I get standard iOS icon collection in PDF format?": "\nUsing a Advanced Google Image Search, you can search based on file type (SVG, not PDF) and also usage rights. If you want a common theme for all buttons, then you can use this tool to find single websites that host similarly designed images that meet your criteria.\nHere is an example of the Resize Buttons with SVG. You'll have to filter by license for your use.\n\n\nTo find PDF files, you can't use the Image Search tool but you can use Google Advanced Search, and specify PDF as the file type.\n\nTo create or edit vector files (i.e. PDF or SVG), there is a very professional open source program called Inkscape (similar functionality to Adobe Illustrator). With this tool, you can also easily make your own buttons in vector format. You can also easily convert SVG to PDF, using tools described here. \n", "geospatial - Airport / airline data from all over the world": "\nA quick google search found me the data page of OpenFlights.org. This page has a free (donation request) CSV file with 8000+ airports: LINK.\nOpenFlights.org points to OurAirports.com, which provides extensive CSV downloads with data being in the public domain. See their data page.\n\nRegarding contact info for airport management, the FAA provides this info for US airports: LINK. Use this form to filter and then scroll down to the download section:\n\nHere is a direct link to the Airport Facilities Excel file (6.5 MB). Screenshot below. Lots of contact info when you scroll right.\n\nThere is also a 'download data' link but it requires registration.\n", "data request - Global calendar of Open Source-related events": "\n\nHackathon Watch - not specific to open source or open data though\nEuropean PSI / Open Data Events - European, not global\nLanyard's Open Data and Open Source lists (can be saved to your calendar, see the right column)\n...\n\n(please edit if you want to add a calendar)\n", "api - Drug time series produces unexpected results": "\nThe short answer is that when you do the following API call, you see that bextra shows up in the patient.drug.medicinalproduct instead of patient.drug.brand_name.\nhttps://api.fda.gov/drug/event.json?search=receivedate:[20050101+TO+20050430]+AND+bextra&limit=100\nSo I suggest doing the following:\nhttps://api.fda.gov/drug/event.json?search=receivedate:[20050101+TO+20050430]+AND+(medicinalproduct:bextra%20OR%20brand_name:bextra)&count=receivedate\nYou can read up on why this is at https://github.com/FDA/openfda/issues/22\n", "usa - Opening up Federal Trade Commission data on Safe Harbor": "\nI checked out the webpage. There is a 'Export to Excel' link on the page. I tried it. It downloads a file called 'OrganizationList.xls'. But you cannot open it in Excel. I checked the contents of the file and it is an HTML table. To view the file, change the file suffix to '.html'. \nAs far as extracting the data, I would use a convert HTML table to CSV tool. Below is one site that I tried with this file and it appeared to do a correct conversion:\nhttp://www.convertcsv.com/html-table-to-csv.htm\n", "How to get access to half-open subscription-only data?": "\nI can't speak for other libraries, but at the university library where I work at in the United States...\nIf we had a CD copy of this, you would need to get an account with us (which would cost $25 and be restricted to residents of the state) to be able to check it out.\nIf we had online access to this, you'd need to come by in person to one of our libraries and use one of the guest computers. Typically we allow guests access for an hour a day but for folks doing research, we'd gladly extend that time if their research intent was made clear. Folks not affiliated with the university can't get off-site access to our electronic subscriptions per our contracts with the vendors.\n", "data request - Hospital originated infections and mishaps": "\nThe data referenced appears to be an aggregation of Hospital Acquired Infections which are part of the CMS Hospital Compare family of data sets.\nThe various measures included in the dataset are listed at  http://www.medicare.gov/hospitalcompare/Data/Measures-Displayed.html \nThere are a number of ways to access the raw data which are listed at http://www.medicare.gov/hospitalcompare/Resources/Download-Data.html including zipped archives of CSVs, APIs at data.medicare.gov, and more.\nIf you're going to visit one line.. go to https://data.medicare.gov/Hospital-Compare/Healthcare-Associated-Infections-Hospital/77hc-ibv8\n", "api - How can I get accurate data re Adverse Events?": "\nActually, Andrew from OpenGeoCode's answer isn't quite correct; it is a much wider query than expected. I'll explain why.\nFirst, to the OP: This is exactly what is necessary to answer your original question\u2014a pasted link to the queries you used to arrive at your results! Otherwise there's no way to reproduce the queries.\n\nhttps://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct.exact:EthinylEstradiol+Norelgestromin+patient.drug.medicinalproduct.exact:Ethinyl%20Estradiol+Norelgestromin&count\nThis returns about 83k results. Why so many? Here's what's going on.\nThe API is going to search for:\n\npatient.drug.medicinalproduct.exact:EthinylEstradiol\nAll records where medicinalproduct contains exactly \"EthinylEstradiol\" 0 results\n+Norelgestromin\nBecause Norelgestromin is \"loose\" in the query\u2014it's not prefixed by a field to search in\u2014the API will search in every field, in every record, for the word Norelgestromin ~14k results.\n+patient.drug.medicinalproduct.exact:Ethinyl\nBecause there's a space between Ethinyl and Estradiol, but these are not grouped with quotation marks to indicate an exact phrase match is desired, the API searches for all records where medicinalproduct contains exactly \"Ethinyl\" 0 results\n%20Estradiol\nAgain, this is \"loose\" in the query so the API does a big keyword search across all fields for the word Estradiol, wherever it may be found. ~83k results\n+Norelgestromin\nAgain, this is \"loose\" in the query so the API does the same broad keyword search it did before. No new records are returned.\n&count\nThis does nothing now. count needs a parameter\u2014a field to count. i.e. count=patient.drug.medicinalproduct.exact will return a list of the top medicinalproduct entries across the matching records; it's a good way to ballpark your query logic and see whether you're casting too wide a net.\n\n\nHere are different, more specific queries.\n\nThis first one is a much more precise intersection of two ingredients, but may not capture all the misspellings or combinations that are possible.\nhttps://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:(%22EthinylEstradiol%22+AND+%22Norelgestromin%22)+patient.drug.medicinalproduct:(%22Ethinyl%20Estradiol%22+AND+%22Norelgestromin%22)\nIn this query (about 30 results) we're looking for any records where the medicinalproduct contained BOTH \"Ethinyl Estradiol\" AND \"Norelgestromin\" or contained BOTH \"EthinylEstradiol\" AND \"Norelgestromin\".\nHere's a more compact version of the same query, using more parentheses to group all the things we want to search for within medicinalproduct:\nhttps://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:((%22EthinylEstradiol%22+AND+%22Norelgestromin%22)+(%22Ethinyl%20Estradiol%22+AND+%22Norelgestromin%22))&limit=25\nNote that the + is just a \"space\" and to the API, it's an implicit OR. In other words, if you just write a bunch of words with spaces between them, the API is going to try to match the biggest number of records that contains any of those words. You can use quotation marks and the keyword AND to be more specific.\nThis second one is more like what I think Andrew from OpenGeoCode was going for\u2014a union of records that match ANY of \"Ethinyl Estradiol\" OR \"EthinylEstradiol\" OR \"Norelgestromin\". It returns about 7k records.\nhttps://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct:(%22EthinylEstradiol%22+%22Ethinyl%20Estradiol%22+%22Norelgestromin%22)&limit=25\nRemember, the parentheses are grouping all the things we want to look for WITHIN medicinalproduct, and the + between them is an implicit OR.\n\n", "How to download all datasets from a CKAN instance using the API?": "\nThis is not supported by the CKAN API, so you would need to script it.\n", "data request - Dataset of sentences translated into many languages": "\nTatoeba.org is exactly that: http://tatoeba.org\nTheir data is human-edited and used by dozens of products/websites including electronic dictionaries, so it is of reasonable quality.\nIt has 471468 English sentences and 179 languages.\n(not all sentences are translated to all languages though, far from that)\nThe structure is not a simple 1-1-1, a sentence can have several translations.\nFor your example, see http://tatoeba.org/eng/sentences/show/406004 :\n\nThe whole data is open (Creative Commons CC-BY) and downloadable at http://tatoeba.org/eng/downloads in various formats.\nDisclaimer: I am a member of Tatoeba (I mentored a GSoC that created a webapp to enrich your Anki decks using Tatoeba)\n", "Data request: NBA data to practice statistical programming": "\nReputation too low to comment (could someone please change this into one?).\nFor older data: http://www.databasesports.com/\n", "data request - Video game dataset": "\nGiant Bomb (API info) is probably going to be your best bet. They have a very large database of games, and they keep track of details like release date, genre, platforms, publishers, franchises, characters, locations, etc. However, they restrict their API to non-commercial use and 200 requests/hour.\nAnother one to check out is TheGamesDB (API info). Their database doesn't support some of the more advanced stuff Giant Bomb has like locations, characters, etc, but they are much less restrictive with their data. Unfortunately, their API is known for having a fair bit of downtime -- I wouldn't recommend relying on them for a user-facing app.\nThere's also IGDB (API info), which is also free and emphasizes that they allow commercial use.\n", "data request - IMDb users (reviews, watchlist, and ratings) dataset": "\nUsers-related data is not currently released.\nOne option could be screen-scraping, but it is forbidden by the usage conditions:\n\nRobots and Screen Scraping: You may not use data mining, robots, screen scraping, or similar data gathering and extraction tools on this site, except with our express written consent as noted below. \n\n", "usa - Basic Financial Data for Non-Governmental Organizations (NGOs)": "\nSelected financial information on all entities that file using form 990 with the IRS is public. A variety of datasets can be found in the IRS Statistics of Income (SOI). I would start at this page:\nhttp://www.irs.gov/uac/SOI-Tax-Stats-IRS-Tax-Exempt-Organization-Population-Data\n", "data request - Is there a publicly available database of all Apple products?": "\nApple provides this information in on their Trademarks site, which includes all Apple product names and descriptions (an easy scrape off the page)\nYou reference MacTracker, but that does provide what you need (latest database). The terms are relatively lenient for use.\n", "geospatial - R package for geographic regions": "\nThe best place to find classes of R packages is with a task view:\nhttp://cran.r-project.org/web/views/\nWithin task views, the Spatial view is going to have a large number of options:\nhttp://cran.r-project.org/web/views/Spatial.html\nPart of the complexity is that there are different levels of granularity or zoom levels.  You probably are looking for something that has polygons for nations and polygons for states/provinces within nations.  That would make the \"maps\" and \"mapsdata\" packages most relevant,\n[I don't have enough reputation to post links to those]\nThe best way to learn R tools is with vignettes, so a search for \"R vignette maps mapdata\" is a great way to start.\nMapping data is a big ball of wax, because many different disciplines have very different needs for their data, hence you have tools from the GIS community, spatial statistics, geology, political science, etc.  The \"Applied Spatial Data Analysis with R\" is a nice overview of working with spatial data in R.\n", "data request - List of all restaurants in a city (EU)?": "\nHere is my obligatory pointer to overpass turbo, a \"graphic\" way to explore the data quality for certain POIs (points of interest) in the OpenStreetMap database.\nThis examplary query for amenity=restaurant in Rome (patience: the query takes several seconds to execute) yields (probably incomplete) 543 locations with varying metadata.\n\nHow-to: For a given POI, find out its most canonical way of tagging through the page Map Features on the OSM Wiki. Then use the \"Wizard\" button on overpass turbo to generate the query. Browse the map to a suitable test location and hit execute. The map then can be navigated and the POIs inspected for data quality. \n", "data.gov - Is there any open data FDA, or other, regarding U.S. Pharmacopoeia standards?": "\nI found a definition from the NIH site related to humans and other animals. (PDF link)\n\nPharmaceutical-grade compound: A pharmaceutical-grade compound (PGC) is defined as any active or inactive drug, biologic or reagent, for which a chemical purity standard has been established by a recognized national or regional pharmacopeia (e.g., the U.S. Pharmacopeia (USP), British Pharmacopeia (BP), National Formulary (NF), European Pharmacopoeia (EP), Japanese Pharmacopeia (JP), etc.). These standards are used by manufacturers to help ensure the products are of the appropriate chemical purity and quality, in the appropriate solution or compound, to ensure stability, safety, and efficacy. The Food and Drug Administration (FDA) maintains a database listing of FDA approved commercial formulations for both FDA approved human drugs (the Orange Book) and veterinary drugs (the Green Book).\n\n", "data request - Sample dataset: files of a typical company": "\nI think there are too many diverse filetypes to expect a good set. For example, wikipedia has many listed at List of File Formats, with this disclaimer:\n\nThis is an incomplete list that may never be able to satisfy particular standards for completeness. \n\nBut, if you make a list of possible extensions (see here from a list of lists, and here for A-E), you can pass them all in a big loop to a search engine with a \"filetype:abc\" argument (and download the results).\nThe problem is that I don't know of a search engine that has robust filetype searching. I checked both Google and DuckDuckGo and neither found actual files for the few I checked.\nSo, this isn't really an answer, but perhaps someone knows a search engine for robust filetype searching. Hopefully that search engine also has a way to filter based on license.\n", "data request - Where to download the Forvo database?": "\nI guess because it's part of Anki, it's somewhat related, but I just stumbled on this\nA half-automatic Forvo Downloader\n\n\nYou select the field you want the audio to go into and press an editor button or keyboard shortcut.\n\nThe addon will open the relevant forvo.com page in your web browser.\n\nYou decide on one or more pronunciations and download them the regular way in your web browser.\n\nThe addon will automatically pick up the files from your web browser's downloads directory and insert them into the field.\n\n\n\nSource code: https://github.com/yunidatsu/anki-forvodl\n", "data request - Credit card metadata database": "\nif you need a free bin database, you can get it here:\nhttps://getcreditcardonline.com/free-bin-database-download/\nData in CSV, JSON, SQL, TXT, Excel formats, etc.\n", "data request - Taxation systems across countries": "\nThis EU documentation shows various Value Added Tax rates for countries in Europe:\nhttp://ec.europa.eu/taxation_customs/resources/documents/taxation/vat/how_vat_works/rates/vat_rates_en.pdf\n", "Why does a report listing a generic drug (drospirenone and ethinyl estradiol) have multiple brand names listed in the drug\u2019s openfda section?": "\nWhat's in a drug name?\nIt turns out that drug names are a complicated business. Drugs are known by a generic name (usually the name of the active ingredient)\u2014like IBUPROFEN, and often multiple brand names\u2014like ADVIL, or MOTRIN; even IBUPROFEN itself can be a brand name, like NIGHTTIME IBUPROFEN. Drugs may also have multiple ingredients, so the generic name may describe all of those ingredients\u2014like ACETAMINOPHEN/CODEINE.\nIt\u2019s important to note the difference between two \u201cdrug name\u201d fields in openFDA\u2019s drug adverse events API\u2014medicinalproduct and openfda.brand_name.\nmedicinalproduct\nThis is imported from the original FDA adverse drug event report. It notes the drug name as reported, and even for a single \u201cdrug\u201d there can be wide variation among records. For instance:\n\nGeneric or brand. Sometimes reporters write ACETAMINOPHEN, and sometimes TYLENOL.\nFormat. For the same drug, you might see ACETAMINOPHEN/CODEINE or CODEINE/ACETAMINOPHEN or ACETAMINOPHEN + CODEINE or CODEINE WITH ACETAMINOPHEN or ACETAMINOPHEN AND CODEINE.\nMisspellings. IBUPROFEN appears in about 40,000 reports but even IBUPROPHEN (a misspelling) appears in almost 60! It\u2019s not possible to know a priori all the ways that humans might have misspelled drug names when reporting.\n\nBecause of all this variation, you can imagine that it might be difficult to find ALL the records for a particular drug. It\u2019s not even easy to capture all the variation in correctly spelled drug names, never mind the misspelled ones.\nopenfda annotations, which are added to the original record (like a sticky note), help by listing other known names and codes for the drug that was written in medicinalproduct. If the openFDA system was able to match a correctly spelled drug product name, it\u2019ll add an openfda section for each drug; this section sits alongside medicinalproduct, and doesn't replace it. This is called \u201charmonization\u201d in openFDA.\nThis section makes it easier to search for reports by those names and identifiers\u2014no matter what name someone used in the original report. However, when you search in openfda fields, you\u2019re distinctly NOT searching for the name used in the original report. You\u2019re casting a wider net.\nopenfda.brand_name\nThis is a list of brand names that a drug may be known by. If you search in medicinalproduct for IBUPROFEN, a record may be returned with a long openfda.brand_name list including dozens of possible names that IBUPROFEN is marketed under, including:\n\"PAIN RELIEF ANTI INFLAMMATORY\", \"ADVIL\", \"HEALTH SENSE INFANTS IBUPROFEN ORAL SUSPENSION\", \"DOVER ADDAPRIN\", \"HEALTHY ACCENTS IBUPROFEN CHILDRENS\", \"PROFEN IB\", \"SHOPRITE IBUPROPHEN\", \"JUNIOR STRENGTH ADVIL\", \"PEDIACARE CHILDRENS\", \"GOOD NEIGHBOR PHARMACY IBUPROFEN CHILDRENS\" \u2026\n\nThis list isn\u2019t doesn\u2019t necessarily tell you what drug brand the patient was taking, but it gives you an idea of all the products you might want to search for if you want a complete count of adverse events for a particular drug (if you think of the drug as its ingredient, i.e. its generic name).\nSo, now it\u2019s possible to answer the question\u2026\n1. Why are there so many brand names listed in patient.drug.openfda.brand_name, when the original report listed DROSPIRENONE AND ETHINYL ESTRADIOL?\nDROSPIRENONE AND ETHINYL ESTRADIOL is the generic name of the drug YAZ. Although the original report listed DROSPIRENONE AND ETHINYL ESTRADIOL as the medicinal product being taken, the openfda section shows possible brand names (and other information) for the the drug. This particular drug, a combination of two hormones, is marketed under many brand names, all with the same ingredients (but sometimes in different dosage forms and strengths, meant to be taken on different schedules). Those brand names include, but are not limited to:\n`GIANVI, SYEDA, YAZ, OCELLA, LORYNA, YASMIN, VESTURA, ZARAH`\n\nThis Mayo Clinic article lists _even more names that this drug (DROSPIRENONE AND ETHINYL ESTRADIOL) goes by. What\u2019s important to understand is that the openfda annotation is additional information that can be used to learn more about the drug listed in the report\u2014it does not necessarily list the name of the drug the patient was taking.\n2. Why does a search in patient.drug.openfda.brand_namefor one of the brand names\u2014YAZ\u2014include reports listing a generic drug (DROSPIRENONE AND ETHINYL ESTRADIOL)?\nSearching in openfda.brand_name tells the API to look in the annotated list of possible brand names for a drug, not in the original report\u2019s medicinalproduct field. The results include all the records that have an openfda.brand_name list that contains YAZ.\nHere\u2019s why a record would have YAZ in openfda.brand_name:\n\nThe patient.drug.medicinalproduct contained YAZ.\nThe patient.drug.medicinalproduct contained DROSPIRENONE AND ETHINYL ESTRADIOL, which is marketed under many brand names, including YAZ.\n\nhttps://api.fda.gov/drug/event.json?search=safetyreportid:\"4990905-5\"\nFor this report, the original medicinalproduct contains DROSPIRENONE AND ETHINYL ESTRADIOL\u2014the generic name of YAZ. So, openFDA matched the generic name and made an openfda section that lists the known possible brand names for DROSPIRENONE AND ETHINYL ESTRADIOL\u2014including YAZ. That\u2019s why it showed up in a search for YAZ in patient.drug.openfda.brand_name.\n", "data request - Food expiration database": "\nI found a link to the Food Bank of Alabama, which has a 26 page PDF with a table of food expiration dates (pdf link). Here is another similar PDF, but even less machine readable. And here is yet another one (PDF).\n\nIt's not too machine readable, but you may be able to copy individual pages and paste them into a spreadsheet tool.\nIf this data source is useful for you, I can also help get it to be machine readable using a tool like pdftotext.\n", "Why does the number of results change when I search for YAZ in the field patient.drug.medicinalproduct vs. patient.drug.openfda.brand_name?": "\npatient.drug.medicinalproduct and patient.drug.openfda.brand_name are not the same.\nFirst, patient.drug is a list of multiple drugs from the adverse event report. For each drug, there is always a medicinalproduct (name), and sometimes an openfda section.\n\nmedicinalproduct is the drug name as written in the original adverse event report (FAERS record), imported into the openFDA database. It could be a brand name, or a generic name. It might even be misspelled. (cf. IBUPROPHEN which has ~59 results, instead of IBUPROFEN which has ~40k results.)\nbrand_name is part of the openfda section. It\u2019s an annotation, like a \u201csticky note,\u201d added to each item in patient.drug. It has additional possible names and codes that can be used to identify the drug in medicinalproduct. It\u2019s only added if the drug name in medicinalproduct was spelled correctly, and the name matched in certain other FDA and NLM datasets; openFDA calls the process of adding this section \u201charmonization.\u201d\n\nWhy do the searches have different result counts? Four ways to search, with explanations.\nIn this case, YAZ is one of the many brand names for a certain generic drug, DROSPIRENONE AND ETHINYL ESTRADIOL. In fact, many brands contain the same ingredients, but in different quantities and forms: GIANVI, SYEDA, YAZ, OCELLA, LORYNA, YASMIN, VESTURA, ZARAH, etc. And this Mayo Clinic article lists even more brand names that contain the same generic drug. Here\u2019s how different searches work, and what the result counts mean. (The diagram may help and is followed by specific examples.)\n\n1. Find exact matches on the drug name (brand or generic) in the original report.\nCriterion: The original report listed at least one drug with the exact name YAZ.\nUse patient.drug.medicinalproduct.exact.\nhttps://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.medicinalproduct.exact:\"YAZ\"\nResults: 19,946.\n2. Find \u201cfuzzy\u201d matches on drug name (brand or generic) in the original report.\nCriterion: The original report listed at least one drug with a name like YAZ, YAZ (24), YAZ 28, etc.\nUse the field patient.drug.medicinalproduct.\nhttps://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.medicinalproduct:\"YAZ\"\nResults: 20,033. As expected, this is larger than the exact match, because it included more possible names.\n3. Find reports for a drug, when you know its brand name\u2014whether the original report used the brand name or generic name.\nCriterion: The report has an openfda annotation with the brand name YAZ.\nUse the field openfda.brand_name.\nThis will match when:\n\nThe original report listed at least one drug with a name like YAZ, YAZ (24), YAZ 28, etc. (in the medicinalproduct field).\nThe original report listed at least one drug with the generic name DROSPIRENONE AND ETHINYL ESTRADIOL (in the medicinalproduct field).\n\nhttps://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.openfda.brand_name:\"YAZ\"\nResults: 22,028. As expected, this is even larger than the previous search, because it included even more possible names.\n4. Find reports for a drug, when you know its generic name\u2014whether the original report used the generic name or any of the known brand names.\nFor drug adverse events, it\u2019s often important to find all the reports for a drug\u2014like IBUPROFEN, no matter what brand name was reported (ADVIL, MOTRIN, etc.). However, it\u2019s not always appropriate\u2014and may not be for a hormone therapy for contraception like DROSPIRENONE AND ETHINYL ESTRADIOL, where the different brand names may represent drug products with different strengths and dosage schedules.\nCriterion: The report has an openfda annotation with the generic name DROSPIRENONE AND ETHINYL ESTRADIOL.\nUse the field openfda.generic_name.\nThis will match when:\n\nThe original report listed at least one drug with a name like YAZ, YAZ (24), YAZ 28, or YASMIN, or OCELLA, or any other brand names that the generic drug is marketed as.\nThe original report listed at least one drug with the generic name DROSPIRENONE AND ETHINYL ESTRADIOL.\n\nhttps://api.fda.gov/drug/event.json?search=receivedate:[20040101+TO+20120930]+AND+patient.drug.openfda.generic_name:\"DROSPIRENONE+AND+ETHINYL+ESTRADIOL\"\nResults: 30,459. As expected, this is the largest result set, since it includes the largest possible set of drug names.\n(The result counts above are valid as of the openFDA \u201clast updated date\u201d of 2014-08-06. In the future, when this date changes, the data may be slightly different.)\n", "data request - Where can I find the source code for Liferay Sync?": "\nHere is a github https://github.com/liferay/liferay-plugins for liferay plugins. May be, it's here.\n", "economics - Open alternatives to the IMF data": "\nThe UN has several sources of datasets relating to government expenditures and trade that are free to use for commercial and non-commercial usage. Generally, the terms require attribution. \nNote, not all UN datasets are free to use. \nUN Data - data.un.org\nTerms of Use - http://data.un.org/Host.aspx?Content=UNdataUse\nAll data and metadata provided on UNdata\u2019s website are available free of charge and may be copied freely, duplicated and further distributed provided that UNdata is cited as the reference.\nWorld Bank - data.worldbank.org/\nTerms of Use - http://data.worldbank.org/summary-terms-of-use\nYou are free to copy, distribute, adapt, display or include the data in other products for commercial and noncommercial purposes at no cost subject to certain limitations summarized below.\nYou must include attribution for the data you use in the manner indicated in the metadata included with the data.\n", "data request - Open database of industrial companies and products, with bill of materials": "\nThe National (US) Renewable Energy Laboratory provide open-source LCI (Life Cycle Inventories) for products, which contain inputs to and outputs from production of products.\nFrom the \"on the project\" page: \n\nThe U.S. Life Cycle Inventory (LCI) Database is a publicly available database that allows users to objectively review and compare analysis results that are based on similar data collection and analysis methods.\n\nYou can get to the site at: This link\nSee an example of an inventory of Ethylene, at plant here\n", "data request - Information about congresspeople": "\nThe data you are looking for is available at openstates.org. This is a project of the Sunlight Foundation. They have an API and a bulk download page (JSON and CSV) at : http://openstates.org/downloads/\nThe about page on their website:\nOpen States is a collection of tools that make it possible for citizens to track what is happening in their state's capitol by aggregating information from all 50 states, Washington, D.C., and Puerto Rico.\nUsing the site is simple: enter a U.S. address or select a state to start to research bills, review voting records, contact elected officials and more. Check out this Sunlight Academy tutorial to see how Open States can help citizens, journalists and activists learn more about their state government.\nOpen States is a project of the Sunlight Foundation. Thank you to the Rita Allen Foundation, Minnesota Historical Society and Open Society Foundations for their generous support.\n", "data request - Retrieve affiliations from research paper": "\nMany of the bibliographic databases offer APIs, but they might not be 100% open.  Typically, you can't get access to the ones that charge for access unless your institution has a subscription to the service.\nI know that link-only answers are bad, but the problem is that policies change over time, and the folks at MIT libraries has a rather long list of bibiometric APIs and info about how open they are, which I assume they'd maintain/update over time:\n\nhttp://libguides.mit.edu/apis\n\nFor your purposes, I'd avoid the subject specific ones (pubmed, arXiv) or publisher specific ones, and look to JSTOR Data for Research or Web of Science (the one run by Thomspon-Reuters). \nAs you're interested in tracking by people, I'd like to recommend ORCID, but it requires your faculty registering with them and acknowledging their papers, so they don't mis-attribute papers because of similar names.\n", "data request - Country specific information for travellers using opendata api/dataset": "\nThe Department of State's API covers security issues relating to travel, relations with countries - but not travel visa/passport requirements.\nhttp://www.state.gov/developer/\nThe also have a humanitarian information unit that publishes maps and datasets about risk areas of the world: such as conflict in Syria and Ebola in West Africa.\nhttps://hiu.state.gov\nThis is the State Department's XML feed on travel warnings:\nhttp://cadatacatalog.state.gov/storage/f/2013-11-24T21%3A00%3A58.223Z/tws.xml\n", "usa - Data set of US Congress in-session or recess": "\nKeeping in mind the caveats raised in comments about what counts as being \"in-session,\" any day that at least one chamber of Congress meets will result in an entry in the Congressional Record, which you can find at the Library of Congress. See, as an example, August when neither chamber typically meets much at all.\nYou should be able to scrape that page to obtain a complete calendar of session dates since 1989.\n", "Looking for historical data for WWII era navy fleet makeups": "\nThere's a surprisingly comprehensive list of ships, naval equipment, and vessels for WWII at the World War II Database.  You can search by type of vessel (destroyer) or by country (Germany).  There is also detailed information about each vessel mentioned (such as the U.S. Casabianca submarine).\n", "data request - List of competitions at Tokyo 2020 Olympics": "\nThe official source for Olympic data is http://odf.olympictech.org/\nThis site does not yet have any references to the Tokyo olympics. \n", "data request - Dataset with informations about donations": "\nYou can find aggregated data on charitable donations from the IRS's Statistics of Income (SOI). Here are some links to look around:\nhttp://www.irs.gov/uac/Tax-Stats-2\nhttp://www.irs.gov/uac/SOI-Tax-Stats-Statistics-of-Income\nIn the UK, the National Council for Voluntary Organisations (NCVO) and the Charities Aid Foundation (CAF), has been conducting and compiling survey results on charitable contributions by individuals. The data goes back to 2004:\nhttp://data.ncvo.org.uk/datastore/datasets/dataset-6-uk-giving-survey/\n", "medical - Where would I find Step or Pedometer Data?": "\nThere are human activity datasets available for research. Some of the better known ones are:\nFordham University, NY\nWISDM dataset (walking, jogging, standing, sitting, etc) - no gender/age\nThe WISDM (Wireless Sensor Data Mining) Lab is concerned with collecting the sensor data from smart phones and other modern mobile devices (e.g., tablet computers, music players, etc.) and mining this sensor data for useful knowledge. Currently our efforts are mainly focused on the acclerometer and GPS sensor data from these devices, but in the future we will mine the audio sensors (microphones), image sensors (cameras), light sensors, proximity sensors, temperature sensors, pressure sensors, direction sensors (compasses) and various other sensors that reside on these devices.\nhttp://www.cis.fordham.edu/wisdm/dataset.php\nUniversity of Michigan\nCollective Activity Dataset \nThis page describe a Collective Activity Dataset. This dataset contains 5 different collective activities : crossing, walking, waiting, talking, and queueing and 44 short video sequences some of which were recorded by consumer hand-held digital camera with varying view point. \nhttp://wwweb.eecs.umich.edu/vision/activity-dataset.html\nHere's something else that is fairly new. It is a FitBit activity dataset and analysis. Though I think it is collected from one individual:\nhttps://rpubs.com/dmaurath/24643\n", "api - Does data stored in Mendeley qualify as open data?": "\nData\nMendeley does not seem to grant me any license to reuse the content (in particular the academic papers uploaded by other users), so by default the data must be considered closed: We are not allowed to redistribute it. Even if an API allows us to retrieve it:\n\nYou may not use our Services to [...] download, use or re-use any Academic Papers without authorization.\n\nSoftware\nMendeley's software is clearly not FOSS, as seen in the Terms:\n\nyou undertake not to copy, rent, lease, sub-license, loan, translate, merge, adapt, vary or modify the whole or any part of our Software\n\nThe question about the data itself is independent of the software itself being open source or not. You can run MediaWiki (open source software) to run a private server and store non-open data on it. On the opposite, you could run IIS (proprietary software) to host open data.\n", "releasing data - How to embed licenses within SVG?": "\nThis is not RDFa, but rather embedded RDF/XML, which is allowed by the SVG Tiny 1.2 Specification.\nThere are the following errors:\n\n<cc:license>\n\ninstead of  \n<cc:License>\n\nin lines 26, 29 and 30, 33. The former is a property, the latter is a class.\n<cc:attributionName rdf:resource=\"Laurent Notarianni and LittleMap.org\" />\n<cc:attributionURL rdf:resource=\"LittleMap.org\" /> \n\ninstead of  \n<cc:attributionName/>Laurent Notarianni and LittleMap.org</cc:attributionName>  \n<cc:attributionURL rdf:resource=\"http://littlemap.org\" />  \n\n\nLiterals are not IRIs.\n\n<cc:license rdf:about=\"http://opendatacommons.org/licenses/odbl/1.0/\">\n   <cc:legalcode rdf:resource=\"http://opendatacommons.org/licenses/odbl/1.0/\">\n      <dcq:hasversion>1.0</dcq:hasversion>\n    </cc:legalcode>\n</cc:license>\n\ninstead of\n<cc:License rdf:about=\"http://opendatacommons.org/licenses/odbl/1.0/\" >\n   <cc:legalcode rdf:resource=\"http://opendatacommons.org/licenses/odbl/1.0/index.html\" />\n   <dcq:isVersionOf rdf:resource=\"http://opendatacommons.org/licenses/odbl/\" />\n</cc:License>\n\nIn short, hasVersion is intended to be used with non-literal values (IRIs and blank nodes).\nNext, if a license has a version, it doesn't mean that this version is applicable.\n\nFinal version:\n<svg xmlns=\"http://www.w3.org/2000/svg\" version=\"1.2\" baseProfile=\"tiny\">\n<metadata xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n          xmlns:dcterms=\"http://purl.org/dc/terms/\" \n          xmlns:dc11=\"http://purl.org/dc/elements/1.1/\"\n          xmlns:cc=\"http://creativecommons.org/ns#\" \n          xmlns:geo=\"http://www.w3.org/2003/01/geo/wgs84_pos#\" >\n  <rdf:RDF>\n    <cc:Work rdf:about=\"\">\n      <dc11:format>image/svg+xml</dc11:format>\n      <dc11:type    rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\" />\n      <cc:license rdf:resource=\"http://creativecommons.org/licenses/by-sa/4.0/\" />\n      <cc:license rdf:resource=\"http://opendatacommons.org/licenses/odbl/1.0/\"  />\n      <cc:attributionName>Laurent Notarianni and LittleMap.org</cc:attributionName>\n      <cc:attributionURL  rdf:resource=\"http://LittleMap.org\" />\n    </cc:Work>\n    <cc:License    rdf:about=\"http://creativecommons.org/licenses/by-sa/4.0/\">\n      <cc:permits  rdf:resource=\"http://creativecommons.org/ns#Reproduction\" />\n      <cc:permits  rdf:resource=\"http://creativecommons.org/ns#Distribution\" />\n      <cc:permits  rdf:resource=\"http://creativecommons.org/ns#DerivativeWorks\" />\n      <cc:requires rdf:resource=\"http://creativecommons.org/ns#Notice\" />\n      <cc:requires rdf:resource=\"http://creativecommons.org/ns#Attribution\" />\n      <cc:requires rdf:resource=\"http://creativecommons.org/ns#ShareAlike\" />\n    </cc:License>\n    <cc:License rdf:about=\"http://opendatacommons.org/licenses/odbl/1.0/\" >\n      <cc:legalcode rdf:resource=\"http://opendatacommons.org/licenses/odbl/1.0/index.html\" />\n      <dcterms:isVersionOf rdf:resource=\"http://opendatacommons.org/licenses/odbl/\" />\n    </cc:License>\n    <geo:Point>\n      <geo:lat rdf:datatype=\"http://www.w3.org/2001/XMLSchema#decimal\">55.701</geo:lat>\n      <geo:long rdf:datatype=\"http://www.w3.org/2001/XMLSchema#decimal\">12.552</geo:long>\n    </geo:Point>\n  </rdf:RDF>\n</metadata>\n</svg>\n\nRDFa 1.1 Distiller will produce the following Turtle:\n<file:///cygdrive/c/users/.../Desktop/map.svg> a cc:Work ;\n    cc:attributionName \"Laurent Notarianni and LittleMap.org\" ;\n    cc:attributionURL <http://LittleMap.org> ;\n    cc:license <http://creativecommons.org/licenses/by-sa/4.0/>,\n               <http://opendatacommons.org/licenses/odbl/1.0/> ;\n    dc11:format \"image/svg+xml\" ;\n    dc11:type <http://purl.org/dc/dcmitype/StillImage> .\n\n<http://creativecommons.org/licenses/by-sa/4.0/> a cc:License ;\n    cc:permits cc:DerivativeWorks,\n        cc:Distribution,\n        cc:Reproduction ;\n    cc:requires cc:Attribution,\n        cc:Notice,\n        cc:ShareAlike .\n\n<http://opendatacommons.org/licenses/odbl/1.0/> a cc:License ;\n    cc:legalcode <http://opendatacommons.org/licenses/odbl/1.0/index.html> ;\n    dcterms:isVersionOf <http://opendatacommons.org/licenses/odbl/> .\n\nMore info: RDF 1.1 XML Syntax\n", "data request - Emu population in Australia from 1900": "\nThere are 9 different types of emu whose populations range from common to extinct.  The Grey Emu is prevalent, while the Kangaroo Island Emu is extinct.  \nIf you want general populations of emu in Australia, try Birdata, which shows populations of Australian birds by location and time, and allows you to download custom datasets.  Here's the link for emus.  Many other sites simply state that the emu is not endangered.\nFor a truly interesting read on the Great Emu War, check out this Scientific American article.\n", "usa - Data Licenses for US Government Data not in data.gov": "\nEach of the datasets on Data.gov describes the license used (see the upper left items on the dataset page). The intent for data provided by the U.S. Government (whether it is on Data.gov or not) is to have an open license, as defined by Project Open Data. The license field in the Data.gov metadata schema is defined as well.\nIn most cases, the license is \"public\", meaning that it is in the public domain and open for free and unrestricted use. If the license is \"License not specified\", you can assume that it is in the public domain. See for example, the Campus Security Data.  Agencies that have a more restrictive license must provide that restriction in the metadata. For example, NOAA's National Mosaic of Weather Data explicitly notes it uses the Creative Commons license and cites a disclaimer.\nThe intent of U.S. Government in publishing data on Data.gov is for individuals and organizations to freely use that data for transparency, civic good, insight, analysis, and economic development.  \nSpecific to your questions:\n\nAs above, there is a general expectation that these data are in the public domain, with any restrictions noted.\nThe classification system for licenses is open and noted at Project Open Data.\n\"Initially, SOI\u2019s ZIP code data were not produced on an annual basis. Therefore, some years are missing. However, beginning with the 2004 data, the SOI Division began producing annual updates to the data and we are committed to continuing this trend into the future.  In similar fashion, SOI began producing machine-readable formats of the data beginning with the 2007 data and has produced these formats for all years thereafter. Given our current resources, there are no plans to produce ZIP code data for the missing years or to create machine-readable versions of the data prior to 2007.  We regret the inconvenience this causes. The Statistics of Income ZIP code data are available on the Tax Stats section of this IRS web page.\" (via Diane Austin, IRS)\nViolations can be reported by following the FEC use rules, specifically here (via Paul Clark at the FEC)\nInformation is anonymized by the publishing agency before being posted to Data.gov, in accordance with federal regulations for government officials to protect personally identifiable information (see a summary here).\n\nData.gov's own data policy notes: \"License: U.S. Federal data available through Data.gov is offered free and without restriction. Data and content created by government employees within the scope of their employment are not subject to domestic copyright protection under 17 U.S.C. \u00a7 105. Non-federal (city, county, and state) data available through Data.gov may have a different licensing method as noted under \u201cShow more\u201d at the bottom of the dataset page. Non-federal data can be identified by name of the publisher and the diagonal banner that shows up on the search results and data set pages. Federal data will have a banner noting \u201cFederal\u201d and non-federal banners will note \u201cUniversity\u201d, \u201cMultiple Sources\u201d, \u201cState\u201d, etc.\"\n(Disclaimer: I have worked on the Data.gov project.)\n", "Healthcare.gov's API Finder app returns \"File not found\"": "\nOn the command line, give a try to the following successful request (or see/edit it in runscope):\n$ curl -XPOST https://api.finder.healthcare.gov/v3.0/getIFPPlanQuotes -d '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<p:PlanQuoteRequest xmlns:p=\"http://hios.cms.org/api\" xmlns:p1=\"http://hios.cms.org/api-types\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://hios.cms.org/api hios-api-11.0.xsd \">\n  <p:Enrollees>\n    <p1:DateOfBirth>1959-03-03</p1:DateOfBirth>\n    <p1:Gender>Male</p1:Gender>\n    <p1:TobaccoLastUsedMonths>2</p1:TobaccoLastUsedMonths>\n    <p1:Relation>SELF</p1:Relation>\n    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>\n  </p:Enrollees>\n  <p:Enrollees>\n    <p1:DateOfBirth>1982-03-03</p1:DateOfBirth>\n    <p1:Gender>Female</p1:Gender>\n    <p1:TobaccoLastUsedMonths>2</p1:TobaccoLastUsedMonths>\n    <p1:Relation>SPOUSE</p1:Relation>\n    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>\n  </p:Enrollees>\n  <p:Location>\n    <p1:ZipCode>22901</p1:ZipCode>\n    <p1:County>\n      <p1:FipsCode>51003</p1:FipsCode>\n      <p1:CountyName>ALBEMARLE</p1:CountyName>\n      <p1:StateCode>VA</p1:StateCode>\n    </p1:County>\n  </p:Location>\n  <p:InsuranceEffectiveDate>2014-10-01</p:InsuranceEffectiveDate>\n  <p:Market>Individual</p:Market>\n  <p:IsFilterAnalysisRequiredIndicator>false</p:IsFilterAnalysisRequiredIndicator>\n  <p:PaginationInformation>\n    <p1:PageNumber>1</p1:PageNumber>\n    <p1:PageSize>20</p1:PageSize>\n  </p:PaginationInformation>\n  <p:SortOrder>\n    <p1:SortField>BASE RATE</p1:SortField>\n    <p1:SortDirection>ASC</p1:SortDirection>\n  </p:SortOrder>\n\n</p:PlanQuoteRequest>'\n\nFWIW, I was able to reproduce the 503 \"File not found\" by POSTing the same request but with trailing characters after the url like https://api.finder.healthcare.gov/v3.0/getIFPPlanQuotesXYZ\n", "licensing - No-attribution open license, applicable to documentation": "\nCreative Commons ShareAlike 1.0 Generic (CC SA 1.0) should match all of your requirements. However, CC SA 1.0 has been retired by Creative Commons in 2004 due to inadequate demand, and they recommend using one of their current licenses instead.\n", "Where I can find data to help build family trees?": "\nThere are a lot of paid services that provide this type of information, such as Ancestry.com.  They are unlikely to offer access to their highly curated content.\nHowever, the Church of Latter Day Saints at FamilySearch does provide a free service for others to use.  That site provides developer resources specifically for people like you, and encourages others to build apps using APIs and accessing their data.  \nData formats are described in more detail here.\nGood luck!\n", "Music lyrics timing data": "\n(Not a very practical answer, but maybe can get the ball rolling...)\nIf you can create a list Youtube links to the songs or albums you want, you can download the captions (subtitles). These have a timestamp along with the lyrics. \nWith YouTube you can search for videos with captions. Unfortunately, I found that many are listed with captions but actually don't have any meaningful text.\n\nHere is an example of one video with good captions (link).\n\nYou can see the transcript by going to \"More\" and then \"Transcript\".\n\n\nOne tool to do download captions is called Google2SRT.\n\nWhat is Google2SRT? Google2SRT is a tool that can download \"not embedded\" subtitles (Closed Captions - CC) from YouTube and former Google Video and convert them to a standard format (SubRip - SRT) supported by most video players. \n\nThere are some online tools, too, such as keepsubs.com and yousub.net (although questionable how reliable).\nAlso, a Firefox add-on.\n\nYou could also use OpenSubtitles.org to download subtitles soundtracks or concerts.\n", "economics - Looking for datasets with sigmoidal relationships": "\nIn technology forecasting, the market penetration of a new technology often follows a sigmoidal curve. Hence, with one variable being the time, the other variable being the market share of\n\nebooks\nLED\norganic food\n\ncould form examples. There is a classic paper by Fisher and Pry on this, however behind a paywall.\n", "data.gov - Is there a way to get a drug's experimental name on open FDA?": "\nMy only guess is that you could find the previous version of that setid (it looks like that it is currently on version 30), and perhaps the name you are look for would be in there. The OpenFDA API is only ever going to have the latest version. \nThe DailyMed site has an API for pulling SPL history, so you know the date that it changed its name, then you can determine which version to track down. \nHope that helps.\nThe API call would be: http://dailymed.nlm.nih.gov/dailymed/services/v2/spls/8bc6397e-4bd8-4d37-a007-a327e4da34d9/history.json\nThe response looks like: \n```\n{\n    \"metadata\": {\n    ...\n    },\n    \"data\": {\n        \"history\": [{\n            \"spl_version\": 31,\n            \"published_date\": \"Mar 05, 2015\"\n        }, {\n            \"spl_version\": 30,\n            \"published_date\": \"Aug 12, 2013\"\n        }, {\n            \"spl_version\": 29,\n            \"published_date\": \"Mar 13, 2013\"\n        }, \n          ...\n           {\n            \"spl_version\": 12,\n            \"published_date\": \"Sep 18, 2009\"\n        }, {\n            \"spl_version\": 11,\n            \"published_date\": \"Jul 23, 2009\"\n        }, {\n            \"spl_version\": 1,\n            \"published_date\": \"Nov 14, 2008\"\n        }],\n        \"spl\": {\n            \"setid\": \"8bc6397e-4bd8-4d37-a007-a327e4da34d9\",\n            \"title\": \"ERBITUX (CETUXIMAB) SOLUTION [IMCLONE LLC]\"\n        }\n    }\n}\n\n```\n", "releasing data - Interest of double licenses CC-BY-SA + ODbL for SVG maps": "\nLet's give this a try then.\n1. To allow people inserting our data into OSM, do we require to use ODbL?\nNot necessarily. This is what OSM has to say on the topic:\n\nWe are only interested in 'free' data. We must be able to release the data with our OpenStreetMap License. Obviously we are allowed to use public domain data sources, of which there are quite a few, but beyond that, it gets more complicated.\n\nSo as long as your license is either very liberal (e.g. CC0) or compatible with the ODbL, it should be fine. Which brings us to your next question.\n2. Is CC-BY-SA enough for OSM compatibility?\nAccording to this discussion on the OSM wiki, I'm going to say no. The main problem seems to be the attribution requirement, although I don't quite understand why. However, when you license your data under both CC-BY-SA as well as ODbL, there should be no problem.\n3. Else, what could be the advantage of CC-BY-SA in our case?\nAs you pointed out yourself, Creative Commons licenses are currently the best-known open licenses. A lot of time has gone into refining them over time, and since version 4.0 they are perfectly suited to cover data and not just content.\n4. Does CC-BY-SA prevent OSM data to be inserted within our maps?\nThis is where things start to get interesting.\nOSM state on their copyright page:\n\nIf you alter or build upon our data, you may distribute the result only under the same licence.\n\nHowever, the ODbL states:\n\n4.4 Share alike.\na. Any Derivative Database that You Publicly Use must be only under the terms of:\ni. This License;\nii. A later version of this License similar in spirit to this License; or\niii. A compatible license.\n\nSo as long as CC-BY-SA 4.0 is deemed a \"compatible license\", all is well.\n5. Is it correct to license XML file (SVG) under a Database license?\nWell, I guess it depends. If your SVG is a creative work (such as a painting or drawing), then no. If it is just simple geometric shapes, then yes. \nWhat about SVG file generated from OSM data?\nIf they are generated automatically, there's probably no creative process involved, so it should be fine to assume that a database license always fits.\nThis last question is actually a great example of one of the big advantages of CC-BY-SA 4.0: You simply don't have to care if your content is a database or a creative work, because the license covers both.\n\nDisclaimer: I am not a lawyer, this is just my own personal assessment. I'm happy about any input and feedback :)\n", "business - Transactional data over multiple years (Customer ID, Date, Price)": "\nKaggle once conducted a competition with 22GB of real transaction data:\n\nhttp://www.kaggle.com/c/acquire-valued-shoppers-challenge/data (registration required)\n\nBut look after the terms of use (only for the purposes of competition) and so on.\n", "data request - Looking for datasets of tumor or cancer growth": "\nI asked on Twitter and got this response: https://twitter.com/sachsmc/status/532600589033951234\nLooks like there is some data at http://dtp.nci.nih.gov/index.html, but he said it's hard to navigate\n", "data request - FIFA 2014 tweets dataset for academic project": "\nGetting access to historical Twitter data may end up coming with a price.  Here are some options:\n\nI've had great luck using Topsy in looking at a wide variety of tweets ranging from disease vectors in Africa to sentiment analysis. Here's the link for 86K #FIFA tweets for the last 30 days.  You can expand to \"all time\", search by language, and look at influencers.\nUse the Twitter API to get the data you can for free.  Good developer resources are available.\nThe most comprehensive historical archive may be via Gnip, but unfortunately it is not free and it's unclear what the actual costs are.\n\nGood luck!\n", "data.gov - Missing medical device recall information? OpenFDA not finding recalls from FDA database": "\nCouple things possibly at play but this is not a complete answer. I noticed that there are reports form 2003 on the FDA site you are referencing and ones from October 2014. Both sets (and only a handful) are not found in the API.\nA litte deeper digging I queried Monoject and could not find any of the recalls associated with them in the API return - http://www.researchae.com/recalls?reporttype=device&from_date=2000-01-01&to_date=2014-11-30&search=Monoject\nNot sure the reason for this but did notice some missing ones as well.\n", "data request - Where to download decimals of Pi?": "\nUPDATE: Unfortunately this website is not available anymore, see philshem's comments below for a potential? workaround.\n\nThis Japanese site has downloadable chunks up to 13x10^12:\nWeb site: http://piworld.calico.jp/estart.html\nThere are 130000 files (pi-0001.txt-pi-130000.txt), each chunk is a ZIP file about 55MB (so it's around 7.15TB in total, 15.34TB uncompressed).\nNo registration, agreement or anything needed.\n", "data request - London rail and tube station locations": "\nAlex, I looked through the TFL APIs, documentation and datasets. The station list (KML) format indicates it has locations for light rail (DLR), tube and overground. But as you observed there are no entries for overground stations. I also did not find anything on National Rail in the documentation.\nI did find some additional resources. Below is the National Rail's list of stations in London (which you would need to scrape):\nhttp://www.nationalrail.co.uk/css/OfficialNationalRailmaplarge.pdf\nThe Guardian maintains its own downloadable dataset of all rail stations in Great Britian. .\nhttp://www.theguardian.com/news/datablog/2011/may/19/train-stations-listed-rail#data\nhttps://docs.google.com/spreadsheet/ccc?key=0AonYZs4MzlZbcktheEZFeF84U1J4dFFvckI5X0VBcEE#gid=7\nIt appears the Guardian reversed engineered the dataset from the Office of Rail Regulation' station usage statistics.\nhttp://orr.gov.uk/statistics/published-stats/station-usage-estimates\nThe UK data.gov has a dataset labeled: National Public Transport Access nodes (NAPTAN). \n\nNaPTAN is a GB national system for uniquely identifying all the points\n  of access to public transport in GB. It is a core component of the GB\n  national transport information infrastructure and is used by a number\n  of other UK standards and information systems. Every GB station, coach\n  terminus, airport, ferry terminal, bus stop, etc., is allocated at\n  least one identifier.\n\nhttp://data.gov.uk/dataset/naptan\n", "data request - Adverse Events for Avastin - vision issues": "\nThis API call should help you get started with all adverse event (AE) reports involving Avastin:\nhttps://api.fda.gov/drug/event.json?search=brand_name:avastin&limit=100\n\nbut be sure to read through the comprehensive docs at https://open.fda.gov/drug/event/reference/ to see if you want to be searching for a subset of this data set which is where Avastin was the suspect drug. I am not 100% sure that would be possible through the API since you want to limit your search to a nested object.\nTo search for only AE reports where a certain AE related to vision was reported, you'll want to look through the list of all AEs from the above API call (https://api.fda.gov/drug/event.json?search=brand_name:avastin&count=patient.reaction.reactionmeddrapt.exact) and then filter based on those.\nYou can also use a more user-friendly tool that Brian Norris and I built at http://researchae.com. Here's a hyperlink to only Avastin AE reports.\nHope this helps and good luck!\n", "usa - Source of ABA Numbers for US banks": "\nThe source for this data is going to be accuity, that takes care of the routing numbers for the ABA, \nhttp://www.aba.com/Products/Pages/PS98_Routing.aspx\nTo fast forward, it looks like you can download a version from the federal reserve board here:\nhttp://www.fededirectory.frb.org/download.cfm\nbut it states:\n\nI understand that the terms of use prohibit selling, relicensing, or otherwise using information in the directory for commercial gain\n\nYou can buy an official copy from accuity for $495, \"ABA Key to Routing Numbers\"\nhttp://store.accuitysolutions.com/order.html\nUp until last year, the ABA was claiming copyright to this list of 9 digit numbers, so I don't know if there is a legally open version of the dataset around.\nhttp://www.gregthatcher.com/Financial/LawyerProblems.aspx\n", "data request - Database of inaccuracies in mainstream media": "\nIt's just a subset of \"mainstream media\", but you can use the NYTimes API to search for \"Correction: \".\nThey also maintain a page with recent corrections, which point to the amended article.\nTheir format looks like this, so it should be easy to find with the API.\n\n\nYou'll probably have to create the \"database\". This NYTimes data would plug into the data model and some fields you'd have to generate (i.e. tags)\n", "licensing - How do I license a work with CC license version X \"or later\"?": "\nCC BY-SA 4.0 states that \"The Adapter\u2019s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\" Similar clauses can be found since CC 2.0.\n", "HUD Section 8 100% Median Income Data": "\nAs Andrew has suggested, you could just use ACS data here. Make sure to download as a .csv to get the FIPS State-County codes.\n", "usa - Understanding the population sample represented in CDC WONDER database": "\nI would go with the population denominator of 878,964. According to the Census Bureau, the 20 to 24 year old 2011 population in Illinois was 881,738.\nAs to your second issue, just remember that it only makes sense to combine them when you are looking for rate of cases among the population. If you want to reference it as the rate of chlamydia among the population, you would have to keep it separate from the calculation of gonorrhea because some people might have both. The best practice would be to only reference them as separate rates. Ignore whatever CDC wonder is doing with the sum total at the bottom. It is probably a programming oversight.\nLet us know if you're looking for any more information.\n", "data request - Dataset of historical economic forecasts": "\nThe World Bank provides an excellent overview and summary, as well as detailed current and historical data. \nAdditionally:\n\nYou can get information from individual government open data portals (scroll to bottom), but it's inconsistently formatted and may not be available from all governments.\nOxford Economics provides country economic forecasts.  There's summary information here, and an archive of free open data here. (Note that there is a fee to get access to all their data, although you can get a free trial.)\nThe International Monetary Fund has a summary of GDP and detailed data.\n\n", "linked data - \"DBpedia as Tables\" not having all the properties": "\nAccording to DBpedia as Tables, DBpedia uses the following data to create the tables:\n\nEach instance is described by its URI, an English label and a short abstract, the mapping-based infobox data describing the instance (extracted from the English edition of Wikipedia), geo-coordinates, and external links.\n\nDBpedia basically provides two different types of data: the Infobox Dataset (everything beginning with dbpprop) and the Mapping-based Dataset (everything beginning with dbpedia-owl). If you're interested, you can read more about the difference.\nFor these tables, only the mapping-based data (dbpedia-owl:\u2026) is used. That's why you won't find any raw infobox properties starting with dbprop \u2014 however, you can download the raw data if you're really interested.\nQuick note from DBpedia about the raw infobox properties though:\n\nInformation that has been extracted from Wikipedia infoboxes. Note that this data is in the less clean /property/ namespace. The Mapping-based Properties (/ontology/ namespace) should always be preferred over this data.\n\n", "usa - Looking for US County elevation data": "\nHave you tried the Area Health Resource File (AHRF)? It has elevation data and then some for this type of analysis.\n", "usa - License for data product of music lyrics?": "\nThis from the U.S. Copyright Office might be a better reference on U.S. copyright law in relation to musical lyrics.  They provide an excellent plain-language description of the copyright rules for those wanting to register a copyright.\nThere is also a description for use that may allow what you are trying to do: \"Under the fair use doctrine of the U.S. copyright statute, it is permissible to use limited portions of a work including quotes, for purposes such as commentary, criticism, news reporting, and scholarly reports. There are no legal rules permitting the use of a specific number of words, a certain number of musical notes, or percentage of a work. Whether a particular use qualifies as fair use depends on all the circumstances. See FL 102, Fair Use, and Circular 21, Reproductions of Copyrighted Works by Educators and Librarians.\"\n", "data request - List of all passenger airlines with regular flights to anywhere in New Zealand": "\nThere are many commercial sites that will provide this info, but for a CC license I'd use the data from the NZ government site: transport.govt.nz.\n\nThe license is CC 3.0 BY NZ (details).\n", "data request - Pictures of taxis in Karachi": "\nWith Google you can search for images and choose the general license terms. For specific companies you'd have to do a specific search.\nHere is a search with \"Reuse with modification\"\n\nHere is one example of results, from Wikipedia.\n\n", "data request - I'm looking for an Oracle Express Edition 11g testing database": "\nBased on this stackexchange question, I found the page:\nCreating the Sample Database in Oracle 11g Release 2 (Other instructions)\n\nAlso, if you are using XE, there is a demo DB ready to play with.\n", "releasing data - What are some OpenData torrents to seed?": "\nCommunity wiki to collect data sets available to download/seed on bit torrent\n\n\nDNS Census, the DNS registration dataset snapshot taken in 2013 (compressed ~15GB and uncompressed 157GB).\n\n\nThe DNS Census 2013 is an attempt to provide a public dataset of registered domains and DNS records. It was inspired by the Internet Census 2012 which showed that releasing data anonymously via BitTorrent is a good thing to do. The dataset contains about 2.5 billion DNS records gathered in the years 2012-2013.\nAll data is compressed using xz/LZMA2.\nDNS records are written into CSV files. There is one file for each DNS record type (A/AAAA/CNAME/DNAME/MX/NS/SOA/TXT). The records are sorted lexicographically by hostname and by time.\n\n\n\nNYC taxi trip data - 2013 Trip Data (11.0GB) and 2013 Fare Data (7.7GB)\n\n\nFare data looks like this, showing medallion, hack_license, vendor_id, pickup date/time, payment type, fare, tip amount (look at all those zeros!), tolls, and total.\nTrip data (the good stuff!) looks like this.  Each file has about 14 million rows, and each row contains medallion, hack license, vendor id, rate code, store and forward flag, pickup date/time dropoff date/time, passenger count, trip time in seconds, trip distance, and latitude/longitude coordinates for the pickup and dropoff locations. \n\n\n\nNatural Earth data BitTorrent mirror (description,  4.81 GB)\n\n\nNatural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110m scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software.\n\n\n\nUK Government expenditure (multiple files, more information)\n\n\nThe Combined Online Information System (COINS) is the database for UK Government expenditure. The data is used to produce expenditure data in the Budget report; Supply Estimates; Public Expenditure Statistical Analyses (PESA); Whole of Government Accounts (WGA); the monthly Public Sector Finance Releases. It is also used by the ONS for other National Statistics releases. \n\n\n\nAcademic torrents (multiple files, browse datasets or collections)\n\n\nThis service is designed to facilitate storage of all the data used in research, including datasets as well as publications. There are many advantages of using bittorrent technology to disseminate this work. \n\n\n\n911datasets - (3,254 GB) (3 TB) in 256,673 files (as of 31 Dec 2014)\n\n\nCrowdsourcing 9/11 information distribution.\nThe idea behind 911datasets.org is to make raw information about 9/11 available to a broad audience, and to provide a way to ask useful questions about the data. \nMost of the material was obtained using the Freedom Of Information Act (FOIA) process. \n\nLink to all torrents\n\n+ Wikileaks data storage - wlstorage.net\nYou can either download everything, or select files by \"project\", as folders or single files: torrents (1827 in total at time of posting).\nMore details here\n\n\n\n2012 Internet Census (568 GB torrent)\n\n\nWhile playing around with the Nmap Scripting Engine (NSE) we discovered an amazing number of open embedded devices on the Internet. Many of them are based on Linux and allow login to standard BusyBox with empty or default credentials. We used these devices to build a distributed port scanner to scan all IPv4 addresses. These scans include service probes for the most common ports, ICMP ping, reverse DNS and SYN scans. We analyzed some of the data to get an estimation of the IP address usage.\n\nFull data download \nHilbert Browser tool\nImage gallery\n(Taken from this answer)\n\n\nSci-Hub Bulk Access\n\n\nSci-Hub is a paywall-bypassing website that uses \"shared\" user credentials to provide PDF or HTML scientific papers. The website itself doesn't store any papers. Answer from this open data stack exchange question.\n\nCollection of more than 1 million books.\nCollection of more than 50 million scientific papers.\n\nAll data gathered during our research is released into the public domain for further study. \n\n\n\nGeocities archive (641 GB)\n\n\nThis is a collection of Geocities data downloaded by a bunch of people who\n    call themselves ARCHIVE TEAM, who began scraping the Yahoo! Geocities site\n    during a six month period in 2009, before Yahoo! shut down geocities.com \n    on October 26th, 2009. This collection is compressed in a UNIX filesystem\n    with both 7zip archives and tape archives (gtar).\n\n\n\nWikimedia data dump (over 23 TB total, but smaller torrents available)\n\n\nThis is an unofficial listing of Wikimedia data dump torrents, dumps of Wikimedia site content distributed using BitTorrent...\nThis includes both dumps already being distributed at dumps.wikimedia.org and dumps created and distributed solely by others.\nBitTorrent is not officially used to distribute Wikimedia dumps; this article lists user-created torrents. Please protect your computer and verify the md5sum for any file downloaded from these unofficial mirrors.\n\nIn particular, Wikidata\n\nThe GHTorrent project \n\nGHTorrent monitors the Github public event time line. For each event, it retrieves its contents and their dependencies, exhaustively. It then stores the raw JSON responses to a MongoDB database, while also extracting their structure in a MySQL database.\nCurrently (Jan 2015), MongoDB stores around 4TB of JSON data (compressed), while MySQL more than 1.5 billion rows of extracted metadata. A large part of the activity of 2012, 2013, 2014 and 2015 has been retrieved, while we are also going backwards to retrieve the full recorded history of important projects.\n\nSince 2015, the dumps are daily as mysql format\nDownloads: http://ghtorrent.org/downloads.html\n\n", "data request - Cost of living dataset": "\nI found an API from Numbeo (documentation) that claims\n\n8,282,934 prices in 11,072 cities entered by 706,591 users\n(information updated 2022-10-18, source)\n\nHere is their page related to the Cost of Living.\nAnd an overview of the data for Zurich.\nTheir Terms of Service state that the license is CC BY-SA 3.0 and GNU Free Document License (GFDL).\nWhat I can't find is how to get an API key (or quotas), although I think you can get one once you register as a user.\n.\n", "economics - Historical monthly farm/agricultural data 1950 to the present": "\nThe \"FOOD AND AGRICULTURE ORGANIZATION OF THE UNITED NATIONS - Statistics Division\" (link) provides historical data for commodity prices and agricultural production.\nYou can download for a single country or for all countries. Some data sets go back to at least the 1960s. You'll have to investigate which data sets may be best suited to your purpose.\nData formats are typically zipped CSV.\n\n", "licensing - What are the legal uses of the data from livingwage.mit.edu?": "\nThis site helps to calculate the cost of living in different parts of the United States.  There's a caveat about the fidelity and comprehensiveness of the data (\"Consider the results a minimum cost threshold that serves as a benchmark, but only that.\").\nFor licensing, the site references that it is part of the Living Wage Project. However, in looking through all those sites, there are no references to data use and licensing.\nTo verify the licensing, you can contact the author of the page or the project team. (I'm in contact now with the author to clarify the data use rules.)\n", "Data.gov: Is the API Version 3 down completely?": "\nRebecca, from the Data.gov team here. Following up on this, to our knowledge the Data.gov CKAN API hasn't been down. Can you provide more details on the issues you were having? \nIf helpful in constructing your query parameters, here is documentation on the CKAN API: http://docs.ckan.org/en/latest/api/index.html#making-an-api-request\nAlso, we are working to provide additional documentation specific to the Data.gov CKAN API, you can follow that progress or weigh in here: https://github.com/GSA/data.gov/issues/180\nI hope that helps! \n", "best practice - Is it okay to download all datasets from a government open data portal?": "\nBad manners have nothing to do with it. As a taxpayer, it's your data, and they're making it available for you to download. Your country also benefits from having backups of their dataset floating around out there. \nYou could take the public service angle of it to the next level by uploading a copy of the full archive to the Internet Archive using their S3-compatible interface: https://archive.org/help/abouts3.txt\nIf you think their system is creaky and might strain at giving you the full dataset, or you think there's a more efficient way for them to give you a full dump -- then you can always email or call and ask them. \nBut do it after you've got your first complete copy downloaded, just in case.\n", "data request - Datasets of detailed statistics from MMORPGs": "\nGreat question! There are several MMORPG games that provide this type of data, and APIs to others from which you can extract the data.  Check out the following:\n\nWorld of Warcraft APIs on Github with everything from pets to professions extending over players, guilds, and regions\nLeague of Legends API (includes game data and assets, champions, items, runes, masteries, summoner spells, and profile icons)\nEve Online data access explained (market and universe data at a regional level)\nGuildWars 2 API covering recipes, items, and commerce\nAnd another 179 game APIs via the Programmable Web\n\nSeveral of these require registering but seem to be free. Some, like Eve Online, restrict secondary commercial use, but encourage developers, researchers, and others to use the data for co-development or non-commercial use.\n", "data request - The media word frequency (n-grams) dataset": "\nIf the dataset you want isn't already available, you can create it with some basic programming. Creating an N-gram tool is pretty straightforward, and the volumes of data are not so large if you limit yourself to one topic or a handful of media sources.\n\nCollecting\nFor this type of project, you are looking to cast a wide net and collect as much data as possible, in order to get a strong signal. Errors in data collection will be lost in the noise (not enough frequency to stand out).\nFor the data collection, one idea is to use RSS feeds, which come usually in XML format and are easy to parse. For example, for Yahoo News - Politics. You can also use the URL in each short RSS item and then scrape the page.\nAnother option is the Google News RSS feed or Bing News Developer (details). Here is the URL to an XML output from Google News RSS feed for a particular query.\nhttps://news.google.com/news/feeds?q=apple&output=rss\n\nI like python and I can suggest HTML scraping tools such as lxml, beautifulsoup, scrapy, etc.\nYou might also look for some news aggregator tools that provide news articles from various sources in a simple text format. \n\nParsing\nAfter download as much text as possible, it's time to parse. Python's zip module is ready to go (my source):\ninput_list = ['all', 'this', 'happened', 'more', 'or', 'less']\ndef find_ngrams(input_list, n):\n  return zip(*[input_list[i:] for i in range(n)])\n\nBy running with n=2, the response is this:\n[('all', 'this'), ('this', 'happened'), ('happened', 'more'), ('more', 'or'), ('or', 'less')]\n\nIn reality, you need to create input_list by splitting on all punctuation and white space. For example,\nimport re\ntext = 'all. this, happened more : or ; less'\nprint re.split('\\W+', text)\n\nwould return an array of words from a large piece of text, without punctuation (warning - the '_' character isn't considered punctuation)\nThere are also some pre-built N-gram tools from NLTK (one, two)\n\nStorage\nIf the parsing takes some time because of large raw data volumes, I'd write to an the raw N-grams to an intermediate file (probably just CSV).  Perhaps you also want to store the source relating the N-gram to the source document (something Google provide), as well as the date (or just year).\nThen with some processing you can convert arrays and similar files (like CSV) to a format similar to Google N-grams. To do so, one option is to use the defaultdict library from python's Collections module. To do so, a form of each N-gram would be the key to the dictionary and the integer count would be the value.\nfrom collections import defaultdict\ndata = defaultdict(int)\nfor item in find_ngrams(input_list, 2): # using the function from above\n    data[item] += 1\nprint data\n\nwould return a count for each N-gram (not so exciting in this case).\n {('or', 'less'): 1, ('all', 'this'): 1, ('more', 'or'): 1, ('happened', 'more'): 1, ('this', 'happened'): 1})\n\n\nFurther\n\nConsider tagging your words with a part of speech (POS) with tools like NLTK.\n\n", "tool request - Sentiment search engine": "\nThe first source of raw interactions that fit your needs that comes to mind is Twitter.\nNCSU's Tweet Sentiment Visualization\nhttp://www.csc.ncsu.edu/faculty/healey/tweet_viz/tweet_app/ seems pretty good in that it allows you to enter keywords and get a graph of recent tweets graphed on axes (pleasant/unpleasant and active/inactive). It also has some pretty extensive documentation about how to interpret and how the system arrives at the results. Clicking on a data item shows the raw tweet:\n\nLists of tools\n\nhttp://matei.org/ithink/2012/02/08/a-list-of-twitter-sentiment-analysis-tools/ has a list of other Twitter Sentiment Analysis Tools\nhttp://www.khwrites.com/online-sentiment-analysis-social-media-monitoring-tools/ has a list of free Online Sentiment Analysis: Social Media Monitoring Tools\n\n\nAnother thought would be to ingest tweets on your own and use one of many Sentiment APIs to arrive at your own results.\n", "data request - Educational attainment in the private and public sector": "\nthis is easy if you use the public-use microdata.  \nthe public/private-sector workforce variable in the current population survey (cps) is a_clswkr and the educational attainment variable is a_hga\nopen up cps or acs, subset by public vs private-firm worker, and calculate educational attainment rates.\ncps and acs are both representative at the state-level.  acs also has pumas, which are sub-state areas.\nfor international comparisons, look at the codebook for piaac\ngood luck!\n", "data request - Phones dataset for speech recognition (not telephone number)": "\n\nYou can find US and British english language phones at university linguistic departments such as Berkeley, Ca. Though, what is useful for speech recognition are the features in statistical acoustic models that are extracted from a large amount of annotated audio recordings. These acoustic model features involve hidden markov models and Mel-Frequency Cepstral coefficients. (MFCC tutorial by James Lyons below)  Interpreting and understanding these feature files is complex.\nThe CMU Sphinx speech recognition project provides different acoustic models such as AN4 and RM1.  The distribution of some CMU audio databases such as RM1 are restricted. The AN4 database includes the audio recordings and utilizes 34 phones.  \n\nBerkeley Linguistics Phonetics\n(click on phone, then left click for audio, right click for spectrogram)\nhttps://corpus.linguistics.berkeley.edu/acip/ \nCMU Sphinx acoustic models\nhttp://www.speech.cs.cmu.edu/databases/an4/index.html \nCMU Sphinx dictionary\nhttps://github.com/cmusphinx/cmudict\ncmudict.dict\n  yes Y EH1 S\n  no N OW1\ncmudict.phones  (sample of 39 phones vs 34 phones for AN4)\n  Y       semivowel\n  EH      vowel\n  S       fricative\n  N       nasal\n  OW      vowel\ncmudict.symbols\n  Y\n  EH1\n  OW1  \n\nVoxforge provides open source corpus, audio recordings and acoustic models for multiple speech recognition engines in multiple languages.  CMU Sphinx, ISIP, Julius, and HTK\nhttp://www.voxforge.org/home\nhttps://github.com/julius-speech/julius\nhttps://www.isip.piconepress.com/projects/speech/\nhttp://htk.eng.cam.ac.uk/\nJames Lyons provides an excellent Mel Frequency Cepstral Coefficient (MFCC) tutorial along with python code.\nhttp://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\nhttps://github.com/jameslyons/python_speech_features\nAdam Coates of Baidu Research and Stanford gave an excellent presentation about improving the acoustic model.\nhttp://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf\nhttps://github.com/baidu-research/ba-dls-deepspeech\nhttp://www.openslr.org/12/ (corpus, audio recordings, acoustic models)\n\n", "data request - Return on investment in international development projects": "\ngreat question. \nThe first thing to note is that the IEG database that you're referencing is only one of several that the World Bank provides. You might want to also check out the Bank's projects portal: http://www.worldbank.org/projects. In particular, check out specific country pages for a lot of good details, as well as many project documents (in pdf) for most projects. \nIn addition, you might want to check out this platform that tracks many types of development financing (aid, private foundations, remittance flows, etc.): aiddata.org\nMore and more organizations in international development are also providing data on their own projects. A few examples:\n\nDfID (UK): http://devtracker.dfid.gov.uk/\nAfDB's project portfolio\nAsDB: adb[dot]org[forwardslash]projects\n\nMany countries are now also publishing their own aid information. Just a handful here:\n\nHaiti's MGAE\nNepal aid management system\nTimor aid management system\nBangladesh aid information system\nAfghanistan DAD system\n\nMore and more organizations and countries are reporting actual deliverables from these investments, but that information is more scattered and difficult to synthesize. RoI itself is hard to measure, as there's no clear indication of how you might track that.\n", "finance - Stock market historical data": "\nThere are many good resources described in the comments to your question (1 & 2). One that combines many different types of financial products and has some open access is Quandl.\n\nData browser tool\n\nAPI documentation\n\n\n\nQuandl provides a single easy-to-use API for stock prices and fundamentals. Coverage includes end-of-day prices, harmonized fundamentals, key financial ratios, earnings estimates, analyst ratings, price targets, indexes and more.\n\nThe API comes with 3 levels (1 open/free, 2 paid). The open level is based on community maintained data (details).\n\nEnd of day stock prices, dividends and splits for 3,000 US companies, curated by the Quandl community and released into the public domain.\nHistory to 2004.\n\n\nIn addition to Quandl, I've also had a good experience using the Markitondemand API (documentation). Their interactivechart endpoint can give you historical data.\n\nIf you use programming languages like Python or R, you can automatically integrate historical data via the public feeds (i.e yahoo, google, quandl).\nPython Quandl module\nimport Quandl\nmydata = Quandl.get(\"NSE/OIL\", authtoken=\"your token here\")\n\nR Quandl example\nlibrary(tseries) # Loading tseries library\nmtgoxusd <- read.csv('http://www.quandl.com/api/v1/datasets/BITCOIN/MTGOXUSD.csv?&trim_start=2010-07-17&trim_end=2013-07-08&sort_order=desc', colClasses=c('Date'='Date'))\n\n", "rdf - Is there any work, api, publication, or example of providing a SPARQL query engine ontop of an OSLC web service? - Stack Overflow": "\nThere has been some work by some, such as IBM, to build a SPARQL index by leveraging the Tracked Resource Specification (TRS) to keep the index current.  Though, it could be possible to leverage a similar model and using OSLC's query capability to build such an index and keep it current.  As you say, it depends a lot on what tool(s) you are trying to connect to and what support they expose.  At the simplest level, you could just treat like Linked Data and crawl the links and leveraging HTTP caching controls to minimize requests.\n", "data.gov - Plans and documentation to tidy RAW data in Maude?": "\nI haven't seen any \"plans\" per se from FDA but have noticed that they have split up the MAUDE database into separate components. There is a MASTERLIST but the PATIENT_NARRATIVE has been split off into another set of files.\nI have been poring over MAUDE in efforts to extract \"significant unanticipated adverse effects\". Such events trigger strict reporting and distribution timelines. However, when a clinical trial is conducted under an Investigational Device Exemption (IDE) (as many are), the FDA doesn't seem as interested in ensuring regulatory compliance. \nPerhaps the FDA is not interested because such delays can trigger statutory fines for each additional day of violation. \n", "data request - Database of food sharing places for the whole world": "\nI suspect this type of dataset will have to be constructed from many local sources. Particularly since the 'meaning and access' may vary on culture and environment/conflict.\nBut for the US/Canada, I would suggest searching on 'food banks' and 'public food pantries'.\nUSA\nSome of the major cities with open data portals have such datasets. Including:\nSeattle: https://data.seattle.gov/Community/Food-Banks/ryz5-i54h\nBoston: https://data.cityofboston.gov/Health/Food-Pantries-with-Local-Sourcing-Map-View/7ygz-72yc\nWestchester County, NY: http://giswww.westchestergov.com/Metadata/wcpantry.htm\nHere are agencies that keep online list of their food bank locations:\nSecond Harvest of Missouri : http://www.ourcommunityfoodbank.org/index.cfm/pageid/207/fuseaction/user.alphaSelect/m/0\nOregon Food Bank: http://www.oregonfoodbank.org/Our-Work/Regional-Food-Bank-Network\nFeeding America keeps: http://www.feedingamerica.org/find-your-local-foodbank/?_ga=1.223258523.1626167282.1417548401\nFood Bank of South Jersey: http://www.foodbanksj.org/FindHelp.html\nFood Bank of Alaska: http://www.foodbankofalaska.org/viewPage.php?ID=8\nHere's someone's site attempting to (crowd source) collect comparable information across the US:\nhttp://www.foodpantries.org/\n", "Looking for audio data set for English words": "\nLingua Libre has a lot of pronunciation recordings, including in English, by many speakers:\n\nhttps://lingualibre.org\nhttps://commons.wikimedia.org/wiki/Category:Lingua_Libre_pronunciation-eng\nhttps://lingualibre.org/datasets/\n\nUnfortunately I have not found an easy way to find out what words have the most pronunciations. You might have to download the dataset and write a small script to find them.\nContent is available under Creative Commons 4.0 Attribution-ShareAlike.\n\n[Easy to use but not open data] Forvo is a crowdsourced effort to create sound files for every word of every language.\nThe great thing is that a word in a language can have more than 1 sound file.\nIn fact, you will often find sound files created by males and females from various regions with different accents and voices.\nPronunciations are ranked by clarity, so you can focus only on clear sound files, or on the contrary include difficult-to-understand sound files, depending on your goal.\n\nbread: 10 pronunciations from USA, UK, Malaysia\nanything: 30 pronunciations\n\nThis page contains the words that have most pronunciations:\nhttp://www.forvo.com/languages-pronunciations/en/by-popularity/\nUnfortunately the data can be reused but not for commercial purposes.\n", "Open data on public opinion polls (Gallup, PEW, and others)": "\nAll Pew Research Center data is available for free use. But, you have to manually download each dataset.\nData for almost all other public opinion polling is stored at the Roper Center. These data are publicly searchable but not free to download.\n", "Is there an open data feed of storm tracks in the Asia-Pacific region?": "\nYes, there is!  The real-time datasets related to hurricane, cyclone, and typhoon tracks is at the National Hurricane Center.\nNote that people use the terms hurricanes, cyclones, and typhoons interchangeably (as noted by the U.S. National Oceanic and Atmospheric Administration):\nHurricanes, cyclones, and typhoons are all the same weather phenomenon; we just use different names for these storms in different places. In the Atlantic and Northeast Pacific, the term \u201churricane\u201d is used. The same type of disturbance in the Northwest Pacific is called a \u201ctyphoon\u201d and \u201ccyclones\u201d occur in the South Pacific and Indian Ocean.\n\nHurricane track data is provided by NOAA's National Weather Service, and you can find historic tracks for the Pacific and Atlantic.\n", "data.gov - How does one link two selected elements on two tables within openFDA?": "\nTry adding .exact to the end of drugdosagetext \nAlso ... this does not count on the dosage but does get to a query of the former parameters - http://www.researchae.com/drugevent?from_date=2004-01-01&to_date=2015-01-31&from_age=&to_age=&search=venlafaxine+hydrochloride&country=GLOBALLY&patientsex=&manufacturername=&drugbrandname=&druggenericname=&medicinalproduct=&reactionmeddrapt=hypertension&drugclass=&drugindication=&indsubmit=&productndc=&safetyreportid=#stats\nYou can find the code here where you could simply append one of the \"count\" calls with the desired dosage field in the API. https://github.com/GeekNurse/ResearchAE-Open-Source/blob/master/app.rb#L1024\nHelpful?\n", "The Data.gov organization_list CKAN API is not working": "\nThanks to Philip Ashlock for providing the following answer:\n\nThese are both known, but unfortunately unresolved issues. You can\n  track their status on github at\n  https://github.com/GSA/data.gov/issues/294 and \n  https://github.com/GSA/data.gov/issues/492. You can still filter by\n  type using the web UI though, eg\n  https://catalog.data.gov/dataset?organization_type=Federal+Government\n\n", "data request - Where can I find open etymology databases?": "\nOne option is the XML data dump from Wiktionary (link to a recent one).\n\nA complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML. A number of raw database tables in SQL form are also available. \n      These snapshots are provided at the very least monthly and usually twice a month.\n\nThere is also an API and file download based on the Wiktionary data, but I don't know how comprehensive: http://www1.icsi.berkeley.edu/~demelo/etymwn/\n\nThe Etymological Wordnet project provides information about how words in different languages are etymologically related. The information is for the most part mined from Wiktionary. The semi-structured data is turned into a machine-readable etymological database that also incorporates some additional manually added etymological relationships. \n\n", "tool request - Looking for web scraper that works on YouTube channel video catalogs": "\nTubeKit might be of interest to you:\n\nTubeKit is a toolkit for creating YouTube crawlers. It allows one to build one's own crawler that can crawl YouTube based on a set of seed queries and collect up to 16 different attributes.\n\nThe tool is open source (licensed under CC BY-NC-SA*) and has been developed for research purposes.\n* Creative Commons advise against using one of their licenses for software \u2014 but that's really beside the point in the context of this answer ;)\n", "data request - API giving ship positions worldwide": "\nI've used MarineTraffic.com for finding details about yachts and ships I've seen in ports. It's a really cool website.\n\nThey also have an API, but, unfortunately, there is no free access.\n\nThere is an API option from FleetMon that is mostly-free:\n\nThe FleetMon Public API lets software developers create great software apps that are able to display ship positions and master data, port calls, weather conditions, ports and much more. Developers get a powerful tool to connect their software to the FleetMon.com Vessel Database, enabling them to develop own services as well as integrate with existing IT systems and logistics solutions.\n\nThis would be a better option, although I don't know how complete their DB is.\n\n", "data request - Database of Ingress portals": "\nDisclaimer: The Ingress Terms of Service disallows \"extracting, scraping, or indexing\" game data. It's possible to get banned if you use this data.\nI had the same question and found an answer. A helpful person called Lanced has done the hard work of extracting portal data from the game and provides a public API, available at https://lanched.ru/PortalGet/ However, the author warns not to download the whole database.\nThere are essentially three ways to use this API:\n\nThe most basic API takes minimum and maximum bounds in lat/lon coordinates as input and provides GUID, coordinates and title for 1000 portals at a time. You can make several requests (using the offset= parameter) to retrieve all portals in a larger area.\nA more detailed request (using telegram=true), which also returns portal image URL and street address, but is limited to 50 portals per request.\nWith the search API you can search for portal names and addresses. This one also returns images and addresses.\n\nExamples\nExample 1:\nhttps://lanched.ru/PortalGet/getPortals.php?swlat=59.47106&swlng=24.8862&nelat=59.4719&nelng=24.88749&offset=0\n{\n  \"nextOffset\": -1,\n  \"portalData\": [\n    {\n      \"guid\": \"4c95b3ae447d4605a8421e21ace654c4.16\",\n      \"lat\": 59.47143,\n      \"lng\": 24.887374,\n      \"name\": \"Tallinna Teletorn\"\n    },\n    ...\n  ]\n}\n\nExample 2: https://lanched.ru/PortalGet/getPortals.php?swlat=59.47106&swlng=24.8862&nelat=59.4719&nelng=24.88749&offset=0&telegram=true\n{\n  \"nextOffset\": -1,\n  \"portalData\": [\n    {\n      \"guid\": \"4c95b3ae447d4605a8421e21ace654c4.16\",\n      \"lat\": 59.47143,\n      \"lng\": 24.887374,\n      \"name\": \"Tallinna Teletorn\",\n      \"image\": \"http:\\/\\/lh3.ggpht.com\\/GG_O7UHlFcqw6tlGvKPxU9jKZ9y-9oBiheIibRtQUP5Q-hZykSnDVrjzL5v_2SlgQmRxxirIAze79lDuDq1jbg\",\n      \"address\": \"Kloostrimetsa tee 58a, 11913 Tallinn, Estonia\"\n    },\n    ...\n  ]\n}\n\nExample 3: https://lanched.ru/PortalGet/searchPortals.php?query=teletorn\n[\n  {\n    \"guid\": \"4c95b3ae447d4605a8421e21ace654c4.16\",\n    \"lat\": 59.47143,\n    \"lng\": 24.887374,\n    \"image\": \"http:\\/\\/lh3.ggpht.com\\/GG_O7UHlFcqw6tlGvKPxU9jKZ9y-9oBiheIibRtQUP5Q-hZykSnDVrjzL5v_2SlgQmRxxirIAze79lDuDq1jbg\",\n    \"name\": \"Tallinna Teletorn\",\n    \"address\": \"Kloostrimetsa tee 58a, 11913 Tallinn, Estonia\"\n  },\n  ...\n]\n\n", "Look for sets of data of historical dates": "\nYou can find datasets like this from Universities (particularly history depts.). Here's a few:\nUniversity of Pittsburg: http://www.dataverse.pitt.edu/external/datasets.php\nUniversity of North Texas: http://www.paulhensel.org/icowcol.html\nUniversity of Wisconsin-Madison: http://www.sage.wisc.edu/download/crop1700/hist_croplands.html\nUppsalla University: http://www.pcr.uu.se/research/ucdp/datasets/ucdp_prio_armed_conflict_dataset/\n", "data request - Where can I find the locations of factories in the U.S. and Japan?": "\nIn the United States, you might start by looking at water permits. Most large factories have to get permits for their waste and storm water runoff.\nHere are discharge permits for Illinois. http://dataservices.epa.illinois.gov/dmrdata/dmrsearch.aspx\nStorm water permits: Many industrial facilities have to get a permit for the pollution caused by water running off a location. Here's data for Illinois\n", "data request - Coordinates of all embassies and consulates": "\nFor a wider listing of diplomatic entities, consider using Open Street Maps. \nFor example, you can use the Tag: amenity=embassy. This data source includes the following dimensions:\n \"lat\"\n \"lon\"\n  \"tags\": {\n    \"addr:city\"\n    \"addr:country\"\n    \"addr:housenumber\"\n    \"addr:postcode\"\n    \"addr:street\"\n    \"amenity\"\n    \"contact:email\"\n    \"contact:fax\"\n    \"contact:phone\"\n    \"country\"\n    \"name\"\n    \"name:de\"\n    \"name:en\"\n    \"name:fr\"\n    \"opening_hours\"\n    \"website\"}\n\nWith 5,600 objects including consulates, missions, embassies (as of Dec 2014).\n\nTo test it out, you can use the overpass-turbo API.\n\nAnd the corresponding JSON data:\n\n", "healthcare finder api - Is 2015 data available for small group plans?": "\nYes, 2015 data is certainly available. Here is a sample request, that returns 309 plans for the Fairfax, VA area as of 12/08/2014:\ncurl 'https://api.finder.healthcare.gov/v3.0/getSMGPlanQuotes' -H 'Content-Type: application/xml' --data-binary $'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<p:PlanQuoteRequest xmlns:p=\"http://hios.cms.org/api\" xmlns:p1=\"http://hios.cms.org/api-types\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://hios.cms.org/api hios-api-11.0.xsd \">\\n  <p:Enrollees>\\n    <p1:DateOfBirth>1984-01-01</p1:DateOfBirth>\\n    <p1:Gender>Male</p1:Gender>\\n    \\n    <p1:Relation>SELF</p1:Relation>\\n    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>\\n  </p:Enrollees>\\n  \\n\\n \\n    \\n\\n  <p:Location>\\n    <p1:ZipCode>22031</p1:ZipCode>\\n     <p1:County>\\n         <p1:FipsCode>51059</p1:FipsCode>\\n         <p1:CountyName>FAIRFAX</p1:CountyName>\\n         <p1:StateCode>VA</p1:StateCode>\\n      </p1:County>\\n  </p:Location>\\n  <p:InsuranceEffectiveDate>2015-01-01</p:InsuranceEffectiveDate>\\n  <p:Market>SmallGroup</p:Market>\\n  <p:IsFilterAnalysisRequiredIndicator>true</p:IsFilterAnalysisRequiredIndicator>\\n  \\n  <p:PaginationInformation>\\n    <p1:PageNumber>1</p1:PageNumber>\\n    <p1:PageSize>10</p1:PageSize>\\n  </p:PaginationInformation>\\n\\n  <p:SortOrder>\\n            <p1:SortField>BASE RATE</p1:SortField>\\n            <p1:SortDirection>ASC</p1:SortDirection>\\n  </p:SortOrder>\\n  <p:Filter>\\n      \\n      \\n      \\n      \\n      \\n      \\n     \\n      \\n    \\n</p:Filter>\\n  \\n</p:PlanQuoteRequest>\\n' --compressed\n\n", "economics - Global customs and trade import/export data on the shipping container level": "\nI'm clear you're looking for open data and NOT vendors, but I've found some of these vendors do give attribution to their sources and thus aid your search from their marketing materials:\n\nPanjiva\nImport Genius\nDatamyne\nGoodwill\nFleetmon\nMarine Traffic\nVessel Finder\nContainer Ship\nMaritime Connector\nMarine Vessel Traffic\nAir Nav Systems\nCMA\nMarine Vessel Traffic (more)\n\n", "data request - A table mapping from US county or ZIP to Nielsen Designated Market Area (DMA)": "\nA 2011 court decision found that Nielsen's DMA maps are protectable by copyright. This article from Bloomberg goes into the details. Here is the formal opinion\nThus, technically, DMA shapes are not freely available and must be licensed from Nielsen.\n", "data.gov - Accessing datasets from HUD and VA data portal sites": "\nTrainingday,\nI believe the master list tht fhamap.com refers to of all HUD owned, managed or approved properties is derived from their dataset of physical inspections of those properties. This and related datasets can be found here:\nhttp://www.huduser.org/portal/datasets/pis.html\nI've used these datasets myself. I also have a version of these datasets converted to CSV format using our linked data vocabulary.\nhttp://www.opengeocode.org/cude1.2/HUD/PHA/index.php\nIt was as clear what you were looking for as a 'master list' from the VA. But the VA website has an open data portal and API. You can download a complete (national) list of VA hospitals and homeless services for Vets.\nhttp://www.va.gov/data/\n", "data.gov - FedBizopps Raw Data - API": "\nFull disclosure: I am a GSA employee and the Tech Lead for FBOpen, a website and API for search and discovery of federal business opportunities.\nThere is indeed both bulk data and an fbo.gov API available, although I can only offer experience with the former. There are two different versions of the bulk FTP downloads, weekly files and nightly files. These two are quite different. The weekly XML is proper XML and straightforward to parse. The nightly XML is XML in name and appearance only, but there is a special parser grammar available for it.\nhttps://github.com/presidential-innovation-fellows/fbo-parser\nDepending on your needs, you can also use our FBOpen API. Since we rely on the nightly files, we will generally be about a day, sometimes two, behind fbo.gov. You can find more information at our Github repo, and please offer feedback and ask any questions by opening an Issue.\n", "usa - Looking for Arrests Data at a state level across years": "\nhere is complete documented R code to work with all of the microdata that you are looking for\nhttp://www.asdfree.com/search/label/national%20incident-based%20reporting%20system%20%28nibrs%29\n", "data request - Official APIs for postal stamps releases by French postal service La Poste": "\nThe Universal Postal Union (UPU) keeps a database of issued stamps starting in 2002 (it does not go earlier). For France, you can find the stamps issued 2002-2014 here:\nhttp://www.wnsstamps.post/en/\nCopyright information from that website:\n\n\u00a9 Copyright UPU - WADP\n  The stamp designs are the property of their respective issuing postal authority.\n  The issuing postal authorities have allowed the reproduction of the stamps displayed on this website.\n\n", "data request - Where can I download legend for Corine land cover 2006 classes?": "\nI believe I found what you are looking for by googling. This PDF is labeled the CLC 2006 Legend.\nhttp://sia.eionet.europa.eu/CLC2006/CLC_Legeng.pdf\nThey also have the legend as an image on this page:\nhttp://www.eea.europa.eu/data-and-maps/figures/corine-land-cover-2006-by-country/legend\nI also found the spreadsheet version for 1990.\nhttp://www.eea.europa.eu/data-and-maps/data/corine-land-cover-clc1990-250-m-version-8-2005/corine-land-cover-1990-classes-and-rgb-color-codes/clc1990legend.xls\nAs a courtesy, I compared the 2000 XLS spreadsheet to the 2006 PDF. The data is identical.\n", "data request - Crime Statistics on Abductions by Gender": "\nThis publication has a lot detailed statistics, including breakdown by state/city on abduction/kidnappings. But I did not see a breakdown by gender.\nhttp://www.insideprison.com/Crime_Rates_Detailed_splash.asp?crime=100&crimeName=Kidnaping/Abduction\nThe FBI UCR tables do have a breakdown of arrests by gender and category, though they do not breakout Kidnapping/Abductions in this set of tables. I suspect they are merged into other categories. You may need to do some researching on the site.\nhttp://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2013/crime-in-the-u.s.-2013/persons-arrested/persons-arrested\n", "crime - US arrest data in 2010 by race and county": "\nThis link is to the FBI's UCR data on crime statistics (2010) broken down by race and gender\nhttp://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/persons-arrested/persons-arrested\nThis link shows a subset of the tables where the data is broken down by cities and counties.\nhttp://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/2010/crime-in-the-u.s.-2010/persons-arrested/browse-by/cities-and-counties-grouped-by-size-population-group\nThis is the home page for all UCR datasets:\nhttp://www.fbi.gov/stats-services/crimestats\n", "government - Is there any open data for Emergency Room waiting time?": "\nThere is no one source that I know of for realtime wait times though some larger networks do have apps or mobile sites that you might look into scraping though of course that has many disadvantages.\nIn terms of historical data, ProPublica has some great data and analysis at https://projects.propublica.org/emergency/\nI believe the sole source of the ProPublica ER wait time site is CMS Hospital Compare data which can be found at https://data.medicare.gov/Hospital-Compare/Timely-and-Effective-Care-Hospital/yv7e-xc69\n", "usa - ACS PUMS Data Dictionary Codes in .csv?": "\nThere are a plethora of XLS converters and scripts out there, but the easiest way IMO is to use google drive in this case. I uploaded the first of the dictionaries, 2011-2013 3-year Code Lists, took about 20 seconds. Now you can edit it in the browser or download as CSV. you can do the same for the text files. check it out - LINK\n", "data request - Collecting Canadian postal address information": "\nYou can use metadata from the Google i18n Internationalization project (link). For example, the address completion metadata from the libaddressinput package (C++ and Java tools) used for Android (Java) and Chromium OS (C++).\nThe raw data is stored here: https://i18napis.appspot.com/address\nFor Canada, it looks like this:\n    {\n   \"lang\":\"en\",\n   \"upper\":\"ACNOSZ\",\n   \"zipex\":\"H3Z 2Y7,V8X 3X4,T0L 1K0,T0H 1A0,K1A 0B1\",\n   \"posturl\":\"http://www.canadapost.ca/cpotools/apps/fpc/personal/findByCity?execution=e2s1\",\n   \"zip\":\"[ABCEGHJKLMNPRSTVXY]\\\\d[ABCEGHJ-NPRSTV-Z][ ]?\\\\d[ABCEGHJ-NPRSTV-Z]\\\\d\",\n   \"fmt\":\"%N%n%O%n%A%n%C %S %Z\",\n   \"require\":\"ACSZ\",\n   \"name\":\"CANADA\",\n   \"languages\":\"en~fr\",\n   \"sub_keys\":\"AB~BC~MB~NB~NL~NT~NS~NU~ON~PE~QC~SK~YT\",\n   \"key\":\"CA\",\n   \"id\":\"data/CA\",\n   \"sub_names\":\"Alberta~British Columbia~Manitoba~New Brunswick~Newfoundland and Labrador~Northwest Territories~Nova Scotia~Nunavut~Ontario~Prince Edward Island~Quebec~Saskatchewan~Yukon\",\n   \"sub_zips\":\"T~V~R~E~A~X0E|X0G|X1A~B~X0A|X0B|X0C~K|L|M|N|P~C~G|H|J|K1A~S|R8A~Y\"\n}\n\n(link)\nAnd the full list for Canadian addresses:\ndata/CA\ndata/CA--fr\ndata/CA/AB\ndata/CA/AB--fr\ndata/CA/BC\ndata/CA/BC--fr\ndata/CA/MB\ndata/CA/MB--fr\ndata/CA/NB\ndata/CA/NB--fr\ndata/CA/NL\ndata/CA/NL--fr\ndata/CA/NS\ndata/CA/NS--fr\ndata/CA/NT\ndata/CA/NT--fr\ndata/CA/NU\ndata/CA/NU--fr\ndata/CA/ON\ndata/CA/ON--fr\ndata/CA/PE\ndata/CA/PE--fr\ndata/CA/QC\ndata/CA/QC--fr\ndata/CA/SK\ndata/CA/SK--fr\ndata/CA/YT\ndata/CA/YT--fr \n\nYou then use the individual sub-keys (i.e. QC in french) to download the address formats for that region.\n{\n   \"lang\":\"fr\",\n   \"name\":\"Qu\u00e9bec\",\n   \"zip\":\"G|H|J|K1A\",\n   \"key\":\"QC\",\n   \"id\":\"data/CA/QC--fr\"\n}\n\n", "data request - Which Governments publish their Ledgers and Accounts?": "\nOpenSpending\nThis website collects datasets related to public spending by country:\nhttps://openspending.org/\n\nOther resources\nYou should check Open Budget Survey's top 10's national websites for the data.\nAmong all-in-one place resources, see enigma.io. Here are some datasets:\n\nUnited States 2014\nUnited Kingdom 2014\n\n\nCountry-specific: Russia\nRussia does well in opening its budget to the public: 10th place in Open Budget Survey.\nYou can find user-friendly interfaces for:\nBudget\nhttp://budget4me.ru/ (currently under construction)\nContracts\nFor all government units and SOEs. API and dataset access forms:\nhttp://clearspending.ru/ (available in English)\n", "Weather warning data": "\nHere are some US thoughts:\nFor US warnings/watches, you probably want to go the the storm prediction center,\nhttp://www.spc.noaa.gov/wcm/\nand you may want to look at the weather related shapefiles\nhttp://www.nws.noaa.gov/geodata/\nand the current alert system probably has an archive, but I didn't see it on first glance:\nhttp://alerts.weather.gov/\nI would suggest digging into the census tools related to disaster management.\nhttp://onthemap.ces.census.gov/em/\nThe \"on the map\" product is a mashup of (1) weather data and (2) population/business data so that the impact of weather on population/businesses can be assessed.\nIt is only actual federally designated areas, I do not know if it has warnings/watches.\n", "healthcare finder api - I'm trying to make a POST request to HealthcareFinder API v3.0, but I can not. Returns this error": "\nMake sure you include the Content-Type header and the XML for the request in the POST body.\nWhen I try their example it seems to work fine and the HTTP header includes Access-Control-Allow-Origin: * which should work for CORS. \n\n", "data request - Open dataset for the number of pets per country": "\nCommercial Source:\nYou can get what you need for the US from this publication. But it is copyrighted material and expensive (~$300).\nThe all-new 2012 edition of the U.S. Pet Ownership and Demographics Sourcebook is the largest, most statistically accurate and complete survey of the pet owning public and pet population demographics. Drawn from a national survey of over 50,000 households, the survey results are presented alongside the results from similar surveys dating back to 1987, illustrating long-term trends.\nhttps://www.avma.org/KB/Resources/Statistics/Pages/Market-research-statistics-US-Pet-Ownership-Demographics-Sourcebook.aspx\nFree (but Coarse):\nThe US Census publishes some very coarse information on pet ownership, but if you look where the data comes from it is the same source (American Veterinary Medical Association)\nhttps://www.census.gov/compendia/statab/2012/tables/12s1241.pdf\nThe Human Society has some coarse data as well:\nhttp://www.humanesociety.org/issues/pet_overpopulation/facts/pet_ownership_statistics.html\nThe BLS has some coarse data as well:\nhttp://www.bls.gov/opub/btn/volume-2/pdf/spending-on-pets.pdf\nPublic Datasets:\nAustin, TX - animal intake:\nhttps://data.austintexas.gov/dataset/2011-Animal-Intake-Report-Cats-and-Dogs-Exclusivel/wrwk-skv6\nColorado - animal intake:\nhttps://data.colorado.gov/Agriculture/2011-PACFA-Shelter-Intake-Statistics/qv2q-ek3a\nWeatherford, TX - animal intake:\nhttp://www.weatherfordtx.gov/index.aspx?NID=1469\nAdelaide, Australia - dog registration:\nhttp://data.sa.gov.au/dataset?tags=dog+ownership&tags=city\nNew Zealand - Dog Control Statistics\nhttps://data.govt.nz/dataset/show/3282\n", "data request - Where can I get the standard iOS 8 bar icons in a vector graphics format?": "\nAustin Andrews has built (and collated from elsewhere, when licensing permits) a collection of icons that follow the material design. These are also available from his Github repository in a variety of formats, including SVG.\nTo use the actual Apple glyphs within an iOS app, then you can refer to the documentation for UIBarButtonSystemItem in UIBarButtonItem Class Reference\n", "tool request - Vocab to describe Image Data / Pictures": "\nAre you referring to Focal Point?\n\nFocal Point: Pure HTML/CSS Adaptive Images Framework\nA small set of CSS classnames to help keep images cropped on the focal point for responsive designs. Using only HTML/CSS, web authors can specify an image's focal point, which stays as the image's primary focus as the image scales on responsive webpages.\n\nIf this is not what you are looking for, they also link to related resources that might be of interest to you.\n", "data format - Applying filters to headers in a huge CSV file": "\nThe csvkit python library is great for transforming big csv datasets in the style of a unix command line tool (like sed). It has many small utilities that do one thing well each so you can compose them in helpful ways. In your case, csvcut can extract certain columns from a csv. \nFrom their docs:\nExtract columns named \u201cTOTAL\u201d and \u201cState Name\u201d (in that order): \n\n$ csvcut -c TOTAL,\"State Name\" examples/realdata/FY09_EDU_Recipients_by_State.csv\n\nGood luck!\n", "data request - Dump of WikiLeaks": "\nOctober the 18th 2016: https://file.wikileaks.org/file/ was made publicly visible and file dates and timestamps changed to 1984. \n", "data.gov - DOL Wage and Hour data: Counting backwages under Fair Labor Standards Act": "\nThere's a data catalog here: http://ogesdw.dol.gov/views/data_catalogs.php (and go to the Wage and Hour Compliance Action Data page). According to the dictionary, flsa_bw_atp_amt is the \"BW Agreed to under FLSA (Fair Labor Standards Act)\".\nBut it sounds like your question is more about how to get real qualitative information about the data fields. If that is the case I would suggest you try to speak with someone from the department. The WHD's website provides contact information for several offices - I'd give them a call. I've found talking to a real live person to be infinitely helpful in figuring out what exactly I'm looking at.\n", "data.gov - Gov data may be used only for statistical purposes?": "\nMy interpretation is the data can only be used in aggregate (counts and statistics). You cannot use it to identify an individual person. For example, linking a public event (e.g. car accident) with public health data (e.g. Medicare ambulatory event) to gain an individual's medical information. \nhttp://www.bloomberg.com/infographics/2013-06-05/reidentifying-anonymous-medical-records.html\n", "data request - Database of all Coca Cola (and its franchises) bottling plants locations worldwide": "\nThe data is not apparently available as a downloadable dataset, but there are some documents and visualizations available.\n\nA great visualization of all the Coca Cola facilities (and an associated KML file -- thanks @philshem)\nCoca-Cola's description of the facilities in the network\nOverall performance of the facilities in the annual Sustainability Report\nOverview of global facilities\n\n", "where can I find gov datasets (from data.gov, bea etc) in MySQL format?": "\nyou can search data.gov, this search returns over 300:\nhttp://catalog.data.gov/dataset?q=MySQL&sort=score+desc%2C+name+asc\nand this search within datasets hosted by data.gov reveals 12:\nhttp://www.data.gov/search-results?group=site&q=MySQL \n", "data request - Yule's disturbed pendulum time series example": "\nSince you are looking for only a pendulum, and not a generic time series with random noise (like @blairchristian posted), then I would consider simulating the data.\nHere is an example of a pendulum code, although you don't need the visualization part. It's a pretty common coding assignment (not as common as double pendulum it seems), so you can probably find your favorite code.\nTo add the random perturbations, you just need to determine which dimensions are allowed (i.e. if the pendulum is like the Foucault Pendulum, the perhaps the peas can only add random force in the horizontal plane of motion.)\njust for fun, consider using quasi-random perturbations that have burst or breakout behavior, like\n\nstock market volume deviation from the mean of that day of the week (example, Apple stock over last 10 years)\nlive data, i.e. from the twitter public stream, that perturbs the pendulum based on the occurrence of tweets or aggregate data from tweets\n\n\n", "data request - Energy Consumption of individual buildings": "\ni think open ei has this in their building energy data book:\nhttp://en.openei.org/doe-opendata/dataset/buildings-energy-data-book\nhere's a section from that which is a survey of building energy consumption and extends past 2011:\nhttp://www.eia.gov/consumption/commercial/data/2012/\nlastly, try the data hub for the energy performance of buildings for the eu:\nhttp://www.buildingsdata.eu/ \n", "data request - Dictionary containing keywords used on the Web": "\nYou wrote:\n\nI considered crawling twitter hashtags, or facebook hashtags or google trends, but the thing is people use many useless keywords and hashtags nowadays like #I_ate_pizza #I_love_bieber and they don't search for keywords, the majority search What gift to buy for my girlfriend\n\nI don't really know Facebook, but regarding the Twitter trends:\n\nunderscores aren't often used in hashtags\ntrends don't have to be a hashtag. Here is an example of my local trends:\n\n\nThe Twitter API has several methods that can be used to collect trends (for example), or to create your own (public stream). What is missing is the ability to search for tweets from the complete set (relevance, not completeness).\nAlso consider LinkedIn API and the many other web APIs.\nIf you use Google Trends, then the data they provide is stripped of the non-trending words. For example, the search:\nWhat gift to buy for my girlfriend\n\nwould come back as part of a trend on:\ngirlfriend\n\n(maybe on Valentine's day). None of the stop words are included (otherwise, 'the' would be always trending).\nGoogle Trends doesn't have an API, but you can follow instructions from here to hack it. The raw data is in JSON form here:\nhttp://hawttrends.appspot.com/api/terms/\n\nand using this curl command you can change date and place:\ncurl --data \"ajax=1&geo=US&date=201310\" http://www.google.com/trends/topcharts/category\n\nThis request would give you Google trends from the US in October 2013. Here is an unformatted JSON file for the above curl request - LINK. Here is the same data in a JSON formatted web tool - LINK.\n", "extracting - Monthly data in Google Trends": "\nnote: this answer is based on another answer, with some modifications for your question.\nGoogle Trends doesn't have an API, but you can follow instructions from this great blog post to hack it. The raw data is in JSON form here:\nhttp://hawttrends.appspot.com/api/terms/\n\nand using this curl command you can change date and region:\ncurl --data \"ajax=1&geo=US&date=201310\" http://www.google.com/trends/topcharts/category\n\nThis request would give you Google trends from the US in October 2013.\nOutput:\n\nUnformatted JSON file for the above curl request - LINK. \nSame data in a JSON formatted web tool - LINK.\n\nIf you aren't comfortable programming a tool to parse this JSON output, you can use something like OpenRefine as a front-end. With this kind of tool, I think you can convert to CSV. There are also some webtools to convert JSON to CSV (example). Note that the JSON format is for data-interchange and requires understanding the structure for proper transfer to a tabular format (like CSV). Data is stored as key/value and the values can be dictionaries, lists, or any other data-type.\n", "medical - Do you know where I can find health data sets?": "\nI would recommend taking a look at the Medicare Expenditure Panel Survey (MEPS), a nationally representative survey on the health status of individuals that includes patient discharge information. You could also try looking at the National Hospital Discharge Survey (NHDS), which is a selection of patient discharge data from randomly sampled hospitals.\nAs a health researcher, you'll definitely want to become acquainted with the ICD 9 diagnosis and procedure codes available by fiscal year here. For example, ischemic heart disease is \"410**\" or \"411**\". Since you're new to the field, I suggest you seek some in person advice about how to use these two completely different data setups. And it would be worthwhile to become familiar with ICD codes. \n", "data request - Where can i find the surface area and the number of premises for all UK districts?": "\nDisclaimer: I want to say I am not an expert at using UK Ordnance Survey data.\nAs Nils pointed out, page 177 in this wifi/broadband coverage report for 2014 makes references to the number of premises (buildings). The report refers to the data source as coming from the UK Ordnance Survey. I looked at the sample datasets, and they do contain building addresses per city/county equivalent.\nThere are a number of licensing options. Some are free. As far as understanding the licensing of this dataset, I would start here:\nhttps://www.ordnancesurvey.co.uk/business-and-government/licensing/using-creating-data-with-os-products/index.html\nOn the Office of National Statistics (ONS) I searched for 'land area'. The resulting list was fairly long. I did find some CSV/Excel datasets that contained land area on at least a county equivalent.\nhttp://www.ons.gov.uk/ons/datasets-and-tables/index.html?pageSize=50&sortBy=none&sortDirection=none&newquery=land+area&content-type=Reference+table&content-type=Dataset\n", "How do I get a full list of datasets available on Data.Gov using the CKAN API?": "\nFor now the Data.gov CKAN API redirects package_list to package_search and for package_search the relevant Solr parameters to limit your query are rows and start.\nFor example, http://catalog.data.gov/api/3/action/package_search?rows=1000&start=0\nAnd then page through. \nIf you or anyone has additional feedback on what Data.gov CKAN API documentation would be helpful or would like to help contribute there is a relevant issue open on GitHub. \n", "Historical weather data": "\nFor international and historical data, and for a modest number of requests per day, I personally recommend the Wunderground API. Once you register, you can get 500 free requests per day.\nThe URL for historical data will look like this:\nhttp://api.wunderground.com/api/Your_Key/history_YYYYMMDD/q/CA/San_Francisco.json\n\nI've posted a sample code (python 2.7) that you can use (and improve!) - LINK. I would run this code every day, just changing the year (currently it's set for 2013). The output of the code is a CSV file, but you can store the JSON and/or parse as needed.\n", "data request - Fantasy Soccer (English Premier League) datasets": "\nI like what you are working on!  Looks like about the same time I was adding a Github project/repo to start sharing R code for analysis we were motivated to try out.  I've not added much code to Github yet but hope to in the near future with data for examples.\nAnother site to look at - you have more more expertise inputting data of the sites than I do, is the primary EPL site and fantasy game.  My older son and I have been playing that one for many years and like the mix of data made available.  Much more than other sites.  We also manually compile some data on specific areas of interest such as shots (an associated goalie data).  Every good analytics article you read on the web or in print always gives a few more questions.\n", "api - Public domain paintings database": "\nCheck out the Walters Art Museum Collections API:\nhttps://github.com/WaltersArtMuseum/walters-api\nMight give you access to what you are looking for...\n", "geospatial - Where can I download photoperiod (daylength) data?": "\nTurns out there is a function daylength in the geosphere package in R that calculates day length for any latitude and date.\n", "What is the Date Format for the Healthcare Finder API?": "\nShort answer: \"Year-Month-Day\", or CCYY-MM-DD\nLonger answer: The Healthcare Finder API uses XML Schema to define the API schema..\nSo a line like <xs:element name=\"InsuranceEffectiveDate\" type=\"xs:date\"/> indicates that you should format the value according to XML Schema. This is a reference page for the date type \n(the reference to xsd:date on the linked page instead of xs:date as specified in the API schema document is a minor style choice, and not a cause for concern.)\n", "business - Where can I find project risk management data?": "\nSome project risk management data can be found within the following resources:\n\nEnterprise Risk Management Initiative's Surveys and Benchmarking Data (free)\nTM Forum's Business Benchmarking Database (commercial)\nInternational Software Benchmarking Standards Group (ISBSG) Data Portal or, alternatively, ISBSG Industry Data Sets (commercial; industry focus: software / IT project data)\n\nNOTES: 1) I don't think that Project Management Institute (PMP) has project risk management data, as @Joe suggested. At least, I haven't been able to find it. 2) Obviously, there exists other industry-focused project risk management data, similar to the one referenced above, focused on the software / IT industry.\n", "data request - ACS 2013 5 Year METRO areas in KML": "\nAs some comments to your question note, it's not entirely clear what you're looking for.\nIf you're looking for the CBSA geometries as KML, your best route would probably be to get the shapefiles from the Census and then convert to KML. (If you don't have tools to convert, googling shapefile to kml will turn up several online options.)\nShapefiles for metropolitan divisions are also available\nAre you using \"city\" as synonymous with \"metro area\"? Or are you looking for the shapes of cities in given metro areas? The Census Bureau offers a cross reference of principal cities of CBSAs as an Excel file. It would be fairly labor intensive to retrieve the city boundaries for each of those (~1250 cities).\nAre you trying to compare data between CBSAs, NECTAs, and Metropolitan divisions? Proceed with caution. Metropolitan divisions are a subset of CBSAs. NECTAs are geographic agglomerations similar to CBSAs, but a NECTA is defined as a list of cities/towns while a CBSA is defined as a list of counties. \n", "data request - Where can I find an ontology for \u201cproject\u201d?": "\nDOAP - Description of a Project. still alive, now on github\nhttps://github.com/edumbill/doap/wiki\n", "data request - Historical Mobile App Rankings/Downloads and Prices": "\nI have shared the statistics of my 2 million+ downloads app here:\nhttps://github.com/nicolas-raoul/google-play-statistics\nData available: Installs/uninstalls/ratings/crashes data by country/language/device/version/date/etc.\nIt is a free app for Android.\n", "data.gov - Does anyone know of a U.S. city that has crime data with location?": "\nThe Open Knowledge Foundation keeps a list of Cities that publish crime datasets. Most datasets have location data. In some cases the location is in State Plane coordinates instead of Lat/Lon. They list 51 cities.\nhttp://us-city.census.okfn.org/dataset/crime-stats\nBelow is a blog posting I posted a year ago on methods for doing crime analysis:\nhttp://www.opengeocode.org/articles/crime%20analysis.txt\nBelow is a PPT presentation I've given in the Portland, OR area. Starting at slide 31, we show our method and results for doing location based crime analysis in Portland, OR for correlations with public transit stops and alcohol establishments.\nhttp://www.opengeocode.org/articles/Open%20Data.pptx\n", "data request - Database of wheelchair-accessible places": "\nNPR has been developing a crowdsourced database of accessible playgrounds in the US. The data is downloadable as CSV or JSON. The license for the data isn't clearly stated, but the language suggests that they want it to be freely used.\n", "releasing data - Best place to publish my Android app's Google Play statistics?": "\nmake a github repo, save your data there:\ncentral location\nversioning\nsocial coding!!\nyou can upload virtually any format, including csv ->\nbonus! github displays csvs in the browser!\nalso manually editing csvs in the browser on github is actually pretty straightforward.  \nyou could also just upload them to google drive/docs and share/publish the data there...\n", "data request - Open downloadable recipe database?": "\nhrecipe (and microformats in general) are the bees knees and lucky for you are widely employed across the web; here's a list of sites actively publishing hrecipes in the wild; you can scrape and parse as you please!\nhttp://www.eat-vegan.rocks/\nhttp://funcook.com/\nhttp://www.therecipedepository.com\nhttp://sabores.sapo.pt/\nhttp://www.epicurious.com/\nhttp://www.williams-sonoma.com/\nhttp://foodnetwork.com/\nhttp://www.plantoeat.com/recipe_book\nhttp://www.essen-und-trinken.de\nhttp://itsripe.com/recipes/\nthis list was lifted from the hrecipes specification on the microformats wiki ->\nhttp://microformats.org/wiki/hrecipe \nEDIT:\nAuntie's Recipes Repository \n", "data request - Public database of book titles?": "\nOpen library has a goal of one page for every book, and has the data you seek.\nIt's run by the internet archive.\n", "What does it take to get an app featured on data.gov?": "\nOn the Contact page at Data.gov you can select an option in the form to Submit an Application.\nRequirements for the apps listed are also posted. Those apps must:\n\n\"Use open government data from the United States\nBe accessible, vetted, and available\nBe, for the majority, free and do not require registration to use\"\n\nMany of the apps have come out of businesses using open government data (see examples at the Open Data 500), city and Federal hack-a-thons, or challenges issued by the Federal government.  But, anyone is welcome to submit their app or service to Data.gov.\n", "data request - Japan software export statistics": "\nI am not sure of how Japan handles exports but the US Dept of Commerce Bureau of Industry and Security (BIS) has a list of 10 Commerce Control List Categories and  these are broken into 5 Product Groups \nIt looks like software is a Product Group and this means it would need to be aggregated across categories.\nThe staff at BIS may be your best bet on connecting with someone that can lead to the data.\nIf you find out anything, please post back here.\n", "europe - Demographic data for Belgium or the Netherlands": "\nNote that population for postal and political regions are not often provided in the same public dataset.\n\nAn option for Belgium:\n\n2013 population data from the government site - LINK (see right side of page)\n\nExcel file for \"population by commune\" - LINK\nunfortunately, uses \"code INS\" and not \"postal code\"\n\ncan join population data with postal data from BE post office, on commune name - LINK (see Excel links on right side of page)\n\nExcel download, sorted by postal code - LINK\n\n\n\n(It's just a bunch of Googling. If it's useful, I can do the same for the Netherlands.)\n", "usa - Police precinct jurisdiction data for joining UCR with census": "\nAfter asking, I found via Googling a series from the National Archive of Criminal Justice Data (NACJD) called the Law Enforcement Agency Identifiers Crosswalk.\nHere is its description from the NACJD website:\n\nThe crosswalk file is designed to provide geographic and other identification information for each record included in either the Federal Bureau of Investigation's Uniform Crime Reporting (UCR) Program files or in the Bureau of Justice Statistics' Census of State and Local Law Enforcement Agencies (CSLLEA). The main variables each record contains are the alpha state code, county name, place name, government agency name, police agency name, government identification number, Federal Information Processing Standards (FIPS) state, county, and place codes, and Originating Agency Identifier (ORI) code. These variables allow a researcher to take agency-level data, combine it with Bureau of the Census and BJS data, and perform place-level and government-level analyses.\n\nAnd an introductory technical report from 1998, when the first dataset was published: http://www.bjs.gov/content/pub/pdf/lucrdod.pdf\n", "data request - Pollution DataSet": "\nThe WHO provides global data for many cities. It doesn't seem to break down the overall air pollution into molecular components, though.\n\nWebsite with details\nAmbient (outdoor) air pollution database, by country and city (Excel file)\n\nMany individual cities or regions provide air quality data. A short example, although basically every city or region will have some level of data or map available:\n\nOntario, which can be filtered on only Toronto - http://www.airqualityontario.com/history/\nLondon\n\nYou may also find good data from the EPA (US) website:\n\nOverview of data sets\nThere are several resources that have varying levels of details\n\n\nNote that air pollution and greenhouse gases are different molecules and at different levels of the atmosphere. Air pollutants are often measured with PM (particular matter) sizes (wikipedia) \n\nThe composition of particulate matter that generally causes visual effects such as smog consists of sulfur dioxide, nitrogen oxides, carbon monoxide, mineral dust, organic matter, and elemental carbon also known as black carbon or soot. \n\nIf you are looking for only greenhouse gases, you may not find data with such accurate geolocation.\n", "api - Getting more than 100 records from OpenFDA in MS Excel": "\nI believe the limit is now 1000 with an API key. Try getting and adding an API key and changing the limit to 1000\n", "licensing - How I can get list of licenses used in Data.gov?": "\nThe licenses for the data accessible through Data.gov vary, sometimes based on the source (Federal government, local government, or university) or on previous agreements that led to the data collection or publishing.  \nIn general, the licenses are governed by Project Open Data and will reflect any issue with reuse and redistribution.  Some of the licenses used are standardized, while others may be particular to a specific dataset.\nOn each dataset's page, there is a \"license\" field in the metadata that should answer your question about the use or reuse of that dataset.  For example, NOAA provides a dataset of earthquakes for the last 4100 years! Look at the dataset page, scroll to the bottom and expand the metadata (click \"Show More\"). The license field notes, \"Produced by the NOAA National Geophysical Data Center. Not subject to copyright protection within the United States.\"\nIf there is not license noted, the default is that the data is in the public domain and available to use freely and without restriction (\"the state of belonging or being available to the public as a whole, and therefore not subject to copyright\").\n", "data request - Historical OFAC SDN lists": "\nThe Archive of Changes to the SDN and NS-ISA Lists looks complete to me.\nThere's no XML. At a glance the \".txt\" files seem formulaic. They might not be difficult to transform using a programming language.\nIf there is more information that you are seeking, it may be necessary to submit a Freedom of Information Act request to the Office of Foreign Asset Control, which is among the Departmental Offices in the Department of the Treasury.\n", "data request - Database of shrines in Japan": "\nThe Association of Shinto Shrines I would expect to have a database of shrine locations - but they do not have it online. Perhaps you can contact them. Here is their link:\nhttp://www.jinjahoncho.or.jp/en/\n\nThis is a dataset of ancient Shinto shrines (warning: it is in Japanese):\nhttp://21coe.kokugakuin.ac.jp/db/jinja/index_e.html\nFor each shrine, the available information is:\n\nName of the shrine\nCommandery (Japanese territorial subdivision system used centuries ago)\n\n", "data request - Android device name->maker mapping": "\nThis list from Google has devices that support Google Play, and are grouped by manufacturer. The list is updated regularly. Unfortunately, it's a PDF - LINK.\n\nOne option to make the data more machine readable is to convert to TXT with pdftotext - raw text\npdftotext -layout devices.pdf\n\n", "Cosmetic Names (INCI nomenclature) and URLs to OpenFDA": "\nIn which API endpoint are you searching? Drug adverse events? Drug labeling? You should look at the particular API endpoint you're searching to see what fields are available, and do some quick searches for the kinds of products or records you're interested in to see what kinds of data they have and in what fields.\nFor instance, there are a good number of cosmetics products in the drug product labeling API:\nhttps://api.fda.gov/drug/label | Docs at https://open.fda.gov/drug/label/reference/\nThese products tend to list ingredients in the active_ingredient and inactive_ingredient fields. In this API, these fields contain ingredients as reported to FDA by the product manufacturer. I don't know if the manufacturers use INCI standard terms when describing ingredients.\nHere's a search that may be of relevance:\nhttps://api.fda.gov/drug/label.json?search=inactive_ingredient:(Ammonium+AND+Methacrylate+AND+Copolymer)\nIn this case, the API returns ~90 product labels where the inactive_ingredient field contains ALL of those words, in any order, in any combination.\nI did a quick search and didn't find the word nitrophenol in any records in that endpoint. https://api.fda.gov/drug/label.json?search=inactive_ingredient:(nitrophenol)\nWith respect to using UNII, you should know that the product labeling endpoint in particular only provides UNII codes for active ingredients, not inactive ingredients.\nHTH.\n", "Where can I find data about online dating websites": "\nI find it unlikely that a dating website would share a dataset, although OKCupid Trends was one of the first good data blogs (and I'm glad they are back posting after being silent since 2011).\nThere was a Pew research study from 2013 - Online Dating\n\nThis data set contains questions about online dating, technology and existing relationships, and non-internet users.\n\n(requires entering some info to download data)\n\nSample: n=2,252 national adults, age 18 and older, including 1,127 cell phone interviews\nInterviewing dates: 04.17.2013 \u2013 05.19.2013\n\nThe survey is quite long and the data is available in crosstab, csv, or spss formats. Included are questions and answers about internet usage, where people meet each other, usage of dating websites and apps, demographic information.\n", "data request - Are there ID crosswalks between NPI and OSHPD for California Providers?": "\nI approached this issue by extracting the business address of the OSHPD and matching it to the NPI dataset. I only needed to do it for a handful of Clinics but I imagine the error rate would be relatively low and you would be able to fix/ignore whatever didn't match up, depending on your purpose of course.\nThe NPI Core Dataset is a little big for excel (2 GB). I attempted to use Tableau, it could manage with a lot of glitching. I would recommend working with these datasets in SQL to make this connection.\n", "data format - Most reusable way to publish a list as JSON": "\nYour sample data as well as the provided list of country codes are valid JavaScript, but they are not JSON. To make them valid JSON, put all strings in double quotes:\n[ \n  {\"name\": \"Afghanistan\", \"code\": \"AF\"}, \n  {\"name\": \"Albania\", \"code\": \"AL\"}\n]\n\nThis answer on StackOverflow gives a nice overview of the differences between JavaScript and JSON.\nRegarding your initial question, your sample data would look like this:\n[\n    {\"product1\": \"price1\"},\n    {\"product2\": \"price2\"},\n    {\"product3\": \"price3\"},\n    {\"product4\": \"price4\"}\n]\n\nIn most cases1 it's probably better to make your prices actual numbers instead of strings that just look like numbers:\n[\n    {\"product1\": 123.45},\n    {\"product2\": 234.56},\n    {\"product3\": 345.67},\n    {\"product4\": 456.78}\n]\n\nIf you think about adding more properties to your products in the future, you should probably use this instead:\n[\n    {\n        \"name\": \"product1\",\n        \"price\": 123.45\n    },\n    {\n        \"name\": \"product2\",\n        \"price\": 234.56\n    },\n    {\n        \"name\": \"product3\",\n        \"price\": 345.67\n    },\n    {\n        \"name\": \"product4\",\n        \"price\": 456.78\n    }\n]\n\n\n1 As Walter Tross pointed out in the comments, there are edge cases when rounding errors can occur. For example, if you have a product that costs 456789.99, it might happen that the computer program that reads your JSON will actually calculate the price as 456790.00. There is currently no simple solution for this problem. If you think this might be relevant for you, you might want to read up on some related discussions and the general problem of the precision of floating point numbers.\n", "No data dump and low daily-cap API: Can it be called Open Data?": "\nTo quote from the Open Definition v2.0:\n\n1.2 Access\nThe work shall be available as a whole and at no more than a reasonable one-time reproduction cost, preferably downloadable via the Internet without charge. [\u2026]\n\nSo no, what you describe is not Open Data according to the Open Definition.\n", "data request - Mapping counties to zip codes": "\nYou might want to check out the HUD zip code-county crosswalk (screenshot below).\n\nMy gut feeling is that this feature would be in the US census TIGER product line\nhttps://www.census.gov/geo/maps-data/data/tiger.html\nmaybe here:\nhttps://www.census.gov/geo/maps-data/data/relationship.html\nPlease let us know what you find most useful.\n", "data request - Longman English Dictionary Database": "\nThis particular dictionary's legal notice says that:\n\nUsers are not entitled to [...] transmit [..] it on any other website without the express permission of Pearson.\nYou must not use data mining, robots, scraping or similar data gathering or extraction methods on any part of this Site without our express prior written consent (my emphasis)\n\nSo either contact them, or use a more open dictionary.\nSee this list of open English dictionaries.\n", "Where I can get NMEA data dump of GLONASS or GPS simulator?": "\nYou can get NMEA logs from this website: FreeNMEA\nActually, it does not provide real logs (until you uploaded them), but you can generate \"synthetic\" (random) logs for testing. e.g. you can generate logs with GSV sentences only.\nUse this tool without registration: NMEA emitter for generating random logs.\nIf you need to generate logs for GPS/GLONASS you need to log into website and you'll be able to select Talker IDs for your logs.\n", "tool request - What forums / boards do you use to answer open data questions?": "\nThis one obviously. I also use http://reddit.com/r/opendata, and more recently the Open Knowledge Foundation's forums (http://discuss.okfn.org/) but that's for a project specific to them so I can't vouch for it as being good for general discussions or questions.\n", "geospatial - Data on forest cover (land use) in The Gambia from 2010 or more recent": "\nHave a look at the global Forest Change cover From Hansen et al. They used Landsat 7 images from 2000 to 2013 to assess forest cover changes. Tiles can be downloaded from their website under creative common license.\n", "data request - Free English Dictionary": "\nThe English Language & Usage stackexchange site has a question with answers related to your question\n\nWhat's the largest open-source dictionary that includes brief definitions of each word?\n\nSource\nThe popular answer is WordNet from Princeton. You can either browse or download the full data set, although it's about 10 years old.\nThe license allows commercial use.\n\nHere is an example of the page for Data:\nhttp://wordnetweb.princeton.edu/perl/webwn?s=data\n\nAnd the definitions\n\nS: (n) data, information (a collection of facts from which conclusions may be drawn) \"statistical data\"\nS: (n) datum, data point (an item of factual information derived from measurement or research)\n\n", "openfda - FDA Open Data sample URL for date range": "\nYou might want to try hurl.it for testing your calls: https://www.hurl.it/  it's really useful.\nI looked at the API reference:\nhttps://open.fda.gov/api/reference/\nwhich has directions, but all of the info there is geared towards drugs, not food.\nThis URL has food recall info:\nhttps://open.fda.gov/food/enforcement/\nThe examples are pretty good.  Does that page help, or did you have a more specific question?  Just remove the limit from the first example,\nhttps://api.fda.gov/food/enforcement.json?search=report_date:[20040101+TO+20131231]\nand that should be your call.  You may not need an API key if you are below the limit.  Otherwise, you would probably add it in:\nhttps://api.fda.gov/food/enforcement.json?api_key=yourAPIKeyHere&search=report_date:[20040101+TO+20131231]\n", "africa - Where can I find data on African higher education institutions?": "\nThe World Bank data is useful, but not comprehensive. You might also want to keep an eye on data posted to openAFRICA.net for examination & graduation results, as well as data about education facilities, resources, and staffing from some of the countries not currently covered by the World Bank datasets.\nThere seems to be ~218 education related datasets on openAFRICA. Here's a link: http://africaopendata.org/dataset?q=education\n", "transportation - Where can I find data about traffic flow for European cities?": "\nThe UK Department for Transport (DFT) provides a traffic data set for the city of London, for 2000 to 2013.\n\nMain page\n\nLondon\n\nAbout\n\nMetadata PDF\n\n\nThere are two main data sets:\n\nAADF - Annual average daily flow\n\n\nAADF figures give the number of vehicles that will drive on that stretch of road on an average day of the year. For information on how AADFs are calculated, see the guidance on the Traffic Statistics pages on GOV.UK.\nAADF figures are presented as: Units = vehicles per day\n\n\nTraffic - Annual volume of traffic\n\n\nTraffic figures give the total volume of traffic on the stretch of road for the whole year, and are calculated by multiplying the AADF by the corresponding length of road and by the number of days in the year (i.e. one vehicle travelling one mile each day for a year would equal 365 vehicle miles).\nTraffic figures are presented as: Units = thousand vehicle miles\n\n", "metadata - Custom Categories in CKAN": "\nThe filters on the search page sidebar can be configured implementing the IFacets interface from your own extension (you'll need to write some code for that).\nFor instance, these ones are defined like this:\nhttps://github.com/okfn/ckanext-tsbsatellites/blob/master/ckanext/tsbsatellites/plugin.py#L236:L249\n", "How can I add Wikidata properties of item a on the Wikipedia page of item b?": "\nAccording to v0.4 of the Wikidata inclusion syntax, this feature has not been implemented yet. The progress of this feature is available in the Wikimedia bug tracking system.\n", "data request - Graph of Landsat Downloads?": "\nApparently, here:\n\nSource: http://www.usgs.gov/blogs/features/files/2015/01/free-and-open-data-policy-graph-no-footnote.jpg\nHere is the original post, noting cost savings from Landsat. http://landsat.gsfc.nasa.gov/?p=9654\n", "releasing data - API for datapackages?": "\nCKAN apparently, although I haven't done this myself CKAN is a good solution: http://okfnlabs.org/blog/2014/09/11/data-api-for-data-packages-with-dpm-and-ckan.html \n", "data request - Extreme weather dataset for all countries from 2008 to 2011": "\nAddressing partly your question:\nThe NOAA has a Significant Earthquake database that can be downloaded in CSV format. The dataset also indicates if the earthquake event was linked to a tsunami.\n", "usa - federal guidelines for reporting expenses and donations": "\nThe first thing to consider is what data you have and what format is it in. \nWhatever it is, you can first make it available in a bulk download. Providing all the data and updating it on a schedule is a vast improvement that doesn't require a lot of overhead.\nThen, take stock of what kind of format the data is in. If you have the data in PDFs, focus on a way to get that information into a data base. If the records are being kept in excel, that is at least structured but you would want to upgrade to a database as well. \nIf the information is in a database, then making an API is not too complicated. I would suggest a json API. Making the field names clear and readable can go a long way. Having good documentation is very important- even the best schema is frustrating if there is poor documentation. \nThe level of detail that you can give is dependent on the underlying detail in your data. \n", "data request - Household income by ZIP+4": "\nFirst, note that ZIP codes are not geographies (see this other answer).\nAlso, as Kotebiya notes in a comment, there aren't pseudo-geographies for ZIP+4 and if there were, they would probably represent too small a group of people to allow data sharing without concern for individual privacy.\nThere may be commercial services that approximate this level of detail, but I am extremely doubtful that any public agency makes it available.\n", "programming - Open datasets for data mining R": "\nR itself has a datasets package. Check out the R Datasets Package\n\nDescription: Base R datasets\nDetails: This package contains a variety of datasets. For a complete list, use library(help = \"datasets\").\n\n(details)\n\nUpdate: Thanks to @PatrickHoefler for pointing out the full list of easily imported R datasets.\n", "data request - Query medicine descriptions API": "\nA few thoughts:\n\nFor a simple and down to the point description, Brian's suggestion of MedlinePlus' Web Services is a good start however if you do a search such as http://wsearch.nlm.nih.gov/ws/query?db=healthTopics&term=Levodopa you'll notice that it returns generic information about Parkinkson's disease and not this more appropriate page at http://www.nlm.nih.gov/medlineplus/druginfo/meds/a601068.html. I believe the reason for this is that the drug information at the link above is licensed information and is therefore not shown in the MedlinePlus API, but is shown on the website with the appropriate warnings.\nAs Brian also mentioned, DailyMed is an authoritative source, also from NLM which gets drug SPL (structured product labels) from the pharma industry and/or FDA; however this might return too much information for you -- see http://dailymed.nlm.nih.gov/dailymed/drugInfo.cfm?setid=6c1f7cd4-de56-45c1-a734-5e77b4aeb6f7, for the top result for \"LEVODOPA AND CARBIDOPA\", for example. You can get this data as XML and extract just the \"INDICATIONS AND USAGE\" information from the drug SPL, though in my experience they don't all have the same sections, which can make things quite complicated for on-the-fly querying.\nSimilar to bullet #2, similar SPL/\"labeling\" information is available from openFDA, a beta initiative by FDA at https://open.fda.gov. Sometimes, querying openFDA will be easier than DailyMed. For example, you can do this (https://api.fda.gov/drug/label.json?search=levodopa+AND+carbidopa) API call on openFDA and then look at the first result's indications_and_usage field which reads as follows:\n\nINDICATIONS & USAGE section Carbidopa and levodopa extended release tablets are indicated in the treatment of the symptoms of idiopathic Parkinson\u2019s disease (paralysis agitans), postencephalitic parkinsonism, and symptomatic parkinsonism which may follow injury to the nervous system by carbon monoxide intoxication and/or manganese intoxication.\n\nJust note that \"openFDA is a beta research project and not for clinical use\" and DailyMed might be able to be trusted more depending on your use case.\n\nSincerely hope this helps!\n", "licensing - Google translate english monolingual definitions": "\nIf you are using the Google Translate API then you are allowed commercial use. But there is no free quota for the API (terms of service, pricing). If you are a paying customer, you can use the translations for commercial use (with attribution if you use them directly).\nI suspect you are not talking about the paid API but instead either scraping yourself or using a package like goslate. These tools are against the Terms of Google so you may find that they aren't very stable (goslate works by faking a user-agent). Building a commercial app based on these data sources is probably not a good idea. See the comments here for more discussion.\n(if you want simple, single word translations with a flexible license, check out using Anki decks, for example)\n", "data request - Where can I find prices of tickets sold by airlines?": "\nThe US Bureau of Transportation Statistics provides various stats and datasets for historical airline ticket prices. \n\nGeneral overview, datasets and statistics\nAirfare data\n\nFrom the Route Fares page:\n\nRoute fares\n\nIf you go through the PDF report (2012 to Q1 2014), you can find the corresponding raw data (CSV/Excel) for each table.\n\n", "data request - Where can I find a dataset that represents the level of education or/and social class of people and their parents": "\nThis does not sound like panal data to me. It is a very classic problem in sociology and all that is required is to have parental education or class and the respondent's education or class. A simple survey like the General Social Survey (http://www3.norc.org/GSS+Website/) in the USA or the ALLBUS (http://www.gesis.org/en/allbus/allbus-home/) in Germany is sufficient, and a lot easier to work with. \n", "data request - Advertising Corpus for SVM": "\nAfter long hours of searching, I could not find one. The short answer was to crawl many different websites, separate the data, and use multi-nomial bayes or a radial basis function depending on how awful the results were to generate my own corpus via cluster/grouping with the manually classified data forming a kernel.\n", "Forward slashes in OpenFDA queries invariably lead to errors": "\nCarbocation couple things that may be of use to you. HEre is an open source site that allows you to searach in various ways. It might help you with what you are looking for - for example http://www.researchae.com/drugevent?from_date=2004-01-01&to_date=2015-01-31&from_age=&to_age=&search=&country=GLOBALLY&patientsex=&manufacturername=&drugbrandname=&druggenericname=&medicinalproduct=&reactionmeddrapt=&drugclass=Hepatitis+C+Virus+Protease+Inhibitor&drugindication=&indsubmit=&productndc=&safetyreportid=\nAlso the code is open source so the exact API calls are included in the app.rb file which you can see here - https://github.com/GeekNurse/ResearchAE-Open-Source/blob/master/app.rb\n", "data request - Datasets of Oil&Gas/electrical industry machinery for fault detection systems": "\nI've found an interesting project With tons of data available. It's a real data benchmark executed over an industrial valve. This is the website. \nIndustrial Actuator Real Data Benchmark Study.\n", "data request - Average gasoline price for all countries": "\nTry looking at Numbeo. It is a crowd source project on user contributed real-time data. The cost-of-living section has data on gasoline prices.\nhttp://www.numbeo.com/cost-of-living/\nCreative Commons Attribution-Sharealike 3.0 Unported License (CC-BY-SA) and GNU Free Documentation License (GFDL)\n", "data request - Wind energy vs. wind speed": "\nThe wind power production data will likely be proprietary because of energy trading (production companies don't want to freely announce how profitable they are.)\nThere is an online tool to calculate power production for all sorts of turbine types. You can choose a power curve related to a specific turbine type, or manually set one.\n\nSwiss Wind Power Data Website\n\n\n\nWith the power calculator you can estimate the power production for a site for different turbine types.\nA turbine availabiliy of 100% is assumed (no losses due to down time, icing, transformer losses, park effects etc.).\nNo guarantees can be given for the obtained results.\n\n\n\nAnother option would be to find research data for specific turbines, but that isn't real production data.\n", "openfda - I am using open FDA for the first time, How can I get API key?": "\nYou can query the openFDA API without a key, but for more requests, you can freely request one.\nThe screenshots below come from this Reference Page.\n\nTo get a key, you register with your email address on this screen\n\n", "education - Data sets for teaching": "\nIt's hard to know your exact audience, but there is a great blog post with (by now) more than 100 interesting data sets - 100+ Interesting Data Sets for Statistics\n\"Data science\" is pretty far-ranging, so you could do many different types of analysis:\n\nIT-related data mining - for example, Firefox browser data (alternative download site)\nImage processing, recognition, mining - for example, 10,000 annotated images of cats\nLanguage processing - for example, Google N-grams (downloadable raw data)\nText mining - for example, Enron Email dataset\n\n", "government - successful open data advocacy website": "\nPublic advocacy is effective when it shows decision makers the examples they'd like. But it's not always clear what they'd like. The best bet is to demonstrate them good examples widely used across other locations.\nHere are some of them:\nGitHub and Government\nThe most comprehensive collection of public software projects already implemented in the world.\nMany of these projects are data-driven, so you can understand what data adds value.\nData.gov Search\nYou can sort 130,000+ datasets by popularity and find out what're the most demanded data.\nFor example, this is the list of popular data related to transportation on the city level.\nData.gov Applications\nThe list of apps that use public data. Though it's difficult to estimate the impact of each particular app, you may ask your stakeholders what they like.\nOpen Knowledge Foundation (OKFN)\nA major organization closest to your requirements. It has branches in various countries, so you can see what many of them do.\n\nOKFN projects on GitHub. Very specific ones: you can use them as examples.\nOKFN Labs. Experimental stuff.\n\nOpenDataHandbook.org\nA few good examples from their value stories section:\nhttp://opendatahandbook.org/value-stories/en/\nReports by consulting companies\nMcKinsey has a very good reputation among top decision makers and its reports get right to the point: how much money the client can make out of the subject. In this case:\n\nMcKinsey report on open data\nMcKinsey report on big data\n\nThe World Bank also wrote on open data, but their report is a way too general.\n", "data request - Needed: Dataset for Outlier / Anomaly Detection": "\nTry looking at the data here !\nhttps://datamarket.com/data/list/?q=\nLooots of time series. I've found some nice anomaly sets in there. Specfically, time series.\n\nAre you a student ? If so, you can request Yahoo's outlier dataset !\n\nHave you checked out the datasets at quandl? They aren't made for outlier detection but you can definitely find anomalies if you just spend a little time looking.\n\nYou might also appreciate this project: Numenta Anomaly Benchmark (NAB)\nThis is taken directly from the readme in NAB/tree/master/data.\nNAB Data Corpus\nData are ordered, timestamped, single-valued metrics. All data files contain anomalies, unless otherwise noted.\nReal data\n\nrealAWSCloudwatch/\nAWS server metrics as collected by the AmazonCloudwatch service. Example metrics include CPU Utilization, Network Bytes In, and Disk Read Bytes.\nrealAdExchange/\nOnline advertisement clicking rates, where the metrics are cost-per-click (CPC) and cost per thousand impressions (CPM). One of the files is normal, without anomalies.\nrealKnownCause/\nThis is data for which we know the anomaly causes; no hand labeling.\n\nambient_temperature_system_failure.csv: The ambient temperature in an office\nsetting.\ncpu_utilization_asg_misconfiguration.csv: From Amazon Web Services (AWS)\nmonitoring CPU usage \u2013 i.e. average CPU usage across a given cluster. When\nusage is high, AWS spins up a new machine, and uses fewer machines when usage\nis low.\nec2_request_latency_system_failure.csv: CPU usage data from a server in\nAmazon's East Coast datacenter. The dataset ends with complete system failure\nresulting from a documented failure of AWS API servers. There's an interesting\nstory behind this data in the Numenta\nblog\nmachine_temperature_system_failure.csv: Temperature sensor data of an\ninternal component of a large, industrial mahcine. The first anomaly is a\nplanned shutdown of the machine. The second anomaly is difficult to detect and\ndirectly led to the third anomaly, a catastrophic failure of the machine.\nnyc_taxi.csv: Number of NYC taxi passengers, where the five anomalies occur\nduring the NYC marathon, Thanksgiving, Christmas, New Years day, and a snow\nstorm. The raw data is from the NYC Taxi and Limousine Commission.\nThe data file included here consists of aggregating the total number of\ntaxi passengers into 30 minute buckets.\nrogue_agent_key_hold.csv: Timing the key holds for several users of a\ncomputer, where the anomalies represent a change in the user.\nrogue_agent_key_updown.csv: Timing the key strokes for several users of a\ncomputer, where the anomalies represent a change in the user.\n\nrealRogueAgent/\nThis data represents computer usage patterns for different users, where an\nanomaly may occur with a rogue user of the computer.\nrealTraffic/\nReal time traffic data from the Twin Cities Metro area in Minnesota, collected\nby the\nMinnesota Department of Transportation.\nIncluded metrics include occupancy, speed, and travel time from specific\nsensors.\nrealTweets/\nA collection of Twitter mentions of large publicly-traded companies\nsuch as Google and IBM. The metric value represents the number of mentions\nfor a given ticker symbol every 5 minutes.\n\nArtificial data\n\nartificialNoAnomaly/\nArtifically-generated data without any anomalies.\nartificialWithAnomaly/\nArtifically-generated data with varying types of anomalies.\n\n", "What are the practical differences between WikiData's infrastructure and SemanticMediawiki?": "\nIt really depends on what you're trying to achieve.\nThis is what the Semantic Mediawiki project has to say on the relationship between Semantic MediaWiki and Wikidata:\n\nThe software that powers Wikidata is a set of MediaWiki extensions\n  collectively known as Wikibase, and though Wikibase has similarities\n  to Semantic MediaWiki, it is a distinct set of software. However, some\n  of SMW's backend code has been spun off into a separate library,\n  called \"DataValues\", that is used by both SMW and Wikibase as a\n  framework for storing data.\nThere is the potential that Wikibase and Semantic MediaWiki will\n  compete against one another as software, with some wikis choosing to\n  use Wikibase instead of SMW as their data storage system. This seems\n  doubtful, however: the Wikibase user interface is geared for a highly\n  multilingual, highly general knowledge base like Wikipedia. Wikis with\n  a specific focus and only one or a handful of languages would be\n  better off with the greater structure and simplicity of Semantic\n  MediaWiki.\n\nThey also have a dedicated page explaining some of the differences between SMW and Wikidata:\n\nThe main use case of Wikidata (a centralised, multi-lingual site that\n  serves as a data repository) is different from that of SMW (a\n  data-enhanced MediaWiki), and this leads to a number of differences.\nCentral to a wikidata statement is that its factual claim is supported\n  by reference(s) (source of the claim). For example, when SMW makes a\n  claim about the population of Berlin it would only be annotated with\n  Berlin has a population of 3,5 Mio where Wikidata would make an\n  extended statement describing it as Berlin's population being 3,5 Mio\n  as of 2011 according to the German statistical office.\n\n", "open definition - Data Classification": "\nThe state of Washington adopted a standard for data classification that has worked pretty well for us:\n (1) Category 1 - Public Information\nPublic information is information that can be or currently is released to the public.  It does not need protection from unauthorized disclosure, but does need integrity and availability protection controls.\n(2) Category 2 - Sensitive Information\nSensitive information may not be specifically protected from disclosure by law and is for official use only. Sensitive information is generally not released to the public unless specifically requested.\n(3) Category 3 - Confidential Information\nConfidential information is information that is specifically protected from disclosure by law.  It may include but is not limited to:\n    a. Personal information about individuals, regardless of how that information is obtained.\n    b. Information concerning employee personnel records.\n    c. Information regarding IT infrastructure and security of computer and telecommunications systems.\n(4) Category 4 - Confidential Information Requiring Special Handling\nConfidential information requiring special handling is information that is specifically protected from disclosure by law and for which:\n    a. Especially strict handling requirements are dictated, such as by statutes, regulations, or agreements.\n    b. Serious consequences could arise from unauthorized disclosure, such as threats to health and safety, or legal sanctions.\n\nhttps://ocio.wa.gov//policies/141-securing-information-technology-assets/14110-securing-information-technology-assets \n", "usa - H-1 (Work visa) data by year by state": "\nThe Department of Homeland Security has a table with breakdown by State, but not by Visa type. Probably you can find similar data sets for previous years.\n\nWebpage\nExcel file\n\n\nPersons Obtaining Lawful Permanent Resident Status by State or Territory of Residence and Region and Country of Birth: Fiscal Year 2013\n  (XLS, 175 KB)\n\nAlso from DHS is a breakdown by State over year, but without country of origin - Excel, 2008 data.\n\nIn general, you can search the data.gov portal for \"immigration\" and more search terms.\nhttp://catalog.data.gov/dataset?q=immigration\n\nDoing so returns my 359 data sets aggregated from multiple government sources. Scan through those results and perhaps you'll find exactly what you are looking for.\n", "Healthcare Finder PlanSummaryBenefitURL Error": "\nThis is an old question, but there was an issue with the service around that time. I can load the URL you provided and get back an SBC.\nIn Chrome on Android I do get a message asking me to set device security for certificate, but hitting cancel will still download the PDF.\n", "data request - Is there a way to extract names and other public information from Facebook?": "\nThe Facebook Graph-API contains some functionality that you can piece together to do what you asked.\nStep 1: Use the User-Friends endpoint to get a list of user-ids. You can use your own user-id as a starting point.\nStep 2: Use the User endpoint to get the following information of public profiles:\n\nProvides access to a subset of items that are part of a person's public profile. A person's public profile refers to the following properties on the user object by default:\nid, name, first_name, last_name,link,gender,locale,timezone,updated_time,verified\n\nStep 3: Use the user-ids that are in your target region to make additional requests for their friend's lists (step 1), which you can use to get more IDs, more locales, and on and on...\n", "data request - API or dataset with listings (property for sales) in an area?": "\nA couple of suggestions:\nZillow has an api, which can give you price data from postings and estimates of home values in different areas.\n3taps api allows you to search craigslist data, including real estate listings.\nHope that helps!\n", "Need a judicial data set": "\nAustralian laws and cases (all state and federal jurisdictions) are online at: austlii.edu.au.\nSee for instance High Court (the highest) 2014 cases at:\nhttp://www.austlii.edu.au/au/cases/cth/HCA/2014/\nFederal (we call them Commonwealth; Cth) laws starting with letter 'M': \nhttp://www.austlii.edu.au/au/legis/cth/consol_act/toc-M.html\netc etc etc... \n", "data request - Civilian Labor Force Demographics": "\nThis might suit your needs.\n\nSEX BY AGE BY EMPLOYMENT STATUS FOR THE POPULATION 16 YEARS AND OVER\n  Universe: Population 16 years and over  more information\n  2009-2013 American Community Survey 5-Year Estimates \n\nJust generate the proportions on your own.\n", "demographics - Looking for longitudinal population data by age and sex, any country": "\nYou could try a few of the following resources that I know of:\nThe U.S. Census Bureau's International Data Base\nThe U.N. Population Division also has a good resource, though it is available only in XLS format.\n", "data request - Where can I find the music lyrics dataset?": "\nSong lyrics are going to be mostly copyrighted, so distributing them as a dataset would probably not be legally possible.\nTo collect lyrics you can scrape one of the many lyrics websites. Here are some steps for how to do so with azlyrics.com\n\nFind a page that lists all artists: \n\nhttp://www.azlyrics.com/a.html\nNotice that artists with a number are listed under http://www.azlyrics.com/19.html\n\nFind URL to one artist:\n\nhttp://www.azlyrics.com/a/amywinehouse.html\n\nFind URL to one song:\n\nhttp://www.azlyrics.com/lyrics/amywinehouse/fuckmepumps.html\n\n\nOf course, you can't do this manually. But with some simple code, it's not hard to find URLs on a page, then go to those pages and find new URLs. Because you know the structure of the website, specifically where lists of artists are stored in a page called a.html, it's straightforward.\n\nHere's a more technical description. Each artist page has a section of HTML like this:\n<div class=\"artists fr\">\n\n<a href=\"a/anathema.html\">ANATHEMA</a><br />\n<a href=\"a/anberlin.html\">ANBERLIN</a><br />\n<a href=\"b/bonnieanderson.html\">ANDERSON, BONNIE</a><br />\n<a href=\"a/andersoneast.html\">ANDERSON EAST</a><br />\n<a href=\"j/johnanderson.html\">ANDERSON, JOHN</a><br />\n<a href=\"k/keithanderson.html\">ANDERSON, KEITH</a><br />\n<a href=\"a/andreabocelli.html\">ANDREA BOCELLI</a><br />\n<a href=\"a/andreacorr.html\">ANDREA CORR</a><br />\n<a href=\"a/andreasbourani.html\">ANDREAS BOURANI</a><br />\n...\n</div>\n\nEach song has HTML structure like this:\n<b>\"Fuck Me Pumps\"</b><br />\n<div style=\"margin-left:10px;margin-right:10px;\">\n<!-- start of lyrics -->\nWhen you walk in the bar,<br />\nAnd you dressed like a star,<br />\nRockin' your F me pumps.<br />\n\nThat <!-- start of lyrics --> will be a great help to scraping!\nHere is an outline of a python code to get your started:\nimport requests\n# step 1 - get artist pages\nfor page in ['abcdefghijklmnopqrstuvwxyz']: # also add '19'\n    url = 'http://www.azlyrics.com/' + page + '.html' # construct url of artists page\n    html = requests.get(url) # download a.html\n    # parse HTML for list of URLs to artists\n\n# step 2 - make unique list of URLs to artists\n\n# step 3 - loop over artist URLs to get list of URLs to songs\n# go to song HTML page and parse into text.\n\nFor parsing HTML, BeautifulSoup is commonly used.\nHere's a python scraping tutorial.\n", "Subquery in wikidata": "\nOk, I found out how to do this. You can use curly braces to further filter on your qualifiers. So the query to get all people that are Bundesminister and where the Bundesminister has a start but no end time is:\nCLAIM[39:248352]{CLAIM[580] AND NOCLAIM[582]}\n\ndemo link\nI found out by reading: http://magnusmanske.de/wordpress/?p=178\nAnd specifically by the example query for the winner of the Royal Medal in 1853.\n", "data request - I need a list of medications, diagnoses, and medical terms for a police report auto publication project": "\nThe NLM's UMLS Metathesaurus http://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/ is a possibility but, there is some non-trivial technical overhead to using it.  You can, however, get it at no cost.  You might check out FDA's NDC database file.  Perhaps that will help you.  http://www.fda.gov/Drugs/InformationOnDrugs/ucm142438.htm  You can also find a small list of drug ingredient names in the documentation for prescription medications collected by the National Health and Nutrition Examination Survey.  As eluded to above, your request is quite broad so it is difficult to give you more concrete options.\n", "openfda - Is there any better way to get sum number of reactions": "\nDon't use limit, use date boundaries instead:\nhttps://api.fda.gov/drug/event?&search=penicilin+AND+[2004-12-31+TO+2005-01-01]&count=patient.reaction.reactionmeddrapt.exact\nhttps://open.fda.gov/api/reference/#dates-and-ranges\n", "openfda - Any public accessible web service to get MedDRA description?": "\nPart of the difficulty is that Meddra is a licensed product.\nhttp://www.meddra.org/subscription/subscription-rate\nI am not aware of any place that offers the data online, since it is probably against the license agreement. At the top of the list is a minimal charge for non-commercial use, which might be a good option if you are just doing research.\n", "usa - Are there good examples of open read-write APIs in Federal government?": "\nThe Regulations.gov API answer is a good example! It's also an example of government dogfooding its own APIs. \nAs I am sure you are aware, the We the People petition site has a write API as well: http://www.whitehouse.gov/blog/2014/10/23/new-we-people-write-api-and-what-it-means-you\nFor more information on Federal Government APIs, see this Hub of Federal API information & check out the APIs listed on Data.gov.\n", "usa - Is the web performance of U.S. government websites open data?": "\nThe US Government's Digital Analytics Program (DAP) has been working on unified Web analytics for federal agencies for a couple of years\nRecently they launched a nice web site that displays the top data:\nhttps://analytics.usa.gov/ \nPhiladelphia's\nhttp://analytics.phila.gov/\n", "data request - Any available Travel Dataset": "\nThe University of Illinois has an archived dataset of tripadviser reviews for up thru 2008.\nhttp://times.cs.uiuc.edu/~wang296/Data/\nTripAdvisor also has an API. I've never used it so I don't know the terms of use:\nhttp://api-portal.anypoint.mulesoft.com/tripadvisor/api/tripadvisor-api\nThe UK government publishes annual travel surverys, starting from 2007. This data is at the region level, how many visitors, how many miles traveled, etc.\nhttp://data.gov.uk/dataset/national_travel_survey\n", "data request - how to get semantic ontology of words related to technology": "\nThe best place to start looking is Linked Open Vocabularies. As of February 2015 they have indexed more than 450 vocabularies and ontologies and provide a convenient search feature.\n", "Visitors to Data.gov by country of origin?": "\nThanks for filing the GitHub issue too Mitchell. \nFor background on this:\n\nVisitor stats by country and top states were retired with the new\n  data.gov theme launch about an year ago, earlier the process used to\n  be manual where we were taking the data from google analytics and\n  feeding the data to a visualization.\nIdeally we would need to integrate with google analytics api directly\n  and build on these visualization / metrics.\n\nWe're working on a sustainable way to display this information. For others on the Stack Exchange you can follow along here: https://github.com/GSA/data.gov/issues/570\n", "data.gov - Searching for obesity data at the ZIP or County level by age for US 2011-1013?": "\nThe BRFSS identifies respondents in most U.S. counties. However, the coverage is not exhaustive. But there exists small area estimations that usually use BRFSS data as the reference.\n", "data request - Public Website statistics": "\nu.s. city open data census tracks analytics offered by cities\nhttp://us-city.census.okfn.org/ \nEDIT:\nthe epa lists analytics of some sort, and has over a decades worth here:\nhttp://www.epa.gov/reports/objects/emfjulte/ \nEDIT 2:\nPennsylvania Spatial Data Access (PASDA) is PA's gis clearinghouse and offers great stats datasets covering almost a decade.  here's an example:\nhttp://www.pasda.psu.edu/about/publications/PASDAStatistics_20140101-20141231.pdf\nyou can snag them here:\nhttp://www.pasda.psu.edu/about/publications.asp \nEDIT 3:\nusgs landsat recently published some data regarding their data use; the post is more about the economic benefits of open data, however there's a really cool chart about data usage stats over time. hat tip to @dan for answering his question here:\nGraph of Landsat Downloads?\nyou can read the report and view the chart here:\nhttp://landsat.gsfc.nasa.gov/?p=9654 \nEDIT 4:\nA.C.I.S. SEARCH TOOL - Animal Care Information System Search Tool shares some of its visitors statistics, which were much higher than i thought they would be, based off the tools design and functionality. but there's roughly 14000 days of data here:\nvia:\nhttps://acissearch.aphis.usda.gov/LPASearch/faces/CustomerSearch.jspx;jsessionid=7f00000130d77a076f42228f4753ad1492029f4879f5.e38Obx8Sb3yQby0Qa3mQe0 \nEDIT 5:\nanalytics.usa.gov is a dashboard for analytics across dotgov sites:\nhttps://analytics.usa.gov/ \nPhiladelphia's site:\nhttp://analytics.phila.gov/\n", "data request - People of public interest living in Germany": "\nThere is a category called \"Deutscher\" (German) in the German Wikipedia that is accessible with the help of the German DBpedia and contains more than 191000 pages. You could query DBpedia for pages that have a link to this category with the wikiPageWikiLink relation and filter for living people by looking up if their DBpedia page contains a death date or not, using deathDate.  \n", "programming - Scraping columns from a PDF": "\nThe pdftotext library (man page) that comes standard with most linux distros and can be installed on Windows contains a -layout flag that preserves table structure.\npdftotext -layout input.pdf output.txt\n\nAfter that, you can easily parse with any language into your desired JSON structure.\nThere is a python wrapper for pdftotext, but as far as I know, it only works on linux. For my application on Windows, I used a system call to pdftotext.\n(There are also some python libraries for reading PDFs, but I found that pdftotext with the -layout option works best for multi-page PDF files with tables.)\n", "data request - Aerial crop image datasets": "\nI have a few set of aerial images taken by an X8+ Drone with a redEdge camera, maybe they will help you. Contact me. \n", "data request - Where can I find accurate US county boundaries?": "\nYou might find these cartographic boundary shapefiles for U.S. Counties useful: http://www.census.gov/geo/maps-data/data/cbf/cbf_counties.html\n", "programming - Where can software developers interested in open government data be reached?": "\ni'll tell you where i lurk:\ngoogle groups:\nopen gov jobs\nhttps://groups.google.com/forum/#!forum/opengovjobs\ncivic data alliance\nhttps://groups.google.com/forum/#!forum/civicdataalliance\nuk gov data developers\nhttps://groups.google.com/forum/#!forum/uk-government-data-developers\nlinked data api discuss\nhttps://groups.google.com/forum/#!forum/linked-data-api-discuss\ncode for northern virginia\nhttps://groups.google.com/forum/#!forum/nova-brigade\ngeo dc\nhttps://groups.google.com/forum/#!forum/geonerds-dc\ncode for dc\nhttps://groups.google.com/forum/#!forum/dc-cfa-brigade\ncode for fort lauderdale\nhttps://groups.google.com/forum/#!forum/code-for-ftl\nschool of open\nhttps://groups.google.com/forum/#!forum/school-of-open\nokfn task force\nhttps://groups.google.com/forum/#!forum/okfn-task-force\nopen data engagement\nhttps://groups.google.com/forum/#!forum/open-data-engagement\nglobal open data initiative\nhttps://groups.google.com/forum/#!forum/global-open-data-initiative\nopen civic data\nhttps://groups.google.com/forum/#!forum/open-civic-data\nus open gov\nhttps://groups.google.com/forum/#!forum/us-open-government\nfoia machine\nhttps://groups.google.com/forum/#!forum/foia-machine\nopen data day\nhttps://groups.google.com/forum/#!forum/open-data-day \nyour facebook group  \nreddit:\nhttp://www.reddit.com/r/opendata\nhttp://www.reddit.com/r/opendata_pt\nhttp://www.reddit.com/r/opengov\nthere's more on reddit if you feel like digging  \ncode for america lets authors post on their tumblr, you could submit a post there  \nmedium seems to be the cool kid blog of the day, you could double down and post it there as well.  \ncode for philly has a linkedin group  \nevery avenue sunlight foundation and okfn offer you: there's a slew of groups and mailing lists  \ncode for hampton roads and code for atlanta have mailing lists, i'm betting most brigades do...  \nnot to mention github.....man there's a ton of groups there. \ni know of a few flickr groups but they're most gov-related and you have to be in gov to join...so i couldn't join.\nEDIT:\ni also keep all of these twitter lists:\nhttps://twitter.com/jalbertbowdenii/lists/civhax\nhttps://twitter.com/jalbertbowdenii/lists/openglam\nhttps://twitter.com/jalbertbowdenii/lists/dotgov\nhttps://twitter.com/jalbertbowdenii/lists/open-acc-sci-edu-glam-kno\nhttps://twitter.com/jalbertbowdenii/lists/open-data-journo-crypto\nhttps://twitter.com/jalbertbowdenii/lists/open-science-stem\nhttps://twitter.com/jalbertbowdenii/lists/data \nnicar data journalists mailing list:\nhttp://www.ire.org/resource-center/listservs/subscribe-nicar-l/ \nknight-mozilla open news:\nhttp://opennews.org/ \n", "geospatial - Ordnance Survey BoundaryLine Data pre 2014": "\nMySociety.Org has been caching Ordinance Survey Boundary Line data since 2010:\nThis is the mySociety cache of OS OpenData, first released 1st April 2010, and other related similarly-licensed data, as allowed under the licences (please read!). Thanks to Ordnance Survey, data.gov.uk, ONS, and everyone else involved in releasing this data! :) \nhttp://parlvid.mysociety.org/os/\n", "geospatial - Data for water features by state/province": "\nThere are variety of sources both for the US and Worldwide. Here are a few that I am familiar with:\nhttp://nhd.usgs.gov/ (vector data)\nThe National Hydrography Dataset (NHD) and Watershed Boundary Dataset (WBD) are used to portray surface water on The National Map. The NHD represents the drainage network with features such as rivers, streams, canals, lakes, ponds, coastline, dams, and streamgages. The WBD represents drainage basins as enclosed areas in eight different size categories. Both datasets represent the real world at a nominal scale of 1:24,000-scale, which means that one inch of The National Map data equals 2,000 feet on the ground. To maintain mapping clarity not all water features are represented and those that are use a moderate level of detail. \nThe USGS geographic name dataset contains the names of geographic features in the United States. This includes both natural and manmade water features (e.g., canal, dam, beach, channel, lake)\nThe GNIS contains information about physical and cultural geographic features of all types in the United States, associated areas, and Antarctica, current and historical, but not including roads and highways. The database holds the Federally recognized name of each feature and defines the feature location by state, county, USGS topographic map, and geographic coordinates. Other attributes include names or spellings other than the official name, feature designations, feature classification, historical and descriptive information, and for some categories the geometric boundaries.\nhttp://geonames.usgs.gov/domestic/\nThe National Geospatial Intelligence Agency (NGA) provides the same data for the countries of the world.\nThe database is the official repository of foreign place-name decisions approved by the BGN. Geographic Area of Coverage: Worldwide excluding the United States and Antarctica. For names in the U.S. and Antarctica, please visit the United States Geological Survey (USGS) Geographic Names Information System (GNIS) web site.\nhttp://geonames.nga.mil/gns/html/index.html\nAt my website, I have both the USGS and NGA datasets converted to Linked CSV format:\nhttp://www.opengeocode.org/cude1.2/NGA/GNS/index.php\nhttp://www.opengeocode.org/cude1.2/USGS/GNIS/index.php\n", "data request - how to get drug database for free?": "\nIt's hard to say without some more specific information, but RxNorm is probably the best place to start:\n(2nd section is download/API access)\nhttp://www.nlm.nih.gov/research/umls/rxnorm/\n", "usa - Looking for a source of data for these specific United States Demographics for People and Businesses": "\nYou can get census data on population by single year on a county basis here:\nhttps://www.census.gov/popest/data/datasets.html\nI can't recall seeing it published down at the tract/block level (more of what the asker is asking) broken down by single year. You can find it in the 10 year increments as was noted in the question.\n", "data request - Obtaining personal mail corpus": "\nHere is a dataset of emails of a governor in Florida. It contains all email communications this governor made from 1999 to 2007. It contains about 280K emails. It was made public for political reasons I believe.\n", "geospatial - Download of big gis data": "\nHave a look at Geolife Trajectories dataset from MS Research. http://research.microsoft.com/en-us/downloads/b16d359d-d164-469e-9fd4-daa38f2b2e13/\nIts open and free as well.\n", "data request - How can I get aggregate dollar values out of Zillow?": "\nYou need to check the zillow terms of use to make sure your use is appropriate: \nhttp://www.zillow.com/wikipages/Privacy-and-Terms-of-Use/\nThis is not an open data set at this time.  However the API may satisfy your needs:\nhttp://www.zillow.com/howto/api/APIOverview.htm\n", "usa - School Quality Data at the County Level": "\nThe Department of Education maintains rating data on schools in a large number of categories. You can build your own tables here:\nhttp://eddataexpress.ed.gov/state-tables-main.cfm\nThe US Dept. of Ed also has related search tools here:\nhttp://nces.ed.gov/datatools/\nThe link below will allow you to very a variety of aggregated data at state, county, school district and school level. Note, only a minimal amount of data is aggregated at county level.\nhttp://nces.ed.gov/ccd/elsi/expressTables.aspx\nFor those that want to fully customize their results (datasets), use this tool:\nhttp://nces.ed.gov/ccd/elsi/tableGenerator.aspx\n", "data request - Dataset for temporal community detection": "\nMy thought would be to look at the bioinformatics world, specifically looking at metabolic networks in developmental biology or something like that.\nhttp://www.devbio.biology.gatech.edu/?page_id=11223\nThings like phage lambda are the simplest networks (see book, \"A Genetic Switch\"), then you can go to bacteria (E. coli), eukaryotes (yeast, S. cerivisiae), slime molds (dicty), then multicellular. There is a lot of network information out there but it may have some overhead.\nThis question may be very similar:\nExamples or datasets of evolving networks\n", "food - Where can I find data on eating behaviors/habits?": "\nThe UK has done a family food survey for a number of years. It breaks down food categories and percentage of consumption. Has national, region, urban, rural, by age, by income, etc.\nhttps://www.gov.uk/government/statistical-data-sets/family-food-datasets\nI've not review this USDA dataset, but the description seems to be what you are looking for:\nUSDA's National Household Food Acquisition and Purchase Survey (FoodAPS) is the first nationally representative survey of American households to collect unique and comprehensive data about household food purchases and acquisitions\nhttp://www.ers.usda.gov/data-products/foodaps-national-household-food-acquisition-and-purchase-survey.aspx\n", "geospatial - LiDAR data for Palawan (Philippines)": "\nThe Philippine government with the aid of the UN has been doing LiDAR mapping of the major river basins. Here's a press release:\nIt appears the effort is part of the Philippines DREAM project (for disaster risk/assessment):\nhttps://www.dream.upd.edu.ph/\nhttps://www.facebook.com/DREAMLIDAR\nThe University of the Philippines Los Banos has a page for LiDAR mapping of the Palawan province.\nhttp://phil-lidar.uplb.edu.ph/index.php/study-sites/palawan\nfyi> I see you posted the same question on GIS stackexchange.\n", "geospatial - Installing GeoJSON preview plugin on CKAN": "\nJust to be clear, you still need to do the steps on the \"Install the extension\" section to enable the plugin.\nAs an aside, we will move the geospatial previews to a separate CKAN extension to avoid this issue of having to install the ckanext-spatial requirements. I'll add a note here when it's done, or keep an eye on the ckan-dev mailing list.\n", "data request - Seeking global list of registered businesses": "\nProbably the closest thing you'll find to this is OpenCorporates. They claim to have data on over 60,000,000 companies.\nThey don't have a download, but there is an API. It has a mixed usage license depending on whether you are contributing open data back to the community.\n", "Historical flight track and aircraft type data": "\nGive this dataset a shot.  It's maybe a 7/10 for your needs?\nhttp://stat-computing.org/dataexpo/2009/\n\"The data consists of flight arrival and departure details for all commercial flights within the USA, from October 1987 to April 2008. This is a large dataset: there are nearly 120 million records in total, and takes up 1.6 gigabytes of space compressed and 12 gigabytes when uncompressed. To make sure that you're not overwhelmed by the size of the data, we've provide two brief introductions to some useful tools: linux command line tools and sqlite, a simple sql database.\"\nYou might find something here to join that dataset with to make what you're looking for:\nhttp://www.rita.dot.gov/bts/data_and_statistics/index.html\nand this might not be covered above:\nhttp://openflights.org/data.html\n", "analysis - 10 million user name and passwords. How do I analyze?": "\nWhat are your goals? You may need to upgrade your toolset.  Bash command line tools like cut, sort, uniq are very useful for this kind of thing.\nhttps://stackoverflow.com/questions/6712437/find-duplicate-lines-in-a-file-and-count-how-many-time-each-line-was-duplicated\nIf you want to analyze digit frequency, transitions between letters (n-grams), common variations in passwords, then a toolkit like R's natural language processing libraries will get you far:\nhttp://cran.r-project.org/web/views/NaturalLanguageProcessing.html\nIf you change the dataset around and look at variations like LLLNL (letter-letter-number-letter), you could probably gain great insight into the variation inherent in commonly used passwords.\n", "data request - Is a spoken digit dataset available?": "\nhttps://github.com/Jakobovski/free-spoken-digit-dataset is a free spoken digit dataset (FSDD).\nAs an added bonus it comes with a few useful python utility functions.\nI created this dataset because I had the same problem. Please contribute to increase the dataset's size.\n", "data.gov - American Indian reservation boundaries": "\nMy guess is that the census tiger products are going to have what you're looking for.\nhttp://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2016/TGRSHP2016_TechDoc_Ch2.pdf\n\"The legal entities included in these shapefiles are:\n\nAmerican Indian off-reservation trust lands\nAmerican Indian reservations (both federally and state-recognized)\nAmerican Indian tribal subdivisions (within legal American Indian areas) \n...\"\n\nMain tiger site:\nhttps://www.census.gov/geo/maps-data/data/tiger-line.html\n", "data request - Google search by file size": "\nI suspect this is not supported. There's no evidence of it in the \"advanced search\", nor in the guide to search operators (which advises against worrying about memorizing all of the operators because you can use the advanced search page.)\nAlso, the advanced search help page lists filters and doesn't include file size.\n", "data request - Largest US City Budgets": "\nI've not seen that data compiled yet into a single dataset. The Open Knowledge Foundation has a list of cities which have their budget data online:\nhttp://us-city.census.okfn.org/dataset/budget\n", "data request - Where can I find a geographical database?": "\nThe US National Geospatial Intelligence Agency (NGA) has what you are looking for all countries in the world. The datasets (one per country) have country, administrative level divisions (e.g., states/counties), incorporated and unincorporated places, both in native language/script and in English.\nhttps://www.nga.mil/ProductsServices/GeographicNames/Pages/default.aspx\nI also have converted all the country datasets into Linked CSV format:\nhttp://www.opengeocode.org/cude1.2/NGA/GNS/index.php\nThe United Nations Multilingual Terminology Database has administrative divisions/city names in the 6 UN languages. I don't think they have a direct dump, so you may need to scrap it:\nhttp://unterm.un.org/dgaacs/unterm.nsf/\nThe US/NGA also took over maintaining the GEC (Geopolitical Codes):\nhttp://earth-info.nga.mil/gns/html/gazetteers2.html\nHere's my archive of the updates in the original FIPS/XLS format:\nhttp://www.opengeocode.org/cude1.2/NGA/GEC/index.php\nhttp://www.opengeocode.com/\nPer Deer Hunter's comments, here are links to the US Census Gazetteer files:\nhttps://www.census.gov/geo/maps-data/data/gazetteer.html\nHere's my version converted to linked CSV:\nhttp://www.opengeocode.org/cude1.2/US%20Census/index.php\nThere is also the USGS Geographic Name Information System (GNIS) database for domestic and Antarctic names:\nhttp://geonames.usgs.gov/domestic/download_data.htm\nHere's my version converted to linked CSV:\nhttp://www.opengeocode.org/cude1.2/USGS/GNIS/index.php\nThe Canadian (2011) census would also have geographic name information. Here's my version converted to linked CSV:\nhttp://www.opengeocode.org/cude1.2/Canada/Census/index.php\n", "data request - Police Misconduct dataset?": "\nUnfortunately, from PoliceMisconduct.net's own FAQ:\n\nWhy do this?\nSimply because nobody else does. Only a small fraction of the 17,000 law enforcement agencies actually track their own misconduct in a semi-public manner, and even when they do, the data they provide is generic and does not specify what misconduct occurred, who did it, and what the end result was.\n\nThis FiveThirtyEight.com article by Reuben Fischer-Baum goes into more detail about the scarcity of this data:\n\nAs is the case with police shooting statistics, comprehensive numbers on accusations of police misconduct are hard to come by. There is no national reporting requirement for such accusations; in fact, many places have laws to purposefully keep the details of misconduct investigations out of the public eye.\n\nFor the article, Fischer-Baum settles for the NPMRP/PoliceMisconduct.net data.\n", "Will the Sunlight's TransparencyData state campaign finance contributions every be updated again?": "\nWith Labs no longer in existence, the answer is no, Influence Explorer will never be updated.\nHowever, the ProPublica's FEC Itemizer comes highly recommended as the best source for most of the data that lived in Influence Explorer. \n", "data request - car parking Dataset in US": "\nIf you're not looking for real time data, you might want to search on the term 'parking study', which tends to look at the availability of parking in various areas.  \nI would assume that cities that have gone to dynamic pricing aka 'active parking management' would need to be collecting the data to set their pricing ... so Seattle and San Francisco would be prime candidates ... unless you're looking for more typical parking data.\n", "data request - Free reusable aerial photography of the whole world": "\nSounds like a case forOpenAerialMap, which after a hiatus, looks like it might get a reboot. Wish I could think of something that was active. They already have a functional beta:\nOpenAerialMap\nThe OSM wiki also has some notes on aerial imagery.\n", "openfda - Big Data Neophyte Trying To Work With FDA Data -- Any kind souls out there willing to help?": "\nI would suggest filing a FOIA request with FDA for the complete dataset.\nBarring that, web scraping is a common strategy for extracting data from web sites. It might be a stretch for a \"neophyte,\" but it's something you'll find a ton of guidance on from a Google search. (Perhaps too much, but I think getting into the details of scraping are out-of-scope for this site, and definitely for this question.)  \nNote that a simple Google search for how to scrape web pages includes a number of hits which talk about how to do it without writing your own code.\n", "asia - Demographic data for asian countries": "\nif you want something comparable to the united states decennial census, then you want\nhttps://international.ipums.org/international/\n", "data request - How do I find out if a given movie is available in Germany?": "\nThe IMDB data dump contains theater release dates (warning, 46MB file):\nftp://ftp.fu-berlin.de/pub/misc/movies/database/release-dates.list.gz\n\nAn example:\n\nUnfortunately this is only cinema releases, and won't include TV showings or DVD releases. To get TV releases, the data source will probably be TV-specific.\n", "economics - Open data set for fair trade commodities in developing countries (any: bananas, sugar, wine/grapes)": "\nYou can find commodity trade information (by category) between countries in the UN ComTrade database\nhttp://comtrade.un.org/\nThe United Nations Commodity Trade Statistics Database (UN Comtrade) stores more than 1 billion trade data records from 1962. Over 140 reporter countries provide the United Nations Statistics Division with their annual international trade statistics detailed by commodities and partner countries. These data are subsequently transformed into the United Nations Statistics Division standard format with consistent coding and valuation using the UN/OECD CoprA internal processing system.\nThe WTO also has datasets on trade between countries:\nhttp://stat.wto.org/StatisticalProgram/WSDBStatProgramTechNotes.aspx?Language=E\nThe Food Agriculture Organization (FAO) of United Nations' statistical database (FAOSTAT) contains price data on wheat, tea, cereals, rice and sugar per country going back to 2002.\nUPDATE: I answered a related question awhile back ago on datasets about commodities.\nWhere can I get historic prices for a commodity?\n", "data request - where in the uk may i find a register or map of properties and their energy use": "\nThis is not a direct answer, but you could try to extrapolate the data using openstreetmap. As far as I know, the data contains also information about the usage of the building (housing, industrial). Based on the size and the usage you could estimate the energy use of the building. How it is done for the heat demand is described e.g. in this IEEE paper.\n", "api - Any uses of Hydra Core Vocabulary?": "\nIf you mean Hydra-based Web APIs that closely resemble current JSON-based APIs, than I guess the answer is that no \"significant site or project\" has published anything production ready yet. Quite of a few people and a handful of startups mentioned that they are working on it. If, on the other hand, you include Triple Pattern Fragment servers, which are based on the Hydra Core vocabulary, then there are actually quite a few significant projects that serve their data using Hydra. There's even a post claiming there are more than 600,000 of them :-) You can also find a short list of prominent projects on http://linkeddatafragments.org/data/.\nDisclaimer: I'm the chair of the Hydra W3C Community Group and one of the core designers of JSON-LD.\n", "data request - Mapping Business Hours": "\nThis CSV dataset contains opening-closing hours for thousands of US hotels/restaurants/bars/museums/etc:\nhttps://github.com/baturin/wikivoyage-listings/\nBusiness type, name and location are included as well.\nYou will need to filter out:\n\nBusinesses that have no documented opening-closing hours\nBusinesses that are not in the USA (there are columns for city name and latitude/longitude, you might use that)\n\nData license: Creative Commons Attribution-ShareAlike 3.0\n", "data.gov - Trying to find definitions for \"Location\" in Farmers Market datasets": "\nBelow is an excerpt from Establishing Land Use Protections for Farmers' Markets, published by Public Health Law and Policy and funded by the California Department of Public Health. It is an old publication. It doesn't give definitions, but it does provide some examples relevant to your question.\n\nIdentify potential farmers\u2019 market sites on public property, including parks, schools, colleges and universities, and other institutions; on private property, including hospitals and commercial centers; and, where feasible, on streets using street closures.\n\nI believe \"other institutions\" would include the grounds of government buildings.  You might want to take a look at your state's legislative resources page for more information.\n", "research - Ebbinghaus 1885 memory experiment data from credible source": "\nGiven the amount of time, the original data are probably no longer extant. But there is some hope, because the inheritance of Hermann Ebbinghaus was donated to the Adolf-W\u00fcrth Zentrum in W\u00fcrzburg, see\nhttp://www.awz.uni-wuerzburg.de/en/news/news/single/artikel/schenkungs/\nNote that you need some skills to work with this: German language, reading Old German Handwriting, maybe more ... if the desired data are part of the inheritance at all.\n", "data request - Daily electricity usage dataset": "\nThe European countries in the Entsoe-E provide hourly data aggregated on the country level.\nPlease see this answer for more details.\n\nEuropean Network of Transmission System Operators for Electricity (ENTSOE) provides Consumption and Production data for individual countries (and Exchange data between countries). (Link to Data Portal.)\nLink to ENTSOE Consumption data\n\n\n", "data request - SQL queries dataset": "\nNot exactly SQL but SPARQL:\nNL-SPARQL: A Dialog-System Challenge Set for Converting Natural Language to Structured Queries\n\nNL-SPARQL is a data set of natural language (NL) utterances to a conversational system\u00a0in the movies domain and corresponding queries to Freebase in SPARQL. This dataset was collected via Crowdsourcing as described below.\nThe data set is split into two sets: training (3,338 examples)\u00a0and test\u00a0(1,084 examples) set NL utterance and SPARQL query pairs.\n\nTwo examples:\n\nNL utterance:\nHow many movies has Alfred Hitchcock directed?\nSPARQL Query:\n\u00a0SELECT (COUNT(?movie) AS ?count) WHERE {?movie\nhttp://rdf.freebase.com/ns/type.object.type\nhttp://rdf.freebase.com/ns/film.film. ?movie\nhttp://rdf.freebase.com/ns/film.film.directed_by ?director.\n?director http://rdf.freebase.com/ns/type.object.name \"Alfred\nHitchcock\"@en.}\nNL utterance:\nShow me movies in Chinese.\nSPARQL Query:\nSELECT ?movie WHERE { ?movie\nhttp://rdf.freebase.com/ns/type.object.type\nhttp://rdf.freebase.com/ns/film.film. ?movie\nhttp://rdf.freebase.com/ns/film.film.language ?lang. ?lang\nhttp://rdf.freebase.com/ns/type.object.name \"Chinese language\"@en.}\n\n", "data request - Dataset of entry/exit times of vehicles in/out of a parking lot": "\nIf the lot of interest is a paid lot, then you might be able to get this information from the owner.\nInformation from a government run lot might be possible with a Fredom of Information Act, FOIA, request. \n", "Posting about data sets - Open Data Meta Stack Exchange": "\nGo ahead and post a \"question\", and then answer it yourself. For example, \"Where can I find outbreak data... related to ... ?\"  \nHere is a recent question that was self-answered: Graph of Landsat Downloads\nThe meta-stackexchange site has an interesting discussion.\n", "data request - swimming velocity for whales or fishes": "\nThere are a number of research papers/study on this subject.\nThis 1991 paper covers a lot of historical as well as data current at the time.\nFish swimming stride by stride: speed limits and endurance, Videller, 1991\nhttp://www.researchgate.net/profile/John_Videler/publication/226033193_Fish_swimming_stride_by_stride_speed_limits_and_endurance/links/0fcfd506672ef8c64f000000.pdf\nHere's a 2012 paper on sharks in Greenland\nThe slowest fish: Swim speed and tail-beat frequency of Greenland sharks, YY Watanabe, 2012\nhttp://www.researchgate.net/profile/Christian_Lydersen/publication/258980090_The_slowest_fish_Swim_speed_and_tail-beat_frequency_of_Greenland_sharks/links/0deec52988c6c8cd91000000.pdf\nHere's another paper with data, circa 1990\nhttp://pubs.iclarm.net/Naga/FB_1363.pdf\nAnd here's another paper source:\nMorphological predictors of swimming speed: a case study of pre-settlement juvenile coral reef fishes, R Fisher, \u200e2007 \nhttp://jeb.biologists.org/content/210/14/2436.long\n", "data request - open database photos of human faces with age": "\nYou'll have to go through the list yourself to find the dataset that includes age of the face, or other demographics.\n\n\nThere is a long list of face databases at the Face Recognition Home Page. Searching for \" age \" on the page.\n\n\nWhen benchmarking an algorithm it is recommendable to use a standard test data set for researchers to be able to directly compare the results. While there are many databases in use currently, the choice of an appropriate database to be used should be made based on the task given (aging, expressions, lighting etc). Another way is to choose the data set specific to the property to be tested (e.g. how algorithm behaves when given images with lighting changes or images with different facial expressions). If, on the other hand, an algorithm needs to be trained with more images per class (like LDA), Yale face database is probably more appropriate than FERET.\n\n\n\nResults of searching \"facial recognition\" on datahub.io - LINK\n\n", "social media - How do people link twitter handles to e-mail accounts": "\nDepending upon a Twitter user's Twitter configuration you can locate their Twitter profile searching with their email address or telephone number. See more here: https://support.twitter.com/articles/20170001.\nI'm unaware of a quick way to locate an email address with ONLY a Twitter ID.\nI suppose someone with a list of valid email addresses could bang away at Twitter and build their own Db of email addresses to Twitter IDs.\nIf there is a match on an email I think it would be moderately to very reliable. The absence of a match, however, would very unreliably indicate the absence of a corresponding Twitter profile.\n", "weather - How can I get temperature data for each Country (Annual)": "\nThis is very coarse data from the worldbank. It shows historical average temperature per country:\nhttp://data.worldbank.org/data-catalog/cckp_historical_data\nUsing their Climate API you can get slightly more detailed information and by per year. I believe they have data going back to 1961.\nhttp://data.worldbank.org/developers/climate-data-api\nBerkeley Earth provides aggregated data on world temperature averages going back to 1750, and country/city data going back to 1960.\nhttp://berkeleyearth.org/data\nThen there is NASA's global temperatures database collected from 500 stations, some going back to 1720. I haven't used this tool yet, so I'm not sure how to navigate it.\nhttp://gcmd.gsfc.nasa.gov/KeywordSearch/Metadata.do?Portal=GCMD&KeywordPath=&EntryId=Rimfrost&MetadataView=Data&MetadataType=0&lbnode=mdlb3\nThen there is NOAA. It's datasets, while primarily US, includes datasets for data collecting from weather stations around the world:\nhttp://www.ncdc.noaa.gov/data-access\n", "data request - Open, big time-series dataset (ideally web traffic)": "\nI publish the Google Play Store statistics of one of my apps:\nhttp://datahub.io/dataset/google-play-statistics/\nSee for instance the installs per device type. It is fragmented by day, with metrics such as installs/uninstalls/upgrades.\nIt is less than 100K rows, though, as data is pre-aggregated.\n", "language - Data set of news articles and scientific journals": "\nAs far as I know, one of the best data sources for NLP has been the ENRON emails. While this is not a \"news\" source, it would certainly provide some raw data for you to play around with.\nAnother source is PubMed. While this doesn't provide you with the actual article information, it does provide you with a lot of abstracts for scientific writing. There have even been papers published in PubMed about using this topic.\nFinally, there's good old Wikipedia, which allows you to download their entire archived content.\nHope this helps!\n", "programming - Public web API to get list of apps in App Store?": "\nThere is an unofficial API for the Google Play Android Market: android-market-api\n\n\nYou can browse market with any carrier or locale you want. \nSearch for apps using keywords or package name. \nRetrieve an app info using an app ID. \nRetrieve comments using an app ID. \nGet PNG screenshots and icon \n\n\nCommand line usage:\njava -jar androidmarketapi-X.Y.jar myemail mypassword myquery \n\nMy source. Check out the other answers, too.\n", "API for in-season food info based on location": "\nIt doesn't appear to be offered as data, but it would be exceedingly easy to scrape this Eat Local resource from NRDC. There's a page for each state which lists foods by month.\nFor each state they have \"learn more\" links, which probably also lead to their data sources (few of which appear to be structured data.)\n", "data request - ATM locations in UK and/or Ireland": "\nWhile the data is only as good as the community contributes, OpenStreetMap (OSM) has some ATM data.\nIn OSM, points can be tagged with \"amenity=atm\". You can extract these nodes using these steps:\n\nDownload OSM data. This page has a \"British isles\" extract, among others. This page explains more generally how this kind of data is packaged.\nDownload and extract the osmosis command-line java tool\nRun a command such as ./osmosis --read-pbf british-isles-latest.osm.pbf --tf accept-nodes amenity=atm --write-xml british-atms.osm\n\nOsmosis also supports writing to databases instead of XML.\nThe details available for each node vary widely, but then, you get what you pay for, and as you noted, there are a number of commercial sources for this data. \n", "economics - Looking for data on 3D Printer sales trends worldwide": "\nThese data are compiled in the annual Wohlers Report, which is US $495.\nSee https://wohlersassociates.com/2016report.htm.\nThere is no free source for these sales figures if you need a comprehensive dataset. I would call or email Wohler's Associates and ask if there is a research/academic edition available; it's a small company and friendly to work with.\n", "What needs to be done to be 4 or 5-star open data?": "\nThe current situation of your data\n\nData tables in PDF (example from your forestry statistics page) are a great example of 1-star data: The data is on the web, but it's pretty much unusable for computer programs without some heavy-duty data extraction. After all, PDF is where data goes to die. My advice: Just don't do it.\nYour example Excel file is actually a good start. It's 2-star data, since it's still in a proprietary file format, but it's structured and machine-readable. It could easily be turned into 3-star data (\"Save as CSV\"). In other words, the difference between 2-star and 3-star data is mainly ideological and much less technical.\n\nGetting to 3-star data is not a problem. Turning it into 4-star and 5-star data is where things start to get interesting.\nThe structure of your example data\n\nYour data has three dimensions: County (expressed as number, code, and abbreviation), land use class, and year.\nFor each combination of these three dimensions, there is one data value, which is the area in hectare.\n\nExpressing two dimensions in tabular form \u2014 such as an Excel spreadsheet \u2014 is trivial: On dimension (e.g. county) goes on the vertical axis, the other dimension (e.g. land use class) on the horizontal axis. However once the dimensionality is larger than 2, it becomes tricky to represent the data in two dimensions (e.g. in a table on a computer screen).\nThere are standard solutions to deal with multi-dimensional data, such das Pivot Tables and OLAP Cubes. In the context of Linked Data, the RDF Data Cube Vocabulary is the current best practice for expressing multi-dimensional data.\nWhat needs to be done to move to 4- or 5-star data?\n4-star data means that you \"use open standards from W3C\" (such as RDF) and/or \"use URIs to denote things\" so that people can point at your stuff. 5-star data means that you link your data to other people's data.\nWhat does that mean in case of your statistical data? Well, basically it means that you would have to turn your statistical data into RDF Data Cubes. With the current state of tool support, this still requires a lot of effort and expertise. While RDF Data Cubes are a great thing in theory, so far they are not really relevant in practice because, outside of the Semantic Web research community, not that many people use this technology.\nOther things you could do\nIf you do not want to go for RDF Data Cubes \u2014 and unless you have strong reasons in favor, I would not recommend it \u2014 there are other things you could do to make your data more machine-readable:\nPublish your data as (compressed) flat CSV tables\nThis makes it trivial to\n\nimport the data into a spreadsheet application and create a pivot table\nimport the data into OpenRefine and perform advanced operations\nturn the data into RDF Data Cubes, if someone wants to do it and already has the necessary skills\n\nYou could even add another column containing URIs representing the counties (e.g. https://www.wikidata.org/entity/Q104926 for Uppsala County). This would fulfil the requirements for 5-star data (although we skipped the requirements for 4-star data).\nKeep an eye on the Metadata Vocabulary for Tabular Data\nSo far it's only a W3C Working Draft, but it sounds promising:\n\nValidation, conversion, display and search of tabular data on the web requires additional metadata that describes how the data should be interpreted. This document defines a vocabulary for metadata that annotates tabular data. This can be used to provide metadata at various levels, from collections of data and how they relate to each other down to individual cells within a table.\n\n", "geospatial - uk postcode outcode border data": "\nTake a look at this site:\nUK Postcodes\nIt has the CSV data for all the boundaries. You can also download a KML file at each level to display in Google Earth, or QGIS where you can do more analysis. \nUnfortunately you may need to individually download each Area to get detail for the next level down. For example, select CH from the list and then click 'Download the individual postcode data in KMl format' to get data of the next level down (CH1, CH2 etc).\n", "data request - Looking for large multidimensional datasets": "\nYou can search through the UCI Machine Learning Repository by the # of Attributes.\nFor example, the Arrhythmia dataset has 279 attributes.\n", "data request - Datasets in which people make probability estimates": "\nthe health and retirement study asks what's the probability you think you'll be in a nursing home in the next few years.\nhttp://www.asdfree.com/search/label/health%20and%20retirement%20study%20%28hrs%29\nyou would also be able to calculate how many of these probability guesses came true.  hrs is a longitudinal survey, so they follow americans aged 50+ until they die - they also follow people if they move into a nursing home.  you could, for example, you could use the \"Probability to live 75+\" question at the point in the panel when each respondent turns 65 to calculate what share made it vs what share said they would.  the link i provided has usage examples.  look at the RAND codebooks for \"reported probability\" and be careful about how you're using the survey weights.\n", "data request - Is there an API or global database for sports events that is used by famous sports websites?": "\nYou will find that most sports data is collected by commercial concerns who control licensing rather carefully, especially for current data. (See, for example, this from Sports-Reference.com about why they can't provide their bulk data.)\nMost references to sports data that I've found (e.g. DatabaseSports.com) provide web access but not bulk downloads. Other resources frequently seem out of date. (But try googling for \"sports data,\" you'll find lots of leads to follow beyond what I list here.)\nSean Lahman's index of sports data looks pretty carefully maintained and oriented towards bulk data. There's a book, Analyzing Baseball Data with R, which uses Lahman's baseball data as a source.\nTableau Public has a page which collects links to sports data sources for over a dozen sports but a little clicking doesn't show any that have bulk download. (You're advised to check further for the sports which interest you.)\nThere is a book, Sports Data Mining, which has an entire chapter devoted to data sources. The book is expensive, and was published in 2010 so it's data references may be obsolete\u2026 but perhaps you can find it in a library.\n", "data request - Missing private sector credit in IFS for Euro-area countries 1998--1999": "\nI'm pretty sure it's too late to help you, but maybe others will face the same problem. For 1998 you can find data in the IMFs \"International Financial Statistics Yearbook(s)\" (if you are at a university your library might have them), but even there data for 1999 is missing. I'm not sure yet why this is the case, but I try to find out, since I want to know too. I think in the end the only possibility to fill these gaps will be interpolation.\nIt's also a bit strange that the gaps in the online IFS data are longer than those in the (printed) IFS Yearbook(s). Unfortunately there are no hints in the yearbooks why those gaps exist. There are notes for many countries, but they only explain that the establishment of the euro area changed the reporting, no word on the gap in 1999.\n", "data request - Weather Web Service by Postal Code?": "\nThe Weather API from Wunderground allows 500 free requests per day (with registration) and has a geolookup endpoint for finding the nearest weather station.\n\nOnce you have the nearest weather station, you can use other endpoints like forecast and history.\nThe full list of endpoints is here.\n", "data request - Database of Greyhound bus stops": "\ni think this is a good start, but i can't find who authored it. i would take the kml from this map and upload it to geojson.io where you can fiddle and convert it more.\nhttps://www.google.com/maps/d/viewer?msa=0&mid=zzToj9iDb7Q0.kZhGp83_3Et8 \nedit:\nthe wayback machine has what you want albeit a year old. i only checked texas, so maybe i'm wrong. go here and then swap out state two letter digits.\nhttp://web.archive.org/web/20131212034714/http://www.greyhound.com/en/locations/locations.aspx?state=tx \ncanada locations by province:\nhttp://web.archive.org/web/20130817065113/http://greyhound.ca/en/locations/states.aspx \nalso there's this but you'll have to filter out mexico:\nhttp://www.routefriend.com/stations/greyhound\n", "data format - HIOS Plan Year 2016 XSD Schema": "\nThese files are now posted on the SERFF site: 2016 QHP Templates.\n", "HealthCare Finder API Links Broken?": "\nYes this looks like a bug on the site. The APIs themselves seem to be working fine, but the links to the documentation are broken. If you are simply interested in the XSD then you should be able to download it from https://finder.healthcare.gov/api/finder_api_v3.0.xsd\nIf you are looking for a working example, try the following curl request: \ncurl 'https://api.finder.healthcare.gov/v3.0/getIFPPlanBenefits' -H 'Content-Type: application/xml' --data-binary $'<?xml version=\"1.0\" encoding=\"UTF-8\"?> \\n<p:PlanBenefitRequest xmlns:p=\"http://hios.cms.org/api\" xmlns:p1=\"http://hios.cms.org/api-types\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://hios.cms.org/api hios-api-11.0.xsd \"> \\n  <p:Enrollees>\\n    <p1:DateOfBirth>1990-01-01</p1:DateOfBirth>\\n    <p1:Gender>Male</p1:Gender>\\n    \\n    <p1:Relation>SELF</p1:Relation>\\n    <p1:InHouseholdIndicator>true</p1:InHouseholdIndicator>\\n  </p:Enrollees>\\n  <p:Location>\\n    <p1:ZipCode>48001</p1:ZipCode>\\n     <p1:County>\\n         <p1:FipsCode>26147</p1:FipsCode>\\n         <p1:CountyName>SAINT CLAIR</p1:CountyName>\\n         <p1:StateCode>MI</p1:StateCode>\\n      </p1:County>\\n  </p:Location>\\n  <p:InsuranceEffectiveDate>2015-01-01</p:InsuranceEffectiveDate>\\n  <p:Market>Individual</p:Market>\\n  <p:PlanIds>\\n      <p:PlanId>59830MI0010009</p:PlanId>\\n</p:PlanIds>\\n   </p:PlanBenefitRequest>' --compressed\n\n", "data request - finding online finance datasets": "\nHere's a link to datasets (Excel spreadsheets) from a NYU professor whom has been keeping corporate finance data on major corporations in US, Canada, UK and Australia for 20 years.\nhttp://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html\n", "usa - Where to get older digital OCR'd data sets of unsummarized US Census data?": "\nyou're talking about historical public use microdata, right?  you want\nhttps://usa.ipums.org/usa/sampdesc.shtml\n", "usa - Average height of males and females for each of the 50 US states: Where can I download the data?": "\nThe BRFSS would be a good data set to use for this question. Download the data here.\nFor example, for the 2013 data, I recommend getting acquainted with the codebook and then downloading the raw data. It will be an ASCII fixed column width raw file, so refer to the column positions document for transferring to the analysis package of your choice.\n", "data request - Open database of ingredient names?": "\nI think USDA.gov's NDB (full name: USDA National Nutrient Database for Standard Reference) would get you what you need.\nDownload links and data metadata is available on the \"About the Database\" file at http://www.ars.usda.gov/Services/docs.htm?docid=8964. If you're looking solely for a list of food items, the latest Food Descriptions file is at http://www.ars.usda.gov/SP2UserFiles/Place/80400525/Data/SR27/asc/FOOD_DES.txt\nThere are many sites such as nutritionix.com, etc. which have APIs and have added additional commercial off the shelf ingredients like packaged goods, etc. too though so you might look into a more refined/complete collection as well. See http://www.nutritionix.com/api for information about their API.\n", "data request - How would I find on iMDb, all male actors with movies debuts between ages 20 and 30?": "\nHave you taken a look at http://www.imdb.com/interfaces which includes a link to FTP mirrors where you can get data files that can presumably be joined in a RDBMS or other type of database system. Files you probably want to investigate are actors.list.gz, movies.list.gz, and release-dates.list.gz\nHope this helps!\n", "data request - Open database of filename extensions": "\nIn today's modern world we think less about the file extensions (suffix) and classify files by their 'Media Type' (formerly MIME type). IANA is the official repository for registering media types. The most complete list ( as of March 5, 2015) is here:\nhttp://www.iana.org/assignments/media-types/media-types.xhtml\nThis third-party page which lists MIME types with their typical extensions and has links to more information for each one. If there are media types not listed on that page, you can visit the links in the \"template\" column from the IANA registration page and find file extension information.\n", "data request - Novel Blurb Corpus": "\nBlurbs, or short descriptive material to promote a book, may be impossible to legally share do to individual copyrights of the authors or publishers.\nThe Goodreads API has many endpoints, and they include this note:\n\nBook cover images, descriptions, and other data from third party sources might be excluded, because we do not have a license to distribute these data via our API. \n\nIn contrast, the book metadata can be shared (ISBN, author, publisher, etc). See, for example, the Book-Crossing Dataset.\nSo, in order to get a big data set of blurbs, you'd have to contact (large) publishers and ask for access. I noticed you have university affiliation, so you should mention that it is for non-commercial purposes.\n", "data request - Map of Orange County (CA) buildings": "\nAs someone who actually has a fair bit of knowledge into the technological progress Orange County, California has made in the way of GIS, I can say that the data more than likely still does not exist for the whole county. Major cities such as Anaheim, Santa Ana, Irvine, Newport Beach may have building footprint shapefiles (I know for a fact that Anaheim and Irvine do). But many of the smaller cities have not done the work from the last time I can recall having the most relevant information (2012).\nOrange County has only recently taken up the cause of digitizing its infrastructure to data/GIS. They originally hired the utility companies to create the parcel shapefile layout for the county and that was as recent as 2009. The parcel shapefiles have been publicly available for Orange County, CA since 2012 and the most recent data which has sufficient attribute data is for 2014.\n", "Variable/multidimensional json record structure OpenFDA": "\nFor any of the /drug endpoints, I would use the opendfda.substance_name.exact annotated field. It will not be on all records, it is matched in a variety of ways against each dataset, so we took an approach of being conservative in the matching for the initial phases of this project; however, when it is there, it will have all sorts of wonderful information. \nIn the case /drug/event.json, the openfda record is under patient.drug.openfda.substance_name.exact. If this fields is not fully populated (which is might not be at this point), you can always fall back on the patient.drug.medicinalproduct.exact field (which is part of the original adverse event submission). An example would be:\nhttps://api.fda.gov/drug/event.json?search=patient.drug.medicinalproduct.exact:RETINOL\nor\nhttps://api.fda.gov/drug/event.json?search=patient.drug.openfda.substance_name.exact:RETINOL\nAs you can see, there are only two hits against the openfda.substance_name field in this case, which is a good sign that there is not good coverage in the annotation process, which means that you stick with the medicinalproduct field in this case. \nA note about 'exact'. In this case, it is best to use because substance names can have multiple words in them (exact does phrase style searching); however, exact also is case-sensitive and most substance names in the SPL are uppercase.\n", "data request - CPU instruction set": "\nOnly a single architecture, but maybe helpful nevertheless:\nThe educational MMIX architecture by Donald Knuth has a full list of instructions on their current website, including opcodes, signature and timing information (e.g. ADDU = 1 cycle, MULU = 10 cycles). \nHowever, the list of instructions is not available as a machine-readable format, but might be parsable with a bit of RegExp matching of HTML lists and tables. This compact table of all opcodes might facilitate that task.\n", "data request - Datasets that input GPS and output historical time-series": "\nThere Daily Global Weather Measurements, 1929-2009 (NCDC, GSOD) dataset contains:\n\nA collection of daily weather measurements (temperature, wind speed, humidity, pressure, &c.) from 9000+ weather stations around the world.\nGlobal summary of day data for 18 surface meteorological elements are derived from the synoptic/hourly observations contained in USAF DATSAV3 Surface data and Federal Climate Complex Integrated Surface Data (ISD). Historical data are generally available for 1929 to the present, with data from 1973 to the present being the most complete.\n\nThe data is global, including ocean measurements, will contain frequent daily measurements as well as latitude and longitude coordinates.\n\nOne common way to get this dataset is through the hosting by Amazon Web Services, and the original details are found at the NOAA site.\n\nCaveat: the license says\n\nThis data set can only be used within the United States.\n\nSo perhaps your 1 year of free tier AWS instance can be set up in the US.\n", "data request - Simulated dataset agent based gps": "\nTo simulate GPS tracks, consider using the Optimal Roadtrip code from Randy Olson.\nSteps (taken directly from ipython notebook):\n\nConstruct a list of road trip waypoints\nGather the distance traveled on the shortest route between all waypoints (using Google Maps directions)\nUse a genetic algorithm to optimize the order to visit the waypoints in \nVisualize your road trip on a Google map (not important for your application)\n\n\n", "Historic data on number of houses and number of households in UK?": "\nDid you find the Live tables on Household Characteristics page? Table 801: Tenure trend is an Excel spreadsheet which has the total number of households dating back to 1939 (not every year) and for 1918 it has percentages of owners and renters but not absolute numbers. (\"Absolute\", as the data is presented in thousands of households anyway.)\nFiles on the Live tables on House Building have data on the number of dwellings, although the one I checked only dates back to 1969. These files are in ODS format.\n", "usa - Where can I get NY small businesses data, including revenue, number of employees etc?": "\nThere is a New York State Directory of Minority/Women-owned Business Enterprises. You can search using a form on that page, or download the entire directory as CSV or Excel.\nIt has the following columns:\n\nCompany Name\nDBA Name\nOwner First\nOwner Last\nPhysical Address\nCity\nState\nZip\nMailing Address\nCity\nState\nZip\nPhone\nFax\nEmail\nAgency\nCertification Type\nCapability\nWork Districts/Regions\nIndustry\nBusiness Size\nGeneral Location\nLocation\n\n", "real estate - Looking for home sale data": "\nnationwide, the biggest and best data set you're going to get is\nhttp://asdfree.com/home-mortgage-disclosure-act-hmda.html\nbut if you can use a survey instead, start with\nhttp://asdfree.com/american-housing-survey-ahs.html\nand check other surveys on asdfree.com that have household-level statistics like acs, nychvs, and maybe scf\nthis is also worth a read\nHow to construct a database with the underlying real estate data displayed by Redfin, Zillow, or Trulia?\n", "Where can I find publicly available data about internet usage?": "\nEven though it is not open data, I can suggest to use also Statista. In this platform (I'm a free member), I found some data related to Internet usage in China.\nInternet users in China 2005 - 2014.\nI hope this one help you. There's also more information about Internet usage in the World Bank Database or the US Census Office.\n", "food - How can I download the Product Open Data database?": "\nThe Wayback Machine is always your best friend; Here's the data from 2014-02\nhttp://web.archive.org/web/20140209011312/http://product-open-data.com/download/\nOPD Product Browser Web Repository\nThere's also open food facts\n", "data request - Global map of protected areas by IUCN category": "\nI believe you will find what you are looking for at the Global Landcover Facility (though a little dated - 2007).\nhttp://staff.glcf.umd.edu/sns/branch/htdocs.sns/data/wdpa/\nAs a member of the IUCN, the GLCF provides the World Database on Protected Areas for free to the world. This data set contains GIS layers of protected areas that were produced by the World Conservation Union (IUCN) and the United Nations Environment Programme (UNEP)'s World Conservation Monitoring Centre (WCMC). The 2007 data set includes protected areas recognized at the international and national levels. Data are provided as point or polygon shapefiles.\n", "What is the status of OKFN's Open Product Data project?": "\nWe're actually looking for a new maintainer for the project.\nThe new version is running on Django and the source code is here.\n", "best practice - Searching for Open Data Dataset That is No Longer Online": "\nThe Memento Web and the Wayback Machine are two possible solutions:\nWayback Machine\nThe Wayback Machine by the Internet Archive is your best friend for all things that were once online, and even some things that still are, if you want to compare changes.\nFrom Wikipedia:  \n\nThe Wayback Machine is a digital archive of the World Wide Web and other information on the Internet created by the Internet Archive, a non-profit organization, based in San Francisco, California. It was set up by Brewster Kahle and Bruce Gilliat, and is maintained with content from Alexa Internet. The service enables users to see archived versions of web pages across time, which the Archive calls a \"three dimensional index.\"\n\nIf you have a dead link, simply paste it into the search input element of Wayback Machine.\nAlternatively, you can type https://web.archive.org/web/*/DEAD_LINK'S_URL directly into the address bar of your favorite browser.\nMemento Web\nLos Alamos National Lab has been offering the Memento Web project which unifies search across the archives.\n\nMain form to use for online search\nMemento plugin for Chrome\nMemento extension for Firefox\n\nOther\nSome search engines may provide you with a cached version of the page where the dataset resided (possibly, the dataset files as well).\nIf the steps above fail, you may consider\n\nlooking for papers which used the dataset (at arXiv or in academic journals),\ncontacting their authors,\nlooking at various bulletin boards for those who might have downloaded the dataset and contacting them.\n\nYou can also have a look at http://academictorrents.com to see if your dataset is present there.\n", "data request - Where can I find massive and high dimensional survival datasets": "\nIf you're using the \"survival\" statistical package on R, you should also be familiar with the free datasets that Rstudio comes packaged with.\nCheck out this webpage that lists all datasets that come preinstalled. Searching for \"survival\" should yield quite a few different tables that will be readily available to download.\nWhile many of them only contain only a few dimensions, some provide more granularity - but not sure if this meets your requirements. Do you have a more thorough list of requirements (N number of dimensions... X number of rows) that you can specify?\n", "releasing data - Where to host a public KML/GPX/OSM map file? (preview, map links, statistics, conversion)": "\ngeojson.io supports kml. it also allows you to import to github or gist.github\nedit: it doesn't have all of your requirements but the integration into github should allow you to do most except for the gpx\nedit:\nend results in gists: all of these .geojson maps were rendered via kml files from geojson.io\nhttps://gist.github.com/jalbertbowden \nstep by step:\ndownload this .zip file containing points for a history map of Hampton, VA:\nhttps://ckannet-storage.commondatastorage.googleapis.com/2015-03-05T21:52:24.450Z/locations-kmz.zip\nextract Locations.kml out of the .zip.\ngo to geojsion.io:\nhttp://geojson.io/\ngo to open, file, and upload the kml file, which gives us:\nhttp://geojson.io/#map=12/37.0532/-76.3602\nyou are previewing the data. you can edit it. you can redownload the kml. you can download geojson/topojson/csv/shp as well.\nthis also provides you with a bloc.ks url for sharing.\nclick on sharing:\nhttp://bl.ocks.org/d/6dd95a7f0d7d39b1e41a\nmore preview version but for an audience.\nclick on save. you have all the format options i've already listed, as well as github/gist. \nclicking on gist gives you:\nhttps://gist.github.com/jalbertbowden/006bf26bf7e5da3200fd\noptionally you can include it in a github repo as well. clicking on github will pull up your github repos and give you a menu from where to save.  \n", "Consumption data of gasoline by month and city available?": "\na think tank in chicago has done some of this legwork for you, but the price is from 2007; pretty sure you can change it to reflect new prices. and while this is only going to give you a cost estimate, i imagine dividing the cost by price should give you the number of gallons. not sure if you can enter a city, but you can search by zip/address so you could also compile them into cities/localities:\nhttp://abogo.cnt.org/ \nthere's also the urban mobility report, which should give you exactly what you want:\nhttp://mobility.tamu.edu/ums/\n", "data request - Searching for list(s) of babynames containing huge (10k+) amounts of unique names": "\nThe best source of international human given (first) names comes from a German computer magazine. The text file has nearly 50k names that are classified by likely gender, and how popular in each country. It's carefully curated and has a friendly license (GNU Free Documentation License 1.2).\nThe file can be downloaded here : ftp://ftp.heise.de/pub/ct/listings/0717-182.zip (name_dict.txt contains the data).\nArchive Link: https://web.archive.org/web/20200414235453/ftp://ftp.heise.de/pub/ct/listings/0717-182.zip\nInstead of parsing this file, you can use the python port SexMachine (really) - package and github repo. I'm sure other languages have their own ports. There is also a windows executable (details).\n(my reference)\n\nFor US baby names, you can use the Social Security Admin's download (overview) and link to data. This data can be national or on the state level, and going back to the late 19th century.\n\nTo safeguard privacy, we restrict our list of names to those with at least 5 occurrences.\n\nYou'll also find ports of this data to various languages.\n", "tool request - What product is the Australian government Meteor made using?": "\nBased on http://meteor.aihw.gov.au/content/index.phtml/itemId/236643, which states Synop Pty Ltd was awarded the contract to develop METeOR and developed the system by customising their XML-based content management system., I would say it is not free / open source software.\nYou could contact Synop, or you could ask AIHW if they can host some of your data: http://meteor.aihw.gov.au/content/index.phtml/itemId/276559\n", "What are best practices for where to answer agency specific Open Data Policy implementation questions?": "\nthis is a \"depends\" question, but i'll lean to always having the answers on your site. you are the authority. you are releasing the data. you are in control.\nbringing them here could help alert people to the data and the questions, as well as help crowd source solutions or answers you are seeking.\nthere's a saying called \"own your data\" and i believe this falls under it as well. \n", "data request - Alcohol consumption and reaction time": "\ncdc is probably your best bet. start here:\nhttp://www.cdc.gov/alcohol/fact-sheets/alcohol-use.htm\n", "What standards exist for accessibility of open data?": "\nI'm going to assume that the data is in some graphical format -- if it's numerical data, then the visualization would be a function of the software that's using it.\nFor images, there isn't anything that I'm aware of that you can do for blind people.  I've heard of devices that allow a blind person to trace a pen-like device over a surface, and it'll vibrate depending on what it's pointing at.  I've also seen specialty braille books by Noreen Grice that take a few images and apply textures to them.  (one of my co-workers was involved with her 'Touch the Sun' book)\nIf you're serving images via HTML, you can use the 'longdesc' attribute to provide a textual description of the image, with more specificity than would be appropriate for an 'alt' attribute.\nAnother consideration for images are the color-blind.  'Rainbow' style color tables are particularly bad not only for the color blind, but have also been shown to influence people in unintended ways:\n\nHow The Rainbow Color Map Misleads\nDear NASA: No More Rainbow Color Scales, Please\nThe Rainbow color map\nRainbow Colormaps \u2013 What are they good for? Absolutely nothing!\n\nThe last one links to a number of other resources, including alternate color tables and scholarly articles on the issue.  (note that AGU's EOS had a front-page article on this topic years ago, and did absolutely nothing in their editorial process, within a couple of issues they had front-page articles with the problem).\n", "NOAA QCLCD weather data - inconsistencies in hourly data?": "\nSo I emailed the NOAA, and they responded pretty quickly with clarification (props to them!)\nQ: My understanding of the \"WeatherType\" is that it is an \"abbreviated 3-hourly weather observations\" (from the QCLCD summary at https://data.noaa.gov/dataset/quality-controlled-local-climatological-data-qclcd-publication ). Is it more of a 3-hour summarized outlook of the weather, or a spot determination of rain/snow/haze etc?\"\nA: No, it's hourly, not 3-hourly. It's a spot determination at the time of observation.\nAs for the HourlyPrecip column, the measurements are only taken hourly, so there will only be measurements at 9.51, 10.51, 11.51, etc. \n", "data request - What's the train frequency at all stations in the UK?": "\nYou can find the complete timetable in PDF through Network Rail.\nYou might be able to scrape the required information from it.\nOtherwise, perhaps you can contact Network Rail directly to see if the information you're asking for is available.  In Sweden, tydal.nu has had success in scraping information from a similar Swedish PDF timetable, so perhaps the people there have some code they are willing to share.\n", "weather - Hourly data on whether it is snowing for a particular location (NYC)": "\nThe live weather feeds for weather stations, reporting to the NWS, in the State of New York can be found here at the link below. Each station has its own update interval. The XML feed is really HTML, so you will have to parse it as a HTML table.\nhttp://w1.weather.gov/xml/current_obs/seek.php?state=ny&Find=Find\n", "data request - Average Job Salary by Title/Location/Etc. (US or Intl)": "\nI recommend Occupational Employment Statistics. Includes approximately 800 job categories/titles (Standard Occupational Classification). Reports estimated employment, average wages and 10th/25th/50th/75th/90th wage percentiles.\nThe upside is that it's pretty easy to download or hit the API. The downside is that there are quite a few \"holes\" in the data, either due to confidentiality issues (e.g., one large company in an area employing the majority of, say, Aerospace Engineers can lead to those values being blanked out) or insufficient sample size.\nBut for free / publicly available, geographically detailed stats on occupations in the US, it's pretty much the only game in town.\nXLS files for metro and multi-county nonmetro areas here: http://www.bls.gov/oes/tables.htm.\nFor API access, it's similar for most BLS data. Example:\nhttp://api.bls.gov/publicAPI/v2/timeseries/data/OEUM001018000000029114103\n\nThe last component is the BLS \"seriesID\". Breaking it down:\n\nOE: dataset ID\nU: not seasonally adjusted\nM: area type is metro\n0010180: area code for metro area of Abilene, TX\n000000: industry code \"total\", all types of businesses\n291141: occupation code for Registered Nurses\n03: statistic code - get the mean hourly wage\n\nFor additional info on codes, see http://download.bls.gov/pub/time.series/oe/ (esp. the \"oe.area\" and \"oe.occupation\" files). It's a little misleading because this dataset is not actually a timeseries; only the latest year (currently 2014) is available at any given time.\n", "releasing data - What's a good file format for sharing 3D models of cities?": "\nIn answer to the original question, you should look at CityGML \u2013 this is a standardised format that's being heavily used by cities, particularly in Europe. It handles model definition, textures, various feature types (buildings, bridges, street furniture, all sorts) and has been built to keep the data about buildings and other objects intact (eg. which part is the roof and which is the wall). In fact it has various levels of detail that can be used depending on your need, so you could even go as far as modelling the full interior of a building down to the fixtures and fittings. \nHere's an example CityGML building at LOD2 (not the worst, not the best):\n\nThat building is actually within a new open building library that I'm creating with the guys at Mapzen that uses ViziCities (I'm the creator of ViziCities). We hope to release it publicly very soon.\nFailing that \u2013 collada or OBJ or fairly well used by cities for sharing buildings.\n", "government - What's a good file format for presidencies of countries?": "\nThe following has the data you want, but in an HTML table format. You will need to parse it out of the tables.\nhttps://www.cia.gov/library/publications/the-world-factbook/fields/2077.html\n", "Related Books API": "\nProject Gutenberg has what's called a Bookshelf, which are categories.\nAlso, for each title, there is a link called \"Also downloaded\"\nSo for book:\nhttp://www.gutenberg.org/ebooks/20194\n\nthe related titles would be:\nhttp://www.gutenberg.org/ebooks/20194/also/\n\n\nSome resources\n\nProject Gutenberg Catalog and Offline Catalogs\nSample Index (to get the book ID for the URL)\nBulk download\nPython package\n\n", "Interested in the history of the Washington Data Processing Center": "\nThe Washington Data Processing Center was located in the basement and subbasement of the second wing of the South Building of the USDA between 12th and 14th streets along Independence Avenue, SW, Washington, DC. \nThe USDA South building was built in stages between 1930-1936 using the designs of Architect Louis A. Simon.\n", "weather - Are there open historic cloud cover data files available?": "\nFor the satellite era, the best you can get is likely reanalysis data:\n\nA meteorological reanalysis is a meteorological data assimilation project which aims to assimilate historical observational data spanning an extended period, using a single consistent assimilation (or \"analysis\") scheme throughout.\n\nThe two most widely used reanalysis are the ones from ECMWF and NCEP.  Of those, the former is European and semi-free, whereas the latter is fully free.  After making an account, you can download data through this page at UCAR.  Hourly data includes fields like Cloud Amount/Frequency, and Cloud Liquid Water/Ice.\nNormal users of this data are atmospheric and climate scientists, so you might need to be quite careful to see you are interpreting the data correctly.\nFor before the satellite data, there exists no global record of cloud cover.\n", "licensing - Using Google Maps Data": "\nYes, it is possible, using the Google Places API.\nJust for completeness sake: This is most definitely not open data. For an open alternative, have a look at Phil's answer about Open Street Maps.\nIf your usage complies with Google's terms depends on what exactly you plan to do with the data. I guess section 10.1.3 of the Google Maps/Google Earth APIs Terms of Service will be most relevant for you:\n\n(a) No Unauthorized Copying, Modification, Creation of Derivative Works, or Display of the Content. You must not copy, translate, modify, or create a derivative work (including creating or contributing to a database) of, or publicly display any Content or any part thereof except as explicitly permitted under these Terms. For example, the following are prohibited: (i) creating server-side modification of map tiles; (ii) stitching multiple static map images together to display a map that is larger than permitted in the Maps APIs Documentation; (iii) creating mailing lists or telemarketing lists based on the Content; or (iv) exporting, writing, or saving the Content to a third party's location-based platform or service.\n(b) No Pre-Fetching, Caching, or Storage of Content. You must not pre-fetch, cache, or store any Content, except that you may store: (i) limited amounts of Content for the purpose of improving the performance of your Maps API Implementation if you do so temporarily (and in no event for more than 30 calendar days), securely, and in a manner that does not permit use of the Content outside of the Service; and (ii) any content identifier or key that the Maps APIs Documentation specifically permits you to store. For example, you must not use the Content to create an independent database of \"places\" or other local listings information.\n(c) No Mass Downloads or Bulk Feeds of Content. You must not use the Service in a manner that gives you or any other person access to mass downloads or bulk feeds of any Content, including but not limited to numerical latitude or longitude coordinates, imagery, visible map data, or places data (including business listings). For example, you are not permitted to offer a batch geocoding service that uses Content contained in the Maps API(s).\n\n", "data request - List of all NANP area codes and central office / exchanges and their geographic location?": "\nhttp://www.nationalnanpa.com/nanp1/npa_report.csv with definitions found in http://www.nationalnanpa.com/area_codes/AreaCodeDatabaseDefinitions.xls\nLook through nationalnanpa.com's sitemap has some valuable resources.\nhttp://cnac.ca/data/COCodeStatus_ALL.zip gives you Canadian codes.\n", "Public datasets containing less-boring and more detailed data": "\nThe go-to for random, interesting datasets comes from this article:\n100+ Interesting Data Sets for Statistics\nHere are some quotes from the article based on your question:\n\nHow do gender and mental illness affect crime? This data set was collected explicitly with that question in mind.\nThere\u2019s a lot of data from a series of online personality tests available here.\nWho receives H1-B visas?\nList of the most frequently searched-for data (google)\n\netc\n", "api - Is there any way to get a package's resource's field names/datatypes with downloading the entire resource?": "\nIf the site in question has enabled it, the CKAN's Datastore allows you to query tabular data directly via additional API functions. In your case you would probably use the datastore_info function to get the column names and data types for a given resource.\nHere's an example of getting that information for some weather data (using the excellent HTTPie command line tool):\n$ http post https://transparenz.karlsruhe.de/api/3/action/datastore_info id=117ae307-902a-4bf1-9c43-8b1aa9e8c2bd\nHTTP/1.1 200 OK\nCache-Control: no-cache\nConnection: keep-alive\nContent-Length: 642\nContent-Type: application/json;charset=utf-8\nDate: Fri, 21 Jul 2017 11:25:19 GMT\nPragma: no-cache\nServer: nginx/1.11.2\nStrict-Transport-Security: max-age=31536000\nVary: X-Forwarded-Proto,X-Forwarded-Port\n\n{\n    \"help\": \"https://transparenz.karlsruhe.de/api/3/action/help_show?name=datastore_info\", \n    \"result\": {\n        \"meta\": {\n            \"count\": 3550\n        }, \n        \"schema\": {\n            \"Apparent temperature (Degree Celcius)\": \"number\", \n            \"Atmospheric pressure (hPa; not normalized to sea level)\": \"number\", \n            \"Average wind speed (km/h; 10min window)\": \"number\", \n            \"Date and time (YYYY-MM-DD HH:MM:SS)\": \"date\", \n            \"Dew point (Degree Celcius)\": \"number\", \n            \"Global irradiance (W/m^2)\": \"number\", \n            \"Humidity (%)\": \"number\", \n            \"Maximum wind speed (km/h; 10min window)\": \"number\", \n            \"Precipitation (mm)\": \"number\", \n            \"Temperature (Degree Celcius)\": \"number\", \n            \"Wind direction (Degree)\": \"number\"\n        }\n    }, \n    \"success\": true\n}\n\nYou can check whether a CKAN instance is running the Datastore extension via the status_show API function.\n", "data request - List of abbreviations and acronyms": "\nI found a github repository that scrapes the wiki list of roughly 6000 acronyms\nhttps://github.com/krishnakt031990/Crawl-Wiki-For-Acronyms\nraw acronyms file\nand then I wrote this python script to read that code's output and clean it up, as well as put it into a valid CSV (gist)\n#!/usr/bin/env python\n# coding=utf-8\n\nignore_list = ('Search-Navigation','Tools-What links','Top-','Contents','Magyar')\nwith open('AcronymsFile.csv','r') as inp:\n    data = inp.read().split('\\n')\n\nwith open('clean_AcronymsFile.csv','w') as out:\n    out.write('acronym'+'\\t'+'definition'+'\\n')\n\n    for line in data:\n        if not line.startswith(ignore_list):\n            tmp = line.split('-')\n            acronym = tmp[0].strip()\n            definition = '-'.join([x.strip() for x in tmp[1:]])\n\n            out.write(acronym+'\\t'+definition+'\\n')\n\nwrites this CSV output (tab separated): gist (still needs some cleaning)\nsample header:\nacronym definition\n0D  Zero-dimensional\n1AM Air mechanic 1st class\n1D  One-dimensional\n2AM Air mechanic 2nd class\n\n\n", "medical - EEG data, specifically for alzheimer's?": "\nThere are two datasets from the UCI Machine Learning Repository related to EEG, but not specific to Alzheimer's patients.\n\nThe EEG dataset\n\n\nThis data arises from a large study to examine EEG correlates of genetic predisposition to alcoholism.\nIt contains measurements from 64 electrodes placed on subject's scalps which were sampled at 256 Hz (3.9-msec epoch) for 1 second.\n\n\n\nThere is also an EEG Eye State related to video analysis of open/closed eyes and EEG measurement.\n\n\nAll data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after analysing the video frames. '1' indicates the eye-closed and '0' the eye-open state. All values are in chronological order with the first measured value at the top of the data.\n\n", "usa - why doesn't federal reserve services website have all routing numbers?": "\nThere is a GitHub Repo FedACHdir that contains what I think is the same text file and also the specific routing number you are looking for:\nhttps://raw.githubusercontent.com/gatepay/FedACHdir/master/FedACHdir.txt\n\n", "data request - Public transport maps for all cities": "\nThe OpenStreetMap.org website has a \"Transport Map\" layer:\n\nIt's not great. In particular, tram stops are far too prominent (for the city of Melbourne anyway).\nYou could conceivably make a better style, using openstreetmap data, and a tool like Mapbox Studio.\nYou didn't really make it clear whether you want a geographically accurate or a stylised map (like a subwap map). The latter can only be made by hand, as far as I'm aware.\n", "state - USAID DDL - program codes": "\nOur apologies for the delay.  We are updating our settings here and will strive to respond within 48 hours to future inquiries.  Per Project Open Data, the individual metadata fields correspond to the specific \"data asset\" (or dataset) which you are describing.\nA single USAID award could generate data corresponding to every single program code.  Therefore using the program code to refer to an entire award would be of little value.\n", "usa - Open data version of the Foreign Assistance Program Inventory?": "\nWe have requested that a machine-readable (CSV) version of the Standardized Program Structure and Definitions be posted to this website.  Will circle back when we have confirmation of posting. \n", "us census - How can I get a full list of US Zipcodes with their associated names/CSAs/MSAs/lats/longs?": "\nUpdate: I recently found this data at GeoNames.org, among many related data sets. The file includes city, county, state, latitude and longitude. It would be a separate exercise to roll it up to MSA or CSA.\nDirect link to US Postal codes ZIP file\n\nFor many people, the best way to get this data is to buy it. There's a cottage industry of data services that have been providing this to small businesses for years, the costs are pretty low, and the length of the rest of this answer verifies that it's involved, although certainly something you can do if you have some coding and data skills.\nIf you'd rather create this dataset than buy it, here are things to know:\nThe Census Bureau states: \n\nThe USPS ZIP Codes identify the individual post office or metropolitan area delivery station associated with mailing addresses. USPS ZIP Codes are not areal features but a collection of mail delivery routes.\n\nIf the distinction matters, much more information can be found on this page The key things to know: ZCTAs overlap city boundaries and certainly at least a few exist outside of any city boundary. Also, if you're starting from mailing addresses, you will probably have some ZIP codes which aren't in the ZCTA dataset. And, the USPS may have changed ZIP codes since the last time the Census Bureau produced ZCTA definitions. (Some commercial data vendors promise to keep up with those changes as another value-add.)\nSo anyway, this means that naming is not straightforward. The Census Bureau just uses the 5-digit reference ZIP Code as a label. If you want to assign them names based on the nearest town, etc, you have some work to do.\nZCTAs are areas, not points, but when you ask for the latitude and longitude, you're probably interested in the centroid. You can find this in the Census Bureau's ZIP code tabulation area (ZCTA) gazetteer file, which can be downloaded here. (You want the INTPTLAT and INTPTLONG columns.)\nOnce you have a ZCTA centroid, you can locate it in a CSA or MSA. GIS software makes this fairly straightforward, with shapefiles provided by the Census's TIGER program.\nBut your question is further complicated by this: to the Census Bureau, CSA and MSA are different (and overlapping) things. \nThe bureau doesn't use the term \"MSA\" but rather combine \"metropolitan statistical areas\" and \"micropolitan statistical areas\" into a set called \"core based statistical areas\" or CBSAs. \nTo the Census, CSAs are \"Combined Statistical Areas\" and they are composed of some groups of CBSAs that are economically related. Not all CBSAs are part of any CSA. On Wikipedia, there's a page of \"primary statistical areas\" that articulates which CBSAs aren't in any CSA, but I haven't seen a GIS shapefile that matches that list. But maybe you can get by with just the CBSAs.\n", "data request - List of European Hospitals and Health Clinics": "\nThere might not be a database covering all countries of Europe (for instance the non-democratic country Belarus does not publish much information). So if you need more data that OpenStreetMap has, you will probably need to go country-by-country. Below is the data for France.\nThe French government has created a website showing the list of all 4307 hospitals and clinics in France:\nhttp://www.ScopeSante.fr\nClick on \"Voir la liste des \u00e9tablissements\" to get the full list.\n\nYou will need to scrape that. Each hospital name has a link to its detailed information page:\n\nI have just sent them a message asking for a dump of the data. They replied to me saying that all of their data comes from the FINESS database, which is also maintained by the French government, and available online \"for private use\" and unfortunately does not have dump downloads, so that would mean scraping that:\n\n", "data request - Where can I find State tax income rates from 1995 to present?": "\nThe taxfoundation.org provides this information going back to 2000. In addition to their interactive form, they have a downloadable Excel spreadsheet.\nhttp://taxfoundation.org/article/state-individual-income-tax-rates\n", "Exclusion lists when crawling web directories": "\nUsing --reject-regex \"\\?.=.;.=.\" seems to work for me to avoid the extra listings provided by Apache. The full command-line I use to get all the files from an Apache directory listing is:\nwget --recursive --no-parent --timestamping --no-directories --reject-regex \"\\?.=.;.=.\" http://example.com/some/dir/\n\n", "data request - Database of free WiFi hotspots": "\nI don't think the coverage is very complete, but Open Street Maps has a key called internet_access.\nWith this key you can use tags like wlan to find open WLAN.\nTo find free WLAN, add the internet_access:fee=no option\n", "sql - How to get a list of all strengths & forms for a drug from RxNorm?": "\nKind of stumbled upon this, while unlikely to help the OP seeing as this was asked years ago, hopefully someone will find this useful. I have not downloaded the full dataset but you can fetch the desired data here. I believe the rate limit is 20/second/IP address.\nexample call:\nhttps://clinicaltables.nlm.nih.gov/api/rxterms/v3/search?sf=DISPLAY_NAME&ef=DISPLAY_NAME,STRENGTHS_AND_FORMS,RXCUIS,DISPLAY_NAME_SYNONYM&terms={your search term here}\nthis should get you all names, aliases, strengths and forms along with rxcuis.\n", "data request - Electric utility boundary lines in US": "\nThe US Energy Information Administration has a few geographic resources (like the US Energy Mapping System and the State Energy Data System but there's no sign of a collection of electric utility service areas.\nOn a state-by-state basis, there are a variety of resources available. This might provide a roadmap for motivated parties to request shapefiles and assemble the dataset. I'm going to make this answer a 'community wiki' in case other people want to flesh it out with more states or hunt further for GIS files on states where PDFs were easier to find..\n\nArkansas (map, shp, gdb)\nCalifornia (map, shp)  \nConnecticut (gdb, shp) Utility Infrastructure and Pipelines\nIllinois (unofficial PDF map)\nIndiana (map, shp)\nIowa (pdf map)  \nKentucky\nMassachusetts (map, shp; town-level detail only)\nMinnesota (.shp, .gdb, .kmz, OGC GeoPackage)\nNew Hampshire (pdf map)\nNorth Carolina (pdf map of co-ops)\nVermont (.shp)\nWisconsin (pdf map)\n\nUS (National/Federal)\n - Bureau of Transportation Statistics Geospatial Information\nCanada\nAtlas of Canada Energy GIS\n", "calendar - Free data on public holidays?": "\nThere are many attempts to make a public holiday calendar, and one of the best ones is a python module called workalender (my source).\nBut the main problem with a global holiday calendar is:\n\nPlease take note that some calendars are not 100% accurate. The most common example is the Islamic calendar, where some computed holidays are not exactly on the same official day decided by religious authorities, and this may vary country by country. Whenever it's possible, try to adjust your results with the official data provided by the adequate authorities.\n\n", "machine learning - Any data set available for Twitter tweets classification?": "\nThe Streaming APIs\nOverview\nThe Streaming APIs give developers low latency access to Twitter\u2019s global stream of Tweet data. A proper implementation of a streaming client will be pushed messages indicating Tweets and other events have occurred, without any of the overhead associated with polling a REST endpoint.\nIf your intention is to conduct singular searches, read user profile information, or post Tweets, consider using the REST APIs instead.\nPlease refer the link: https://dev.twitter.com/streaming/public\n", "geospatial - Open Data about the internet": "\nThere is a good source of \"unofficial\" internet data from the 2012 Internet Census.\n\nWhile playing around with the Nmap Scripting Engine (NSE) we discovered an amazing number of open embedded devices on the Internet. Many of them are based on Linux and allow login to standard BusyBox with empty or default credentials. We used these devices to build a distributed port scanner to scan all IPv4 addresses. These scans include service probes for the most common ports, ICMP ping, reverse DNS and SYN scans. We analyzed some of the data to get an estimation of the IP address usage.\nAll data gathered during our research is released into the public domain for further study.\n\n\nHilbert Browser tool\n\nFull data download (568 GB torrent)\n\nImage gallery\n\n\n\n\nA similar project is the DNS Census 2013\n\nThe DNS Census 2013 is an attempt to provide a public dataset of registered domains and DNS records.\nThe dataset contains about 2.5 billion DNS records gathered in the years 2012-2013.\n\n(answer from this site)\n", "api - Airline check for availability data": "\nJust thought this might help anyone... https://www.airport-data.com/api/doc.php also provides API. you might have to sign up but its free. data is not real-time, so not to be used for flight travel\n", "data request - 1945 (spring) daily weather Germany": "\nFor German daily data, start here at the Deutschewettersdienst website.\nftp://ftp-cdc.dwd.de/pub/CDC/observations_germany/climate/daily/kl/historical/\nftp://ftp-cdc.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/\nOpen the file KL_Tageswerte_Beschreibung_Stationen.txt to find a station with data for the location and time period that you want.  For Bonn, it looks like station ID 599 \"Bonn-Friesdorf\" covers 1932 through 1999;  for Cologne, station ID 2665 Koln-Botanischer Garten covers 1936 thru 1984.\nThen, find the zipfile for that station, e.g. tageswerte_00599_19321001_19990301_hist.zip\nor tageswerte_02665_19360101_19841231_hist.zip\nWithin the zipfile will be a large textfile named something like produkt_klima_Tageswerte_19321001_19990301_00599.txt which contains daily records of temperature (avg, high, low, air pressure, precipitation, etc.)  It's raw data, so you'll have to find the column headings at the top, then read down the rows until you find the date that you want.\nThe only problem is that, for fairly obvious reasons, the stations stop recording and have data gaps beginning in 1944 or 1945, resuming again in late 1945 and 1946.  However, I have found two Berlin stations - 417 Berlin-Lichterfelde(Sud) and 402 Berlin-Dahlem (LFAG) - that continue recording up to March 31, 1945 and April 24, 1945, respectively.\nMakes you think.\n", "data request - Pictures of all airplanes, organized by tail number": "\nMost commercial aricraft, lots of military and some general aviation aircraft.\nAirliners.net\n", "data request - The spread of cable tv": "\nthere are a few, certainly not all, dates in this pdf about the history of television. one example: Cable television is introduced in Pennsylvania as a means of bringing television to rural areas. (1948)  \nnot a complete answer, but some information in here for you:\nhttp://tarlton.law.utexas.edu/exhibits/mason_&_associates/documents/timeline.pdf\n", "linked data - Are there any dataset for psychology": "\nThe Thesaurus of Psychological Index Terms, published by one of your employer's (?) customers, is known now as Psychology Ontology.\nThis ontology is not even a taxonomy, but rather a flat list of classes.\nBioPortal provides mappings between this ontology and many other ontologies.  \nAdditionally, BioPortal contains other ontologies published by APA:\n\nThe OntoPsychia ontology seems very interesting, but is not available for download.\n\nAberOWL repository provides these links:\n\nMFOEM - Emotion Ontology\nMF - Mental Functioning Ontology\nMFOMD - MFO Mental Disease Ontology\nNBO - Neuro Behavior Ontology\n\n", "Linked Geospatial Data in WFS": "\nYou can use HTTP-URI values from controlled vocabularies in the attributes of your WMS datasets, as properties in your WFS feature types and WCS coverages.  You can additionally add links in your GetCapabilities response documents through metadata urls, and data urls...  And finally you can make links in your metadata documents for your services and datasets.\n", "Does there exist a set of data that measures internet traffic that is specific to image downloading and uploading?": "\nI think you're unlikely to find this as true \"open data,\" since there are substantial privacy concerns to internet traffic monitoring, and the volume of data involved to do proper analysis is considerable.\nThat said, there is an organization, the Center for Applied Internet Data Analysis (CAIDA), which makes datasets available to researchers under oversight. A brief look at their data overview doesn't reveal any sets which are obviously content-oriented, but they may be able to advise.\nBeyond getting the data yourself, you may be able to find a citation in media or academic publishing which substantiates your hypothesis. This GigaOm story, \"How the core of the internet has changed from data to content\" might lead you down the right path.\nUltimately, the amount of still image data looks like it will be a drop in the bucket, if the Cisco study covered in this Recode article is correct: they claim that video traffic is currently 78% of internet traffic, and headed for 84% by 2018. If you still want to get numbers about image data, perhaps digging deeper into the Cisco study will yield results.\n", "data request - Seeking geographical database that, given coordinate boundaries, returns all the mountain peaks within the boundary, their coordinates, and elevation": "\nI think this is another task for OSM.\n\nExtract data within Polygon or by bounding box\nFilter based on tag Peak\n\nSee some other questions about using OSM as a datasource, including the Overpass API, by searching here or Google\n", "legal - Legality of using data against terms": "\nhere's more of an opinion, not an answer:\ni'm free software all the way, and i wouldn't give a care to what their terms say, i'd do whatever i pleased with it, until they killed the service.  \nin reality, corporations will destroy you, unless you have an army of lawyers too. look how broken the patent system is, or the hacker that exposed a public url and went to prison. odds are you won't get nailed, but if you do, it'll probably be severe. remember, hackers are treated like terrorists in america.  \nit really pains me to say that utilizing an rss feed makes one a hacker, but the media will spin it like that, no doubt.\n", "data request - Machine-readable way to determine if plant species is native to Australia": "\nG'day Steve,\nYep APNI (Australian Plant Name Index) list Australian Native and Naturalized plants as well as some weed species.\nAPNI includes some services for looking up plant names.\nAPC the Australian Plant Census is a list of the currently accepted names as recognised by CHAH the Council of Heads of Australian Herbaria. (http://www.anbg.gov.au/chah/)\nTo find a name in APNI you can use https://biodiversity.org.au/nsl/services/apni\nThis search will display an APNI format output of pretty much all we know about the name and synonyms of the name (not about the plant, just the names references, authors )\nA search like this one https://biodiversity.org.au/nsl/services/search?product=apni&tree.id=3029293&name=Hakea+actites&max=100&display=apni&search=true will give you the output for Hakea actites. But your search may return mutiple matches. The match is against the 'full name' of the plant, so it includes the Author.\nThe link icon in that output is a permalink to a name object which is consumable e.g. https://biodiversity.org.au/boa/name/apni/163125 You can ask for that link in JSON, XML, or RDF using the request mime type. To see it in your browser though you'll need to add .json to the end, e.g. https://biodiversity.org.au/boa/name/apni/163125.json\nYou'll notice the links on the name object include https://biodiversity.org.au/boa/Hakea%20actites%20W.R.Barker which uses the full name as an identifier. This is a quick way to see if we have linked data on a name. However we only have the full name in that service right now.\nSo... we have doco happening (the service has just changed rather a lot) and we'll keep people informed via twitter @AuBiodiversity. There are lots of other services.\nedit: add weed and naturalised info:\nOK APNI is just a name store, it stores facts about names wrt references. APC is a classification and it stores some opinion (CHAHs) about the names.\nCurrently the data about something being a weed or naturalised is in the form of notes by APC, so there is not clear cut yes/no.\nSome examples:\nAPC Dist gives the distribution e.g. https://t.co/mo2OD9TbIV\nif naturalised it says so e.g.https://t.co/0i4sAqS7Lg\nweed status is a little sketchy and can be in status or comments e.g.\nhttps://t.co/K4nzt6OOFD and https://t.co/h9WDU61XUQ\n", "data request - Where to find number of companies by year revenue?": "\nGovernment procurement systems\nSee this answer for details on how to get the revenue numbers for firms supplying goods to governments:\nhttps://opendata.stackexchange.com/a/5149/1052\nThough this sample may be biased (government consumption is somewhat unconventional), this data is just two minutes away.\nRequest data from a government agency\nIf you need only the bins by size without disclosing the names of the companies, you may send a request to IRS, BLS, or another agency that may have the data, from their surveys.\n", "tool request - Mapping CSV Header to RDF + SPARQL console": "\nBefore building a completely new system from scratch, you should first check if an existing system satisfies your requirements. There are a few possible candidates out there:\n\nAs outlined in this answer, D2RQ and RDF HDT might be close to what you are looking for.\nIn addition there is Tarql which provides SPARQL access to CSV files \u2014 which is, as far as I can tell, exactly what you want.\n\nOne more comment: If you want performant search for your CSV files, you won't get around a search index. And instead of building one yourself, I recommend having a close look at D2RQ again.\n", "Journalism examples using officials public data portals?": "\nWell, here's one: http://www.theage.com.au/victoria/the-most-ignored-parking-sign-in-melbourne-20140512-zraek.html\nHere's the background: http://www.theage.com.au/data-point/blogs/the-crunch--data-point/melbourne-parking--get-the-data-20140513-386wv.html\nSummary: data journalists from The Age found data on the City of Melbourne's data portal, wrote a story from it.\n", "Open Badges/Tin Can to open up credential & learning data": "\nThe differences between Tin Can and Open Badges seem to be that Tin Can is for one particular sector of micro-credentials (experiences) whereas Open Badges covers all sectors of micro-credentials.\n", "machine learning - Data set of software fault prediction studies": "\nyou can look at the software-artifacts infrastructure repository. this repository contains open source software with manually injected faults,test cases and scripts. This repository is available for everyone to use, we can conduct controlled experiments on these fault injected software.\nhttp://sir.unl.edu/portal/index.php\n", "data request - Where I can find a repository of software usage/execution logs (traces)?": "\nYou might consider using the EVENT table from the Mozilla Labs \"Day in the Life of a Browser\" dataset.\n\nUnfortunately the download links are inactive. But this dataset has come up on another question and hopefully Mozilla will fix it soon. \nLicense is Creative Commons Attribution 3.0 United States.\nUPDATE: data is now hosted here\n", "usa - Hyperloop; Which Open-Source License did Musk choose?": "\nSurprisingly, I can find no mention of license/patent/rights in the Hyperloop Alpha PDF published by Musk in 2013. In a tweet, Musk stated:\n\nI really hate patents unless critical to company survival. Will publish Hyperloop as open source. \n\nI sent an email to hyperloop@teslamotors.com inquiring. I'll update this answer if I hear back.\n", "Where can I find a downloadable grocery store food ingredient database / data set?": "\nhave you tried Open Food Facts? It's a \"free, open and collaborative database of food products from the entire world.\"  The data is offered under the Open Database License\n", "research - Spectral reflectance data for iron rust": "\nAs mentioned in the comments, I found a resource that has a myriad of spectral samples of just about all minerals (including rust) - The RRUFF Project website, where it contains\n\nan integrated database of Raman spectra, X-ray diffraction and chemistry data for minerals.\n\nThis site has a searchable database, the only (albeit minor) downside is that it does not allow for generic terms such as 'rust', but rather, the actual chemical names such as goethite - a main component of 'red' rust.\nRaw spectral data can be downloaded from the pages.\nIf there is difficulty in determining the real name, then WebMineral.com can assist, through a search facility that takes the chemical name and provides mineral names.\n", "machine learning - For a recommender system, is there a real data matrix that is about 500 by 500 that is complete and has no missing entries?": "\nHere is a listing of Recommendation and Ratings Public Data Sets For Machine Learning.\n500x500 may be ambitious, so instead of movies you may have to use music (shorter content means more reviews)\nIn particular, take a look at (copy/pasted from site): \n\nLast.fm - Music Recommendation Data Sets \nYahoo! - Movie, Music, and Images Ratings Data Sets \nAudioscrobbler - Music Recommendation Data Sets \nAmazon - Audio CD recommendations \n\nNote that Last.fm also has an API\n", "weather - Hourly temperature data for specific locations (Arkansas) for the year 2014": "\nI would start here: http://www.climate.gov/hourlysub-hourly-observational-data-hourly-global-%E2%80%93-gis-data-locator.  It looks like you can go to \"View Data\", then select the stations of interest from the map, then specify the dates and data elements that you want.  It does not guarantee that hourly data will be available from every station.\n", "openstreetmap - Importing XML data into ArcGIS Desktop? - Geographic Information Systems Stack Exchange": "\nI would do as below:\nSince this map uses openstreet map as a basemap so follow as below-\nRun the query> Export result> Download geojson>add ineteroperability connection in arc map and add exported geojson> add basemap(openstreet map)\n\nN.B In this case (arcmap 10.1) you need interoperability extesion installed in arc map\nfor 10.2 onward there is a tool called JSON To Features (Conversion) use this to convert this geojson to feature (or use interoperability)\nSeveral format export is possible e.g as geoJSON,as GPX,as KML,raw data- all these format can be imported into arc map using interoperability connection.\n", "data.gov - Latitude and Longitude of US Commuting Zones": "\nYou can find out information about Americans' commuting habits in relation to specific geographic areas through the U.S. Census American Community Survey.  Access to the data is available.  For example, if you are looking for how people in Los Angeles County get to work, you can find the answer through the Easy Stats online.\n\nThe longitude and latitude of all U.S. boundaries (from school districts to counties to roads) can be found via TIGER files (topographically integrated geographic encoding and referencing).\n", "usa - Are there open data sets about commuting patterns in large US cities?": "\nThere's a related question.  This answer may be helpful here too Nick.\nYou can find out information about Americans' commuting habits in relation to specific geographic areas through the U.S. Census American Community Survey.  Access to the data is available.  For example, if you are looking for how people in Los Angeles County get to work, you can find the answer through the Easy Stats online.\n\nThe longitude and latitude of all U.S. boundaries (from school districts to counties to roads) can be found via TIGER files (topographically integrated geographic encoding and referencing).\n", "business - Need sample E-commerce order data": "\nA popular answer comes from a 2010 question on Stackoverflow - Source Link\n\nNorthwind database - (documentation & data model)\nNopCommerce sample dataset\nE-commerce dataset from Amazon / Google Products / Abt Buy \n\nOther ideas:\n\nTableau Superstore (Excel file) (although I think this is aggregated)\n\n", "licensing - License of NOAA hourly temperature data": "\nI have contacted NOAA directly with this question, and the answer was (emphasis mine):\n\nAs you have described it below, you are not in violation of Resolution\n  40. This would apply if you are redistributing the data as-is for profit.\n\nIf you are unsure about your particular use case, I suggest contacting NOAA directly.\n", "data request - Find a missing dataset: \"A Week in the life of a browser - Version 2\" from Mozilla Labs": "\nI just snagged it off the wayback machine here:  \nhttp://web.archive.org/web/20110711102216/https://testpilot.mozillalabs.com/testcases/a-week-life-2/witl_small.tar.gz  \n\nbut more files can be found here by searching for '.tar.gz' or '.db.gz'\nhttp://web.archive.org/web/*/https://testpilot.mozillalabs.com/testcases/*\n\n\nDownload the data\n\nDownload links from Wayback Machine:\n\nwitl.db.gz (1.1 GB, SQLite 3.x database)\nwitl_large.tar.gz (469 MB, CSV files)\nwitl_small.tar.gz (7.4 MB, CSV files)\n\n\nDetails:\n\n", "data request - Where can I find shapefiles for the rivers of Puerto Rico?": "\nHere are the open data sets that match your requirements:\n\nData.gov's Puerto Rico hydrography data (shapefile format)\nData.gov's Puerto Rico hydrography data (all formats)\n\nSeems to be the same data sets, but available via geoplatform.gov site:\n\nPuerto Rico hydrography data (shapefile format)\nPuerto Rico hydrography data (all formats)\n\nFinally, these Puerto Rico hydrography data sets are in ESRI ArcInfo interchange file (E00).\n", "business - Is there a data set listing which pharmacies have self-checkout lanes?": "\nI think no such dataset exists. However, there might be some roundabout ways of approximating that information.\nHere's a Yelp query for reviews that feature the phrase \"self checkout\" in the Boston area. You can see both mapping and also the metadata that the results are \"1-10 of 97\" and you can select specific establishments. You can mirror this query using the Yelp API without going through a browser. \nThis isn't particularly comprehensive because a business needs a review that actually mentions \"self-checkout\" and the \"self-checkout\" phrase doesn't necessarily indicate that self-checkout is actually present (a review could include \"gee I wish this store had self-checkout\" and will come up). However it should still paint an approximate picture and would allow you to find specific locations that use self-checkout.\nYou can repeat this request for different geographies and if you automate it you could even cobble together your own database, or create your own tool for dynamically searching this data. Yelp has a fairly robust developer API and if you're able to accept the caveats of its information, it could be a good way to assemble a data set.\n", "data request - where can I find shapefiles for the highways of Puerto Rico?": "\nSince I have promised, I will answer this question without waiting for its migration, if it will ever happen. Basically, I think that the best and latest data set that you can find now is this one - from the US official open data repository's TIGER/Line database. This page is generated, based on a relevant search (Puerto Rico), and might also contain some data sets of your interest.\nOther potentially useful data sets include ones within U.S. Atlas TopoJSON repository (on how to use the data via R, see this nice tutorial) as well as this repository of U.S. major roads ESRI shapefile and geoJSON data sets (you have to check whether this repository contains PR data).\n", "government - Freshness of Unemployment Data": "\nThere are many sources of surveys/administrative in the United States data you might find useful.\n\nUnemployment Insurance Claims - the number of people in a given week who: newly file for unemployment insurance, are continuing their unemployment insurance. This would be the most timely data.\nCurrent Population Survey (CPS) - (reference period, data source) This is the monthly survey conducted since 1940 that is used to take an assessment of the country's labor force status. Whenever you hear about the \"official\" unemployment rate, it is usually coming from this survey which are published \"Employment Situation\" reports such as the ones here. This is a fairly well-organized source of information since the response rate is usually 93% and above and has less statistical noise since in any given month ~72% of the sample is carried over from the previous month and ~45% is carried over from the previous year.\nCurrent Employment Statistics (CES) survey - (reference period, data) This is the monthly survey that polls random companies about their employment. Be warned, however, its timeliness is questionable since it takes up to three months for the average response rate to go above 93% ( usually in the first month, its response rate is ~60% on average).\nQuarterly Census of Employment and Wages (QCEW) - (Data: source 1, source 2) - This is a far less timely source of data. However, it is a publicly released report of all payroll employment within a given quarter. Historically, it takes on average nine months for the data to release so by the time it is ready you are typically looking at data from a year ago.\nLocal Area Unemployment Statistics (LAUS) - This source of information is different from all the rest since it uses small area estimation to predict state and local unemployment rates. It uses the CPS as its primary source, along with other data such as UI claims or the CES survey act as secondary sources. This source is also less timely for your purposes.\n\n", "metadata - CKAN exposing tables from a relational database": "\nHave you already tried the resources here?\nhttps://lists.okfn.org/mailman/listinfo/ckan-dev\n", "usa - Trying to extrapolate patient costs by physician from public Medicare pricing data": "\nFor the benefit of the community here is a late answer. The average payment by the beneficiary (and third party payers) is: \naverage_Medicare_allowed_amt - avg_Medicare_payment_amt\nFrom the PDF:\naverage_Medicare_allowed_amt is the sum of the amount Medicare pays, the deductible and coinsurance amounts that the beneficiary is responsible for paying, and any amounts that a third party is responsible for paying.\nIt is customary in US health insurance billing to distinguish between submitted charges, that are largely detached from the actual allowed charges. The latter are split between the beneficiary and Medicare. So by subtracting what Medicare actually paid from the price it allowed you get what is left for the beneficiary (or third party payers, meaning other insurers that cover the beneficiary, like Medigap).\n", "usa - Crime rate data for American cities": "\nThis site goes back several decades. Each file is pretty large but also has the data broken down by month. I don't think you'll be able to get around varying degrees of reporting, that's a common problem statisticians have to deal with. \n", "finance - Business performance data of web company like number of employees, revenue etc": "\nHere's a link to datasets (Excel spreadsheets) from a NYU professor whom has been keeping corporate finance data on major corporations for 20 years:\nhttp://pages.stern.nyu.edu/~adamodar/New_Home_Page/data.html\nFilter the companies with classification \"Software (Internet)\", there are 759 of them. I generated the list at https://gist.github.com/nicolas-raoul/1145776cc37d5654c310 but here is an excerpt:\nFund.com Inc. (OTCPK:FNDM)\nSpectrumDNA, Inc. (OTCPK:SPXA)\nNet Savings Link, Inc. (OTCPK:NSAV)\nGlobal MobileTech, Inc. (OTCPK:GLMB)\nSanteon Group, Inc. (OTCPK:SANT)\nAnchorage International Holdings Corp. (OTCPK:AHCP)\nmyContactCard, Inc. (OTCPK:MYCT)\nMetatron, Inc. (OTCPK:MRNJ)\nOptimum Interactive USA Ltd. (OTCPK:OPTL)\nVerecloud, Inc. (OTCPK:VCLD)\nBigsupersearch Com Inc. (OTCPK:BSPR)\nIdle Media, Inc. (OTCPK:IDLM)\nThwapr, Inc. (OTCPK:THWI)\nBizzingo, Inc. (OTCPK:BIZZ)\nDigagogo Ventures Corp. (OTCPK:DOGO)\nRarus Technologies Inc. (OTCPK:RARS)\nSinglePoint, Inc. (OTCPK:SING)\nWrapMail, Inc. (OTCPK:WRAP)\nEventure Interactive, Inc. (OTCBB:EVTI)\nGuard Dog, Inc. (OTCPK:GRDO)\nBullsnBears.com, Inc. (OTCPK:BNBI)\nTruli Media Group, Inc. (OTCPK:TRLI)\nWally World Media, Inc. (OTCPK:WLYW)\nIL2M International Corp (OTCPK:ILIM)\nEFH Group, Inc. (OTCBB:TWYF)\n\n", "data request - Dataset of major newspapers content": "\nWikipedia: List of online newspaper archives\nMany lack \"official\" interfaces and may restrict the use of materials (see their copyrights), but those that fit the requirements can be scraped with web services like import.io or Python's Scrapy:\n\nhttp://en.wikipedia.org/wiki/Wikipedia:List_of_online_newspaper_archives\n\n(It also includes Google's scans of old newspapers.)\nLibrary of Congress: Newspaper Archives, Indexes & Morgues\nThe links to a few dozens of newspaper archives with full-text articles. International newspaper archives included. Mostly historical data.\n\nhttp://www.loc.gov/rr/news/oltitles.html\n\nAll Digitized Newspapers 1836-1922 by state, ethnicity, language:\n\nhttp://chroniclingamerica.loc.gov/#tab=tab_newspapers\n\nTheir API:\n\nhttp://chroniclingamerica.loc.gov/about/api/\n\n", "How to anonymously share data?": "\nThe OP says he is fine with Bittorrent now.  After all, Bittorrent was made for large files (outlawing Bittorrent would be like outlawing bombs.) Now, how to do it. First of all, Bittorrent isn't anonymous (again the creators had only one task in mind), so you will need a proxy or something. Now, create the torrent file. Now the torrent file itself will be small (a couple megabytes I think) so it would fit in a paste.\n", "data request - Dataset of Facebook Users Connectivity": "\nFacebook has the Graph API so you can construct your own queries and collect as much data as you want.\nOtherwise, there are some collected resources described in this thread (warning: from 2010 and maybe involving hacked data).\n\nOnline Social Neworks from UC Irvine\n100 million Facebook pages leaked as torrent\n\n\nIf you don't require Facebook, there are much better resources for Twitter (datasets and public search/stream API access)\n", "economics - microcredit or microfinance data": "\nHave you checked USAID's Microenterprise Results Reporting (MRR) Portal?  You may also be able to find other resources at USAID's Development Data Library.\nusaidopen\n", "finance - Why is my scraper returning inconsistent results and timing out?": "\nSome tips for scrapers:\n\nlog: don't stop at \"sometimes I get data and sometimes I don't\" -- when you get unexpected results in the response, log it so you can learn what happened. Specifically when you are being throttled for making too many requests, you will often get an explicit message saying so.\ncache: write your code to save files you retrieve and use saved versions before making a URL call. \n\nPython users can take advantage of some toolkits which make these two things easier. I've had some good experience with scrapelib and scrapekit. I do not know how to insert seamless caching into Mechanize\u2014last time I wrote a scraper with Mechanize I just did it manually, which is basically what @philshem suggested.\n", "industry - Data on metals and alloys properties (physical and chemical)": "\nMatWeb.com's page Metal & Alloy Composition Search has a searchable database, where you can search for metals and alloys by either (or both) of:\n\nChoose a Material Category\n\nor\n\nChoose up to 3 Material Compositions\n\nor search the entire UNS (Unified Numbering System for Metals and Alloys), which \n\nis a systematic scheme in which each metal is designated by a letter followed by five numbers. It is a composition-based system of commercial materials and does not guarantee any performance specifications or exact composition with impurity limits. Other nomenclature systems have been incorporated into the UNS numbering system to minimize confusion.\n\nThe data sheets provide details about the physical, chemical, mechanical, electrical, thermal and optical properties, and the properties of the components of alloys are provided.  Further references are provided in each fact sheet.\n", "data request - USAID Dataset Downloads Published List": "\nThank you for the inquiry. The DDL is USAID's Development Data Library located at www.usaid.gov/data.  We do collect basic web traffic statistics and will start looking into this with our CIO's office to kick off the process of posting download  statistics on a regular basis. \nAs of November 30, 2015, these stats are now posted here.\n", "usa - How do I find data that helps US businesses export their products?": "\nThe International Trade Administration (ITA) has just released Version 2 of all of its APIs on its Data Services Platform.  This means developers can access even more export data, more easily.  \nHere are the updates\n\nNew Data:  You asked and we answered!  We are now providing access\nto:\n\nTariff Rates from all US Free Trade Agreements\nFAQs About Exporting.\n\nMore Data: We are adding more data to our existing data sets:\n\nAdditional screening lists\nTrade Events from more trade agencies\nOverseas opportunities from FedBizOpps\n\nFriendlier:  We\u2019ve made it easier and more secure for developers to get the data they are looking for.\n\nYou can learn more about the details on ITA's wiki.\n", "data request - Number of employees of large companies?": "\nhttp://finance.yahoo.com\nGE for example Full Time Employees:    305,000:\nhttp://finance.yahoo.com/q/pr?s=GE+Profile\nAnd https://en.wikipedia.org/wiki/Yahoo!_Query_Language for auto extraction.\n", "open Big Data to solve cancer epidemiology challenge": "\nIf you're interested in brain imaging data, you could check out the Connectome Coordination Facility\nI downloaded the original HCP 1200 for our institution a couple of years free of charge. That's 7T & 3T imaging data for 1200 individuals. It's a large dataset but it is freely available for research. \n", "data request - Website visitors statistics": "\nTo all my available knowledge having done significant research on digital business models for a number of years, no, no there is not much of a comprehensive, free source for this type of enterprise data, as it requires expensive resources to keep updated and maintained, and even then isn't scientific or, as you put it, is merely an estimate.\nI know that's not a helpful answer necessarily, but it's reasserting the need for an open alternative and -- to my knowledge -- letting you know that I personally have not come upon one.\n", "data request - Database of all mailing addresses in France, and their coordinates": "\nThe French government has just launched a national open database that aims to contain all currently valid addresses: http://adresse.data.gouv.fr\nIt is a collaboration between the government, the French National Geographic Institute, the postal service, and OpenStreetMap.\n800 megabytes, one CSV file per French departement, WIN1252 encoding.\nThe data can be downloaded for free under the ODbL license without registration at http://openstreetmap.fr/ban\nExcerpt:\nid,nom_voie,id_fantoir,numero,rep,code_insee,code_post,alias,nom_ld,x,y,commune,fant_voie,fant_ld,lat,lon\nADRNIVX_0000000001948049,Chemin des Acacias,0001,15,\"\",97401,97425,\"\",\"\",329906.7,7653235.5,Les Avirons,0001,,-21.214471,55.361181\nADRNIVX_0000000002425821,Chemin des Acacias,0001,11,\"\",97401,97425,\"\",\"\",329895.1,7653266.5,Les Avirons,0001,,-21.214190,55.361072\nADRNIVX_0000000002255091,Chemin des Acacias,0001,15,BIS,97401,97425,\"\",\"\",329875.4,7653241.4,Les Avirons,0001,,-21.214415,55.360880\nADRNIVX_0000000002425822,Chemin des Acacias,0001,60,\"\",97401,97425,\"\",\"\",329224.6,7652935,Les Avirons,0001,,-21.217122,55.354581\nADRNIVX_0000000002425943,Chemin des Acacias,0001,19,C,97401,97425,\"\",\"\",329845.1,7653213.1,Les Avirons,0001,,-21.214668,55.360585\n\nDescription of the CSV fields (in French)\nShapefiles are also available.\n", "food - Data about spoon usage when eating spaghetti, by area": "\nNo dataset was available, so I got 300+ people to answer a questionnaire about the topic.\nGeneral view of the data (green=spoon, red=no spoon):\n\n\nDetailed analysis: http://aegif-labo.blogspot.jp/2015/04/eating-spaghetti-spoon-or-not.html\nRaw data (as a Google Spreadsheet)\nLicense: Public domain\n\n", "api - OpenFDA Data: Labels with Boxed Warnings": "\nYou are pretty close, you just have a syntax error on the query that is getting interpreted as a search string: it should be +AND+set_id: not +AND+set_id=\nAlso, that particular set_id does not have a boxed_warning so it will not return anything. \nThe following query confirm the syntax and that the example is missing a boxed_warning. \nhttps://api.fda.gov/drug/label.json?search=missing:boxed_warning+AND+set_id:3af2e694-6fea-46cf-b680-9ee0b0c83c88\nHope that helps.\n", "Open data of 1 million or more names, for fuzzy matching experiments": "\nWikipedia\nWikipedia let you download the data conveniently, without API limits, so you can get the titles of their 4M+ English articles. Depending on your needs, you can try other languages as well. See\nhttp://meta.wikimedia.org/wiki/Data_dumps\nIn particular, file *-all-titles-in-ns0.gz.\nAlexa One Top Million Websites\nThese are domain names, though not sure it meets your requirements.\nhttp://s3.amazonaws.com/alexa-static/top-1m.csv.zip\n", "data.gov - Forbes.com writer looking for up-and-running examples of data driven applications that improve human services in government": "\nSome colleagues and I started cataloging some different interesting ways government data is being used:\nhttps://github.com/18F/ideas/issues/2\nI'm no expert on this, and this doesn't quite answer your question, but hope it's helpful!\n", "data request - Free heightmap (elevation?) dataset for Krak\u00f3w, Poland?": "\nWow, wow, wow, based on @scruss's hint, I maybe have found something; not exactly free, but really, really cheap (i.e. not \"bussiness prices\" but \"average guy prices\" apparently); only I must yet verify if the data is really good enough for me and what I think it is, but there are some samples. Without further ado, the links are:\nhttp://codgik.gov.pl/index.php/zasob/numeryczne-dane-wysokosciowe.html - an overview of the available subtypes of data, and details on how to request them;\nhttp://codgik.gov.pl/index.php/obsluga-klienta/oplaty-za-udostepnianie-materialow-fotogrametrycznych.html - pricing (seems 2-4z\u0142 per km2 IIUC, which seems totally approachable).\n(Note: found based on information in a GUGiK video.)\n", "data request - source of character-level ngrams that include spaces": "\nBased on @NeilSlater's comment, you can easily calculate character N-grams with a few lines of code. \nIn this snippet, I use Python's Collections library, which is quite fast for these types of applications:\nfrom collections import defaultdict\n\ndef make_char_ngram(text,N):\n    data = defaultdict(int) # for speed\n\n    for i in xrange(len(text)-N+1):\n        x = text[i:i+N] # actual N-gram\n        data[x] += 1    # add 1 to this N-gram key\n    return data\n\nprint make_char_ngram('ABC the quick brown fox the quick brown fox the quick brown fox XYZ',8)\n\ngives you character 8-grams as a dictionary, where the key is the character N-gram, and the value is the count of that case:\ndefaultdict(<type 'int'>, {'brown fo': 3, 'ck brown': 3, 'n fox th': 2, 'own fox ': 3, ' the qui': 3, ' quick b': 3, 'rown fox': 3, 'uick bro': 3, 'k brown ': 3, ' fox the': 2, 'e quick ': 3, 'fox the ': 2, ' fox XYZ': 1, 'ox the q': 2, 'wn fox X': 1, ' brown f': 3, 'ick brow': 3, 'BC the q': 1, 'the quic': 3, 'quick br': 3, 'ABC the ': 1, 'wn fox t': 2, 'C the qu': 1, 'n fox XY': 1, 'he quick': 3, 'x the qu': 2})\n\n(similar question)\n\nIn terms of performance, this code went through a 6.2MB text file with 128k lines in 4.5 seconds on my laptop (but not writing the results).\n\nIf you want to parse massive amounts of text, consider writing these dictionaries to a NoSQL database like MongoDB. In this case, you can parse text in pieces and don't need to run the thing from the start every time. Writing a Python dictionary to MongoDB should be easy (perhaps transform the dict to JSON and then the import is direct).\n", "data request - Has there been a spike in the birth rate in Germany?": "\nIt's much too early to carry out this analysis today. As you say, the 40 weeks period has just passed by. You should not expect the data to be ready in real-time. I am even surprised that you have already found data for 2014. I had a quick look at the two authoritative sources for this kind of data.\nAt the time of writing this answer, Destatis, the Federal Statistical Office of Germany, has only data up to 2013.\nEurostat, the statistical office of the European Union, has monthly data on live births. For Germany it has data for January to April 2014. But these figures are flagged as \u201cforecast\u201d.\nAs a conclusion I think that you will have to wait some more months before you can do this analysis.\n", "usaidopen - How much data/information should be open?": "\nPer Project Open Data, data should be \"complete\" and published \"with the finest possible level of granularity that is practicable and permitted by law and other requirements. Derived or aggregate open data should also be published but must reference the primary data.\"  Please see our FAQ#8 for a discussion of aggregate data.\n", "data request - Cumulative income by decile": "\nEurostat has data for the member states of the European Union as well as for some other countries. \nilc_iw01 - Distribution of income by quantiles\nIf you are eligible you can also apply for an acceess to the data from the Luxembourg Income Study. It has a wider geographical coverage:\n", "data.gov - Water level historical data for California": "\nIf you're okay with talking to people, you might want to contact the chief of the Media & Public Information branch of the California Dept. of Water Resources at (916) 653-9712\n", "tool request - Time-Series data viewer": "\nIt's hard to suggest a good tool without knowing how deep into programming you want to go, or if the tool is for exploration or presentation.\nBut here is a sample of many good tools out there:\nJavascript:\n\nEnvision.js\nRickshaw\nCubism\n\nR:\n\nDygraphs\nGoogle Charts\n\nPython:\n\nMatplotlib\nPandas (using Matplotlib)\nPlotly blog - Time Series Graphs & Eleven Stunning Ways You Can Use Them\n\nTableau\n\nIf you don't mind that the data is shared, you can use Tableau Public for free\nAlternatives to Tableau Public\n\n", "us census - Is there a time series data set that includes employment information at the neighborhood level?": "\nYou will not find such observed data at such a small geographic level in the public sphere. Apart from small area estimation models that predict the proportion in an area based on indirect information, no such data surveys the population with enough of a sample that can sufficiently monitor the unemployed in a small area each and every month.\n", "data request - Recent Macroeconomics dataset of all countries in the World": "\nInternational organisations provide this kind of data. As you want to cover the whole World, have a look at organisations with the same geographical scope:\nThe World Bank has a database that covers a wide range of topics.  \nIf you are more focussed on economic indicators, you can also have a look at the World Economic Outlook Database of the International Monetary Fund (IMF).\nBoth sites allow you to view the data online as well as to download it further uses.\nThe United Nations Statistics Division also has a couple of databases on various topics.\n", "products - Downloadable smartphones data (name, specs)": "\n2022 UPDATE: The best answer is now Wikidata, see this answer.\n\nThe most pragmatic solution is to use DBpedia.\n\nGo to the Smartphone page of DBpedia\nScroll to the is dbp:type of section\nOn the right is a list of hundreds of smartphones\nEach of these pages has a lot of information like CPU speed, weight, battery, storage, etc\nThis information is available as RDF/JSON/CSV. Here is an excerpt of the CSV:\n\n\nThe list of smartphones is also available as RDF/JSON/CSV, so it is very easy to download all of this information programmatically.\nLicense: Creative Commons Attribution-ShareAlike 3.0 Unported License\n", "data request - Phonemic & Syllabic N-Gram Distributions of the World's Languages": "\nBased on Stanislav's comment:\nhttps://www.sttmedia.com/syllablefrequencies\n\nWith the Syllable Counter integrated in the WordCreator, it is possible to create frequency tables for syllables used in different texts.\nThe Syllable Counter can be used for arbitrary symbol systems, alphabets and Unicode-texts.\nFor some languages, we have created frequency profiles for two-party syllables (digrams) and three-party syllables (trigrams) on the basis of texts in the corresponding languages with at least 1.5 million characters.\nThe texts consists of different literature genres to provide the best results.\nYou can find the lists in the menu under \"Syllable Lists\".\n\n", "where i should get USA water historical data for data science in python?": "\nusgs water has some historic, and real-time water data\nhttp://www.usgs.gov/water/\n", "images - Where can I get in-car camera data?": "\nYou may do it a hard way. YouTube and other video hostings have a bunch of DVR videos that capture lots of real-world events.\nFind videos on YouTube\nSay, by the Russians:\nhttp://www.youtube.com/results?search_query=russian+dvr\nTake random screenshots from these videos\nhttps://stackoverflow.com/questions/26850636/youtube-api-grab-screenshot-of-a-video-at-a-specific-time\nLabel it with Amazon MTurk\nhttps://www.mturk.com/mturk/welcome\nThen do the analysis.\nYou can also ask data from authors who published on your issue and may have well-behaving datasets.\n", "data request - Interesting, open datasets for scoring of location attractiveness?": "\nOften those data are released at a local level and the country / city you are interested in will defined what data are available. \nFor example the city of Toronto (Canada) have build a Well Being application to map and weight hundreds of indicators: http://map.toronto.ca/wellbeing/ Once you selected the indicators, you can export the row data.\nThis is just an example. Indication on what municipality you are interested in will help to narrow down the answer.\n", "data request - California water district boundaries": "\nUpdate:\nthis data is now available on open data se's datahub.io account, here:\nhttp://datahub.io/dataset/california-water-district-boundaries \nFor those times when data was posted, but has since disappeared, you can try The Internet Archive's Wayback Machine.  It does have some limitations, as it won't violate a robots.txt file and it may not archive large files, but in your particular case, it seems to have copies of the two files from 2009-2010:\n\nhttps://web.archive.org/web/%2a/http://projects.atlas.ca.gov/frs/download.php/245/usbr_wat_dist_state_2003_03_25.zip\nhttps://web.archive.org/web/%2a/http://projects.atlas.ca.gov/frs/download.php/26/usbr_wat_dist_priv.zip\n\nI've verified that the most recent copies both unzip without errors, but I haven't done any other testing to determine if they're intact / complete.\n", "usa - Where can I find machine-readable data on which US states and DC border each other?": "\nAnother way to look at it is if the states are within a small distance of each other. Using PostGIS, you can do this rather easily:\nSELECT \n  s1.name state1, \n  s2.name state2,\n  ST_DWithin(s1.the_geom::geography,s2.the_geom::geography,500) share_border\nFROM state_polygons s1, state_polygons s2\nWHERE s1.name < s2.name\nORDER BY share_border DESC, state1 ASC, state2 ASC\n\nI have a data table of states in my CartoDB account and ran that query. Depending on the data, you can choose a more accurate number than 500 meters as I did here. I pulled the state polygons from CartoDB's data library. The state polygons were originally from Natural Earth Data.\nThis produces a data table like this:\n\n", "data request - Road surface roughness": "\nIn Italy there is a project named SmartRoadSense, started on February 21th, 2015. The goal of this project is monitoring of Italy's road surface (487,700 km [1]) and all results are open data.\nNow, after nine months, has been collected more than 24,000 km of open data. In the site smartroadsense.it you can see two uses of this data, and donwload it.\nThe project use a simple Android application that senses the roughness via LPC coding (for more detail read the paper SmartRoadSense: Collaborative Road Surface Condition Monitoring). In the roadmap there is also the development of the application for the others platform.\nYou can see a short movie which showing the four phases of the SmartRoadSense process: sampling, map matching, aggregation, and presentation here.\nThe data are available at smartroadsense.it. You find an only CSV file with six fields:\n\nLATITUDE\nLONGITUDE\nPPE, roughness value in this path\nOSM_ID, ID of the road in OpenStreetMap\nHIGHWAY, category of the road in OpenStreetMap\nUPDATED_AT, date of the last update of this roughness\n\n\n[1] \"Italy.\" The World Factbook. Central Intelligence Agency, 2013.\n", "ckan - How to exclude datasets with no data on datahub.io search?": "\nIt's kind of hacky, but you can use a search engine like Google to search within a domain, and then exclude certain strings that indicate the dataset has no data.\n\nLINK\nIn this case, the search includes pages with:\n\nESIS (but can be left blank)\nsite:datahub.io/dataset/ (include the dataset folder because otherwise, the \"no data\" message comes in various languages)\n\nExcludes pages with:\n\n\"This dataset has no data, why not add some?\"\nrelated (non-data folder)\nactivity (non-data folder)\n\nIn the results list, you'll see the ESIS page you mentioned is not included. You can removed ESIS and replace it with the term you want to search, or leave it blank to search all datasets.\n", "data request - Soccer leagues/teams/players API": "\nFootball data is a page with free statistics in .csv with a large data about matches, beats, results, players and other interesting facts.\nAnother very common used by journalists (a friend who is in the business pointed) is the Wikipedia of Soccer, which you download in .xml only. This page is also free. \n", "Download East African NDVI data for 5*5km grids": "\nSearching for 'NDVI' led me to a page from the NASA Earth Observatory website.  So I then searched data.nasa.gov, and got back a few links ... I have no idea which one is what you want, though:\nhttps://data.nasa.gov/data?search=NDVI&category=\n", "data request - Names of popular products found in retail stores": "\nOpen Product Data (an Open Knowledge project) has a lot of this kind of data: http://www.product-open-data.com/download/\nIn my experience, the data quality varies but is improving.\n", "Linked Data vs Linked Open Data": "\nIn 2006, Tim Berners-Lee defined the four rules of Linked Data:\n\n\nUse URIs as names for things\nUse HTTP URIs so that people can look up those names.\nWhen someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL) \nInclude links to other URIs. so that they can discover more things. \n\n\nIn 2010, he introduced the 5 star rating system for Linked Open Data:\n\n\nAvailable on the web (whatever format) but with an open licence, to be Open Data\nAvailable as machine-readable structured data (e.g. excel instead of image scan of a table)\nas (2) plus non-proprietary format (e.g. CSV instead of excel)\nAll the above plus, Use open standards from W3C (RDF and SPARQL) to identify things, so that people can point at your stuff\nAll the above, plus: Link your data to other people\u2019s data to provide context\n\n\nBoth is published on his personal note, Linked Data - Design Issues, where he also explains:\n\nLinked Open Data (LOD) is Linked Data which is released under an open licence\n\n", "geospatial - ISO 3166-1 and ISO 3166-2 data plus borders (e.g. as kml)": "\nI've started to collect a list of world (country, cities, etc.) datasets, guides, etc. in the Awesome World List @ Planet Open Data. For example, the GeoNames, Natural Earth, etc. datasets might be some candidates. Cheers.  \nNatural Earth Data\nhttp://www.naturalearthdata.com/\nyou can download shapefiles here, not kml\n", "finance - Art Market data": "\nMaybe start an open database of auction results? It will never be as complete as commercial ones, but I'm sure there are many people who would use it (and  possibly add content).\n", "medical - Global HIV Incidence Raw Data": "\nThere seems to be a tremendous variety of data on HIV available from World Health Organization: http://www.who.int/hiv/data/en/ \nThis site also seems to have fantastic data plus it lists sources: https://ourworldindata.org/hiv-aids#data-sources\nCDC: https://www.cdc.gov/hiv/statistics/overview/ \nUN data: http://data.un.org/Data.aspx?q=hiv&d=UNAIDS&f=inID%3a35\nI hope some of these help. \n"}